
\chapter{Regularization}
\label{cha:regularization}

We train model on training data but use test data (not used to train the model) to test out model.
The ability to perform on test data is called \keyword{generalization}.
We can use model on test data because we assume that the train data and the test has the same probability distribution (i.e. they have relationship).


\keyword{Regularization} is any modification we make to a learnining algorithm that is intended to reduce its generalization error.

In practice, it is very difficult to find a model with the right number of parameters.
Indeed, we often use a larger model that has been regularized appropriately.
\section{Parameter norm penalties}
\label{sec:param-norm-penalt}


Parameter norm penalties ($\Omega(\bm{\theta})$) can be added to the object function \(J\) to limit the capacity of the model.

\begin{equation}
  \label{eq:norm-penalties}
  \tilde{J}(\bm{\theta;X,y}) = J(\bm{\theta,X,y}) + \alpha \Omega(\bm{\theta})
\end{equation}


\subsection{$L^2$ Parameter Regularization}

The $L^2$ parameter norm penalty commonly known as \keyword{weight decay}.
\begin{equation}
  \label{eq:12}
  \Omega(\bm{\theta}) = \frac{1}{2} ||\bm{w}||_2^2
\end{equation}

Where \(\bm{w}\) is the model parameter matrix.

\subsection{$L^1$ Regularization}

$L^1$ regularization on the model parameter $\bm{w}$ is defined as
\begin{equation}
  \label{eq:l1}
  \Omega(\bm{\theta}) = ||\bm{w}||_1
\end{equation}

\section{Dataset Augmentation}

The best way to make a machine learning model generalize better is to train it on more data.
Of course, in practice, the amount of data we have is limited.
One way to get around this problem is to create fake data and add it to the training set.
Dataset augmentation has been a particular effective technique for a specific classification problem: object recognition.



\section{Early stopping}
\label{sec:early-stopping}


Early stopping is used to avoid overfit.

The algorithm terminates when no parameters have improved over the best recorded validation error for some pre-specified number of iterations.
This strategy is known as \keyword{early stopping}.
It is probably the most commonly used form of regularization in deep learning.


\section{Droupout}
\label{sec:droupout}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "machine-learning"
%%% End:
