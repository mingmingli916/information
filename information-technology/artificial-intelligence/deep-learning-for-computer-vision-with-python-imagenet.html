<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>deep-learning-for-computer-vision-with-python-imagnet</title>
<!-- 2019-12-09 Mon 15:27 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">deep-learning-for-computer-vision-with-python-imagnet</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. Introduction</a></li>
<li><a href="#sec-2">2. Training Network Using Muliple GPUs</a>
<ul>
<li><a href="#sec-2-1">2.1. How Many GPUs Do I Need?</a></li>
<li><a href="#sec-2-2">2.2. Performance Gains Using Multiple GPUs</a></li>
</ul>
</li>
<li><a href="#sec-3">3. ImageNet</a>
<ul>
<li><a href="#sec-3-1">3.1. The ImageNet Dataset</a>
<ul>
<li><a href="#sec-3-1-1">3.1.1. ILSVRC</a></li>
</ul>
</li>
<li><a href="#sec-3-2">3.2. Obtaining ImageNet</a></li>
<li><a href="#sec-3-3">3.3. My method</a>
<ul>
<li><a href="#sec-3-3-1">3.3.1. Google</a></li>
<li><a href="#sec-3-3-2">3.3.2. Kaggle</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sec-4">4. Preparing the ImageNet Dataset</a></li>
<li><a href="#sec-5">5. Traing Alexnet on Imagenet</a></li>
<li><a href="#sec-6">6. Case Study: Vehicle Identification</a></li>
<li><a href="#sec-7">7. Case Study: Age and Gender Prediction</a></li>
<li><a href="#sec-8">8. Faster R-CNNs</a>
<ul>
<li><a href="#sec-8-1">8.1. Object Detection and Deep Learning</a>
<ul>
<li><a href="#sec-8-1-1">8.1.1. Measuring Object Detector Performance</a></li>
<li><a href="#sec-8-1-2">8.1.2. Why do we use Intersection over Union?</a></li>
<li><a href="#sec-8-1-3">8.1.3. Mean Average Precision (mAP)</a></li>
</ul>
</li>
<li><a href="#sec-8-2">8.2. The (Faster) R-CNN Architecture</a>
<ul>
<li><a href="#sec-8-2-1">8.2.1. A Brief History</a></li>
<li><a href="#sec-8-2-2">8.2.2. The Base Network</a></li>
<li><a href="#sec-8-2-3">8.2.3. Anchors</a></li>
<li><a href="#sec-8-2-4">8.2.4. Region Proposal Network (RPN)</a></li>
<li><a href="#sec-8-2-5">8.2.5. Region of Interest (ROI) Pooling</a></li>
<li><a href="#sec-8-2-6">8.2.6. Region-based Convolutional Neural Network</a></li>
<li><a href="#sec-8-2-7">8.2.7. The Complete Training Pipeline</a></li>
</ul>
</li>
<li><a href="#sec-8-3">8.3. Summary</a></li>
</ul>
</li>
<li><a href="#sec-9">9. training a Faster R-CNN From Scratch</a>
<ul>
<li><a href="#sec-9-1">9.1. The LISA Traffic Signs Dataset</a></li>
<li><a href="#sec-9-2">9.2. Tensorflow Object Detection API Install</a></li>
<li><a href="#sec-9-3">9.3. Training Your Faster R-CNN</a>
<ul>
<li><a href="#sec-9-3-1">9.3.1. A TensorFlow Annotation Class</a></li>
<li><a href="#sec-9-3-2">9.3.2. A Critical Pre-Training Step</a></li>
<li><a href="#sec-9-3-3">9.3.3. Configuring the Faster R-CNN</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sec-10">10. Signle Shot Detectors(SSDs) (SSD: Single Shot MultiBox Detector)</a>
<ul>
<li><a href="#sec-10-1">10.1. Understanding SSD</a>
<ul>
<li><a href="#sec-10-1-1">10.1.1. Movtivation</a></li>
<li><a href="#sec-10-1-2">10.1.2. Architecture</a></li>
<li><a href="#sec-10-1-3">10.1.3. MultiBox, Priors and Fixed Priors</a></li>
<li><a href="#sec-10-1-4">10.1.4. Training</a></li>
</ul>
</li>
<li><a href="#sec-10-2">10.2. Summary</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> Introduction</h2>
<div class="outline-text-2" id="text-1">
<p>
Experiment procedure:<br  />
</p>
<ol class="org-ol">
<li>The exact process when training the network.<br  />
</li>
<li>The particular results.<br  />
</li>
<li>The changes made in the next experiment.<br  />
</li>
</ol>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> Training Network Using Muliple GPUs</h2>
<div class="outline-text-2" id="text-2">
<p>
While backends such as Theano and Tensorflow (and therefor Keras) do support multiple GPU training, the process to set up a mulitple GPU experiment is arduous and non-trival. (Thu Sep 26 22:42:30 CST 2019)<br  />
Therefore, for deep neural networks and large datasets, I highly recommend using the mxnet library. The mxnet deep learning library (written in C++) provides bindings to the Python programming language and specializes in distributed, multi-machine learning.<br  />
</p>
</div>

<div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1"><span class="section-number-3">2.1</span> How Many GPUs Do I Need?</h3>
<div class="outline-text-3" id="text-2-1">
<p>
The benefit of using multiple GPUs is obvious – parallelization. The more GPUs we can throw at the problem, the faster we can train a given network.<br  />
</p>

<pre class="example">
You are now entering the world of state-of-the-art deep learning 
where experiments can take days, weeks, or even in some rare cases, 
months to complete – this timeline is totally and completely normal.
</pre>
</div>
</div>

<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2"><span class="section-number-3">2.2</span> Performance Gains Using Multiple GPUs</h3>
<div class="outline-text-3" id="text-2-2">

<div class="figure">
<p><img src="pics/c2_multiple_gpus.png" alt="c2_multiple_gpus.png" /><br  />
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> ImageNet</h2>
<div class="outline-text-2" id="text-3">
<p>
ILSVRC (ImangeNet Large Scale Visual Recognition Challenge) is the de facto benchmark of evaluating image classification algorithms.<br  />
</p>
</div>
<div id="outline-container-sec-3-1" class="outline-3">
<h3 id="sec-3-1"><span class="section-number-3">3.1</span> The ImageNet Dataset</h3>
<div class="outline-text-3" id="text-3-1">
<p>
ImageNet is actually a project aimed at labeling and catogorizing images into all its 22000 catogories based on defined set of words and phrases.<br  />
</p>

<p>
Images follows the WordNet hierarchy. Each meaningful word/phrase inside WordNet is called a "synonym set" or synset for short. Within the ImageNet project, images are categorized according to these synsets; the goal of the project is to have 1,000+ images per synset.<br  />
</p>
</div>

<div id="outline-container-sec-3-1-1" class="outline-4">
<h4 id="sec-3-1-1"><span class="section-number-4">3.1.1</span> ILSVRC</h4>
<div class="outline-text-4" id="text-3-1-1">
<p>
In the context of computer vision and deep learning, whenever you hear people talking about image net, they are likely referring to the ILSVRC. The goal of the image classification track in this challenge is to train a model that can correctly classify an image into 1,000 separate object categories.<br  />
</p>

<p>
Images in ImageNet vary dramatically across object scale, number of instances, image clutter/occlusion, deformability, texture, color, shape, and real-world size. This dataset is challenging, to say the least, and in some cases, it’s hard for even humans to correctly label. Because of the challenging nature of this dataset, deep learning models that perform well on ImageNet are likely to generalize well to images outside of the validation and testing set – this is the exact reason why we apply transfer learning to these models as well.<br  />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-3-2" class="outline-3">
<h3 id="sec-3-2"><span class="section-number-3">3.2</span> Obtaining ImageNet</h3>
<div class="outline-text-3" id="text-3-2">
<p>
image-net.org<br  />
</p>


<div class="figure">
<p><img src="pics/c3_imagenet.png" alt="c3_imagenet.png" /><br  />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-3-3" class="outline-3">
<h3 id="sec-3-3"><span class="section-number-3">3.3</span> My method</h3>
<div class="outline-text-3" id="text-3-3">
</div><div id="outline-container-sec-3-3-1" class="outline-4">
<h4 id="sec-3-3-1"><span class="section-number-4">3.3.1</span> Google</h4>
<div class="outline-text-4" id="text-3-3-1">
<p>
google ILSVRC2015_CLS-LOC<br  />
</p>

<p>
<a href="http://image-net.org/challenges/LSVRC/2015/download-images-3j16.php">http://image-net.org/challenges/LSVRC/2015/download-images-3j16.php</a><br  />
</p>


<p>
development kit: <a href="http://image-net.org/image/ILSVRC2015/ILSVRC2015_devkit.tar.gz">http://image-net.org/image/ILSVRC2015/ILSVRC2015_devkit.tar.gz</a><br  />
object classification/localization: <a href="http://image-net.org/image/ILSVRC2015/ILSVRC2015_CLS-LOC.tar.gz">http://image-net.org/image/ILSVRC2015/ILSVRC2015_CLS-LOC.tar.gz</a><br  />
</p>
</div>
</div>
<div id="outline-container-sec-3-3-2" class="outline-4">
<h4 id="sec-3-3-2"><span class="section-number-4">3.3.2</span> Kaggle</h4>
<div class="outline-text-4" id="text-3-3-2">
<p>
download with kaggle is faster.<br  />
</p>
<div class="org-src-container">

<pre class="src src-sh">kaggle competitions download -c imagenet-object-localization-challenge
</pre>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4"><span class="section-number-2">4</span> Preparing the ImageNet Dataset</h2>
<div class="outline-text-2" id="text-4">
<p>
.rec format (produced by mxnet im2rec tool) is not only more compact than HDF5, but it's more I/O efficient as well.<br  />
</p>

<p>
Imagenet Dataset Structure (download loadded from Kaggle):<br  />
</p>

<ul class="org-ul">
<li>ILSVRC<br  />
<ul class="org-ul">
<li>Annotations (used for the localization challenge)<br  />
</li>
<li>Data<br  />
<ul class="org-ul">
<li>CLS-LOC<br  />
<ul class="org-ul">
<li>test<br  />
</li>
<li>train<br  />
</li>
<li>val<br  />
</li>
</ul>
</li>
</ul>
</li>
<li>ImageSets<br  />
<ul class="org-ul">
<li>CLS-LOC<br  />
<ul class="org-ul">
<li>test.txt (100000)<br  />
</li>
<li>train_cls.txt (1281167)<br  />
</li>
<li>train_loc.txt<br  />
</li>
<li>val.txt (50000)<br  />
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>LOC_synset_mapping.txt.zip (for Kaggle)<br  />
</li>
<li>LOC_train_solution.csv.zip (for Kaggle)<br  />
</li>
<li>LOC_val_solution.csv.zip (for Kaggle)<br  />
</li>
</ul>


<p>
The benefit of using the train_cls.txt and val.txt files is that we do not have to list the contents of the training and validation subdirectories using <code>paths.list_images</code><br  />
</p>


<p>
Development Kit Structure:<br  />
</p>

<ul class="org-ul">
<li>devkit<br  />
<ul class="org-ul">
<li>COPYING (copyright)<br  />
</li>
<li>data<br  />
<ul class="org-ul">
<li>ILSVRC2015_clsloc_validation_blacklist.txt (images that are too ambiguous in their class label, should not be included in validation set)<br  />
</li>
<li>ILSVRC2015_clsloc_validation_ground_truth.mat<br  />
</li>
<li>ILSVRC2015_clsloc_validation_ground_truth.txt (with val.txt to build validation set)<br  />
</li>
<li>ILSVRC2015_det_validation_blacklist.txt<br  />
</li>
<li>ILSVRC2015_det_validation_ground_truth.mat<br  />
</li>
<li>ILSVRC2015_vid_validation_ground_truth.mat<br  />
</li>
<li>map_clsloc.txt (maps WordNet IDs to human readable class labels)<br  />
</li>
<li>map_det.txt<br  />
</li>
<li>map_vid.txt<br  />
</li>
<li>meta_clsloc.mat<br  />
</li>
<li>meta_det.mat<br  />
</li>
<li>meta_vid.mat<br  />
</li>
</ul>
</li>
<li>evaluation (contrains MATLAB routines for evaluating predictions on the testing set)<br  />
</li>
<li>readme.txt<br  />
</li>
</ul>
</li>
</ul>

<p>
Imagenet preparation procedure (with mxnet)<br  />
</p>
<ol class="org-ol">
<li>Download Imagenet<br  />
</li>
<li>Parse into .lst files<br  />
</li>
<li>Build .rec files with imr2rec command<br  />
</li>
</ol>

<pre class="example">
The beauty of this appoach is that the .rec file only have to generated once -- we can use
these record files for any ImageNet classification experiment we wish to perform.
</pre>
</div>
</div>

<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5"><span class="section-number-2">5</span> Traing Alexnet on Imagenet</h2>
<div class="outline-text-2" id="text-5">
<pre class="example">
Sharing the "story" of how the network was trained, and not just the final result, 
will help you in your own deep learning experiments. Watching others, and then 
learning by experience, is the optimal way to quickly master the techniques required to be
successful working with large image datasets and deep learning.
</pre>
</div>
</div>
<div id="outline-container-sec-6" class="outline-2">
<h2 id="sec-6"><span class="section-number-2">6</span> Case Study: Vehicle Identification</h2>
</div>

<div id="outline-container-sec-7" class="outline-2">
<h2 id="sec-7"><span class="section-number-2">7</span> Case Study: Age and Gender Prediction</h2>
<div class="outline-text-2" id="text-7">
<p>
one-off accuracy:<br  />
whether the ground-truth class label matches the predicted class label or if the ground-truth label exists in the two adjacent bins.<br  />
</p>

<pre class="example">
classes: 0-2, 4-6, 8-13, 15-20, 25-32, 38-43, 48-53, and 60+

predicted: 15-20
ground-truth: 25-32
one-off: correct

One-off accuracy is not the same thing as rank-2 accuracy.

4-6 with probability 63.7%
48-53 with probability 36.7%
ground-truth: 48-53
one-off: incorrect
rank-2: correct
</pre>
</div>
</div>
<div id="outline-container-sec-8" class="outline-2">
<h2 id="sec-8"><span class="section-number-2">8</span> Faster R-CNNs</h2>
<div class="outline-text-2" id="text-8">
</div><div id="outline-container-sec-8-1" class="outline-3">
<h3 id="sec-8-1"><span class="section-number-3">8.1</span> Object Detection and Deep Learning</h3>
<div class="outline-text-3" id="text-8-1">
<p>
Object detection has three primary goals:<br  />
</p>
<ol class="org-ol">
<li>A list of bounding boxes, or the (x,y) coordinates for each object in an image<br  />
</li>
<li>A class label associated with each bounding box<br  />
</li>
<li>The probability/confidence score associating with each bounding box and class labels<br  />
</li>
</ol>
</div>

<div id="outline-container-sec-8-1-1" class="outline-4">
<h4 id="sec-8-1-1"><span class="section-number-4">8.1.1</span> Measuring Object Detector Performance</h4>
<div class="outline-text-4" id="text-8-1-1">
<p>
When evaluating object detector performance we use an evaluation metric called Intersection over Union (IoU).<br  />
</p>
<pre class="example">
Any algorithm that provides predicted bounding boxes (and optionally class labels) as output can be evaluated using IoU.
</pre>

<p>
In order to apply IoU to evaluate an arbitrary object detector, we need:<br  />
</p>
<ol class="org-ol">
<li>The ground-truth bounding boxes<br  />
</li>
<li>The predicted bounding boxes from our model<br  />
</li>
<li>If you want to compute recall along with precision, you'll also need the ground-truth class labels and predicted class labels<br  />
</li>
</ol>


<div class="figure">
<p><img src="pics/c15_iou.png" alt="c15_iou.png" /><br  />
</p>
</div>

<pre class="example">
An IoU score &gt; 0.5 is normally considered a "good" prediction.
</pre>
</div>
</div>

<div id="outline-container-sec-8-1-2" class="outline-4">
<h4 id="sec-8-1-2"><span class="section-number-4">8.1.2</span> Why do we use Intersection over Union?</h4>
<div class="outline-text-4" id="text-8-1-2">
<p>
In reality, it's extremely unlikely that the (x, y)-coordinates of our predicted bounding box are going to exactly match the (x, y)-coordinates of the ground-truth bounding box. Due to varying parameters of our model, such as layer used for feature extraction, anchor placement, loss function, etc., a complete and total match between predicted and ground-truth bounding boxes is simply unrealistic.<br  />
</p>

<p>
Because coordinates will not match exactly, we need to define an evaluation metric that rewards predicted bounding boxes for heavily overlapping with the ground-truth, as Figure 15.2 demonstrates.<br  />
</p>


<div class="figure">
<p><img src="pics/c15_iou2.png" alt="c15_iou2.png" /><br  />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-8-1-3" class="outline-4">
<h4 id="sec-8-1-3"><span class="section-number-4">8.1.3</span> Mean Average Precision (mAP)</h4>
<div class="outline-text-4" id="text-8-1-3">
<p>
In the context of machine learning, precision typically refers to accuracy &#x2013; but in the context of object detection, IoU is our precision.<br  />
</p>

<p>
However, we need to define a method to compute accuracy per class and across all classes in dataset. To accomplish this goal, we need mean Average Precision (mAP)<br  />
</p>

<p>
To compute average precision for a single class, we determine the IoU of all data points for a particular class. Once we have the IoU we divide by the total class labels for that specific class, yielding the average precision.<br  />
</p>

<p>
To compute the mean average precision, we compute the average IoU for all N classes, yielding the mean average precision.<br  />
</p>

<pre class="example">
mAP@0.5 : in order of an object in the testing set to be marked as a "positive dection" it must have, at least, 0.5 IoU with the ground-truth.
</pre>
</div>
</div>
</div>


<div id="outline-container-sec-8-2" class="outline-3">
<h3 id="sec-8-2"><span class="section-number-3">8.2</span> The (Faster) R-CNN Architecture</h3>
<div class="outline-text-3" id="text-8-2">
</div><div id="outline-container-sec-8-2-1" class="outline-4">
<h4 id="sec-8-2-1"><span class="section-number-4">8.2.1</span> A Brief History</h4>
<div class="outline-text-4" id="text-8-2-1">
</div><ol class="org-ol"><li><a id="sec-8-2-1-1" name="sec-8-2-1-1"></a>R-CNN<br  /><div class="outline-text-5" id="text-8-2-1-1">

<div class="figure">
<p><img src="pics/c15_rcnn.png" alt="c15_rcnn.png" /><br  />
</p>
</div>

<p>
steps:<br  />
</p>
<ol class="org-ol">
<li>input a image<br  />
</li>
<li>extract region proposals (i.e., regions of the image that potentially contain objects) using an algorithm such as selective search<br  />
</li>
<li>use transfer learning to compute features for each proposal using the pre-trained CNN<br  />
</li>
<li>classify each proposal using the extracted features with a SVM<br  />
</li>
</ol>

<pre class="example">
Looking at the above pipeline, we can clearly see inspirations and parallels from traditional object detectors such as Dalal and Triggs seminal HOG + Linear SVM framework:
1. Instead of applying an exhaustive image pyramid and sliding window, we are swapping in a more intelligent Selective Search algorithm
2. Instead of extracting HOG features from each ROI, we’re now extracting CNN features
3. We’re still training SVM(s) for the final classification of the input ROI, only we’re training this SVM on the CNN features rather than the HOG ones
</pre>

<p>
The primary reason this approach worked so well is due to the robust, discriminative features learned by a CNN.<br  />
</p>

<p>
The problem with the original R-CNN approach is that it’s still incredibly slow. And furthermore, we’re not actually learning to localize via deep neural network. Instead, we’re leaving the localization to the Selective Search algorithm — we’re only classifying the ROI once it’s been determined as "interesting" and "worth examining" by the region proposal algorithm.<br  />
</p>
</div>
</li>

<li><a id="sec-8-2-1-2" name="sec-8-2-1-2"></a>Fast R-CNN<br  /><div class="outline-text-5" id="text-8-2-1-2">

<div class="figure">
<p><img src="pics/c15_fast_rcnn.png" alt="c15_fast_rcnn.png" /><br  />
</p>
</div>

<p>
Fast R-CNN algorithm still utilized Selective Search to obtain region proposals, but a novel contribution was made: Region of Interest (ROI) Pooling.<br  />
</p>

<p>
We apply the CNN to the entire input image and extract a feature map from it using our network. ROI Pooling works by extracting a fixed-size window from the feature map and then passing it into a set of fully-connected layers to obtain the output label for the ROI.<br  />
</p>


<p>
The primary benefit here is that the network is now, effectively, end-to-end trainable:<br  />
</p>
<ol class="org-ol">
<li>input an image and associated ground-truth bounding boxes<br  />
</li>
<li>extract the feature map<br  />
</li>
<li>apply ROI pooling and obtain the ROI feature vector<br  />
</li>
<li>use two sets of fully-connected layers to obtain<br  />
<ol class="org-ol">
<li>the class label predictions<br  />
</li>
<li>the bounding box locations for each proposal<br  />
</li>
</ol>
</li>
</ol>

<p>
Performance suffered dramatically at inference (i.e., prediction) time by being dependent on the Selective Search (or equivalent) region proposal algorithm.<br  />
(To make the R-CNN architecture even faster we need to incorporate the region proposal directly into the R-CNN.)<br  />
</p>
</div>
</li>


<li><a id="sec-8-2-1-3" name="sec-8-2-1-3"></a>Faster R-CNN<br  /><div class="outline-text-5" id="text-8-2-1-3">

<div class="figure">
<p><img src="pics/c15_faster_rcnn.png" alt="c15_faster_rcnn.png" /><br  />
</p>
</div>

<p>
The goal of the RPN is to remove the requirement of running Selective Search prior to inference and instead bake the region proposal directly into the R-CNN architecture.<br  />
</p>


<p>
The first component, the RPN, is used to determine where in an image a potential object could be. At this point we do not know what the object is, just that there is potentially an object at a certain location in the image.<br  />
</p>

<p>
The proposed bounding box ROIs are based on the Region of Interest (ROI) Pooling module of the network along with the extracted features from the previous step.<br  />
</p>

<p>
We place anchors spaced uniformly across the entire image at varying scales and aspect ratios. The RPN will then examine these anchors and output a set of proposals as to where it "thinks" an object exists.<br  />
</p>
</div>
</li></ol>
</div>


<div id="outline-container-sec-8-2-2" class="outline-4">
<h4 id="sec-8-2-2"><span class="section-number-4">8.2.2</span> The Base Network</h4>
<div class="outline-text-4" id="text-8-2-2">
<p>
As the above figure shows, after we input the image to the architecture, the first component we come across is the base network. The base network is typically a CNN pre-trained for a particular classification task. This CNN will be used for transfer learning, in particular, feature extraction.<br  />
</p>

<p>
One important aspect of object detection networks is that they should be fully-convolutional.<br  />
(A fully-convolutional neural network does not contain the fully-connected layers typically found at the end of a network prior to making output predictions.)<br  />
</p>


<p>
A fully-convolutional neural network enjoys two primary benefits:<br  />
</p>
<ol class="org-ol">
<li>Fast, duo to all convolution operations<br  />
</li>
<li>Able to accept images of any spatial resolution<br  />
</li>
</ol>


<div class="figure">
<p><img src="pics/c15_base_network.png" alt="c15_base_network.png" /><br  />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-8-2-3" class="outline-4">
<h4 id="sec-8-2-3"><span class="section-number-4">8.2.3</span> Anchors</h4>
<div class="outline-text-4" id="text-8-2-3">
<p>
In traditional object detection pipelines we would use either:<br  />
</p>
<ol class="org-ol">
<li>a combination of a sliding window + image pyramid or<br  />
</li>
<li>a Selective Search-like algorithm<br  />
</li>
</ol>
<p>
to generate proposals for our classifier.<br  />
</p>

<p>
The core separation between classification and object detection is the prediction of bounding boxes, or (x, y)-coordinates surrounding an object. Thus, we might expect our network to return a tuple consisting of the bounding box coordinates of a particular object.<br  />
</p>

<p>
But there is a problem with this approach:<br  />
</p>
<ol class="org-ol">
<li>How do we handle a network predicting values outside the boundaries of the image?<br  />
</li>
<li>How do we encode restrictionns such as \(x_{min} < x_{max}\) and \(y_{min} < y_{max}\)?<br  />
</li>
</ol>

<p>
It sounds out that this is a near impossible problem to solve. But is solved with a clever and novel one, called anchors. (relative position instead of absolute position)<br  />
</p>



<p>
Instead of trying to predict the raw (x, y)-coordinates of the bounding boxes, we can instead learn to predict their offsets from the reference boxes, namely: \(\Delta_{x-center}\), \(\Delta_{y-center}\), \(\Delta_{width}\), and \(\Delta_{height}\).<br  />
</p>

<p>
So where do these reference bounding boxes come from?<br  />
We need to generate the anchors ourselves without utilizing a Selective Search algorithm. To accomplish this process, we first need to uniformly sample points across an input image (Figure 15.7, left).<br  />
</p>


<div class="figure">
<p><img src="pics/c15_anchors.png" alt="c15_anchors.png" /><br  />
</p>
</div>

<p>
Here we can see an input image that is 600 × 400 pixels — we have labeled each point at a regularly sampled integer (at an interval of sixteen pixels) with a blue circle.<br  />
</p>

<p>
The next step is to create a set of anchors at each of the sampled points. As in the original Faster R-CNN publication, we'll generate nine anchors (which are fixed bounding boxes) with varying sizes and aspect ratios surrounding a given sampled point.<br  />
</p>

<p>
The colors of the bounding boxes are our scales/sizes, namely: 64 × 64, 128 × 128, and 256 × 256. For each scale we also have the aspect ratio, 1 : 1, 1 : 2, and 2 : 1. Each combination of scale and aspect ratio yields nine total anchors. This combination of scale and aspect ratio yields us considerable coverage over all possible object sizes and scales in the input image (Figure 15.7, right).<br  />
</p>

<p>
However, there is a problem here once we break down the total number of anchors generated:<br  />
</p>
<ul class="org-ul">
<li>If we use a stride of 16 pixels (the default for Faster R-CNN) on a 600 × 800 image, we’ll obtain a total of 1,989 total positions. (\((\lceil 600 / 16\rceil + 1) \times (\lceil 800 / 16 \rceil +1) = 1989\))<br  />
</li>
<li>With nine anchors surrounding each of the 1,989 positions, we now have a total of 1, 989×9 = 17, 901 bounding box positions for our CNN to evaluate.<br  />
</li>
</ul>

<p>
Luckily, with the Region Proposal Network (RPN) we can dramatically reduce the number of candidate proposal windows, leaving us with a much more manageable size.<br  />
</p>
</div>
</div>

<div id="outline-container-sec-8-2-4" class="outline-4">
<h4 id="sec-8-2-4"><span class="section-number-4">8.2.4</span> Region Proposal Network (RPN)</h4>
<div class="outline-text-4" id="text-8-2-4">
<p>
If the goal of generating anchors is to obtain good coverage over all possible scales and sizes of objects in an image, the goal of the Region Proposal Network (RPN) is to prune the number of generated bounding boxes to a more manageable size.<br  />
</p>


<div class="figure">
<p><img src="pics/c15_faster_rcnn.png" alt="c15_faster_rcnn.png" /><br  />
</p>
</div>

<p>
The RPN accepts convolutional feature map as input. Then apply a 3x3 CONV, learning 512 filters.<br  />
</p>

<p>
These filters are fed into two paths in parallel. The first output (left) of the RPN is a score that indicates whether the RPN thinks the ROI is foreground or background. The dimensionality of this output is 2 x K where K is the total number of anchors.<br  />
</p>


<div class="figure">
<p><img src="pics/c15_rpn.png" alt="c15_rpn.png" /><br  />
</p>
</div>

<p>
The second output (right) is our bounding box regressor used to adjust anchors to better fit the object that it is surrounding. Adjusting the anchors is accomplished via 1x1 convolution, outputting a 4 x K volume. (predicting the four delta values:  \(\Delta_{x-center}\), \(\Delta_{y-center}\), \(\Delta_{width}\), and \(\Delta_{height}\))<br  />
</p>

<p>
Provided the foreground probability is sufficiently large, then apply:<br  />
</p>
<ol class="org-ol">
<li>Non-maxima suppression to suppress overlapping<br  />
</li>
<li>Proposal selection (take only the top N proposals and discard the rest)<br  />
</li>
</ol>
</div>


<ol class="org-ol"><li><a id="sec-8-2-4-1" name="sec-8-2-4-1"></a>Training the RPN<br  /><div class="outline-text-5" id="text-8-2-4-1">
<p>
During the training, we take our anchors and put them into two different buckets:<br  />
</p>
<ol class="org-ol">
<li>Foreground: all anchors that have a 0.5 IoU with a ground-truth object bounding box.<br  />
</li>
<li>Background: all anchors that have &lt; 0.1 IoU with a ground-truth object bounding box.<br  />
</li>
</ol>

<p>
Based on these buckets we randomly sample between the two to maintain an equal ratio between background and foreground.<br  />
</p>

<p>
loss functions:<br  />
</p>
<ol class="org-ol">
<li>for classification which measures the accuracy of the RPN predicting the foreground and background,  binary cross-entropy works nicely<br  />
</li>
<li>for bounding box regression, the loss function only operates on the foreground anchors as background anchors would have no sense of a bounding box. (Girshick used Smooth L1 loss)<br  />
</li>
</ol>
</div>
</li></ol>
</div>


<div id="outline-container-sec-8-2-5" class="outline-4">
<h4 id="sec-8-2-5"><span class="section-number-4">8.2.5</span> Region of Interest (ROI) Pooling</h4>
<div class="outline-text-4" id="text-8-2-5">

<div class="figure">
<p><img src="pics/c15_roi_pooling.png" alt="c15_roi_pooling.png" /><br  />
</p>
</div>

<p>
The goal of the ROI Pooling module is to accept all N proposal locations from the RPN module and crop out feature vectors from the convolutional feature map.<br  />
</p>

<p>
Cropping feature vectors is accomplished by:<br  />
</p>
<ol class="org-ol">
<li>Using array slicing to extract the corresponing patch from the feature map<br  />
</li>
<li>Resizing it into 14 x 14 x D where D is the depth of the feature map<br  />
</li>
<li>Applying a max pooling operation with 2 x 2 strides, yielding a 7 x 7 x D feature vector.<br  />
</li>
</ol>

<p>
The final feature vector is fed into the Region based Convolutional Neural network.<br  />
</p>
</div>
</div>

<div id="outline-container-sec-8-2-6" class="outline-4">
<h4 id="sec-8-2-6"><span class="section-number-4">8.2.6</span> Region-based Convolutional Neural Network</h4>
<div class="outline-text-4" id="text-8-2-6">
<p>
The final stage is the Region-based Convolutional Neural Network (R-CNN). This module serves two purposes:<br  />
</p>
<ol class="org-ol">
<li>Obtain the final class label predictions for each bounding box location based on the cropped feature map from the ROI Pooling module<br  />
</li>
<li>Further refine the bounding box prediction (x, y)-coordinates for better prediction accuracy<br  />
</li>
</ol>


<div class="figure">
<p><img src="pics/c15_final_rcnn.png" alt="c15_final_rcnn.png" /><br  />
</p>
</div>

<p>
These two outputs again imply that we'll have two loss functions:<br  />
</p>
<ol class="org-ol">
<li>Categorical cross-entropy for classification<br  />
</li>
<li>Smooth L1 loss for bounding box regression<br  />
</li>
</ol>
</div>
</div>


<div id="outline-container-sec-8-2-7" class="outline-4">
<h4 id="sec-8-2-7"><span class="section-number-4">8.2.7</span> The Complete Training Pipeline</h4>
<div class="outline-text-4" id="text-8-2-7">
<p>
We have a choice to make when training the entire Faster R-CNN pipeline. The first choice is to train the RPN module, obtain satisfiable results, and then move on to training the R-CNN module. The second choice is to combine the four loss functions (two for the RPN module, two for the R-CNN module) via weighted sum and then jointly train all four. Which one is better?<br  />
</p>

<p>
In nearly all situations you’ll find that jointly training the entire network end-to-end by minimizing the weighted sum of the four loss functions not only takes less time but also obtains higher accuracy as well.<br  />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-8-3" class="outline-3">
<h3 id="sec-8-3"><span class="section-number-3">8.3</span> Summary</h3>
<div class="outline-text-3" id="text-8-3">
<p>
The architecture includes four primary components:<br  />
</p>
<ol class="org-ol">
<li>base network (feature extract)<br  />
</li>
<li>RPN (region proposal)<br  />
</li>
<li>ROI pooling (extract feature from RPN)<br  />
</li>
<li>R-CNN (predict the label and box)<br  />
</li>
</ol>
</div>
</div>
</div>


<div id="outline-container-sec-9" class="outline-2">
<h2 id="sec-9"><span class="section-number-2">9</span> training a Faster R-CNN From Scratch</h2>
<div class="outline-text-2" id="text-9">
</div><div id="outline-container-sec-9-1" class="outline-3">
<h3 id="sec-9-1"><span class="section-number-3">9.1</span> The LISA Traffic Signs Dataset</h3>
<div class="outline-text-3" id="text-9-1">
<p>
The dataset consists of 47 different United States traffic sign types. There are a total of 7,855 annotations on 6,610 frames. Road signs vary in resolution, from 6 × 6 to 167 × 168 pixels. Furthermore, some images were captured in a lower resolution 640 × 480 camera while others were captured on a higher resolution 1024 × 522 pixels. Some images are grayscale while others are color. The variance in camera quality and capture color space make this an interesting dataset to study in terms of object detection.<br  />
</p>
</div>
</div>

<div id="outline-container-sec-9-2" class="outline-3">
<h3 id="sec-9-2"><span class="section-number-3">9.2</span> Tensorflow Object Detection API Install</h3>
<div class="outline-text-3" id="text-9-2">
<p>
Implementing the entire Faster R-CNN architecture from scratch has the following defects:<br  />
</p>
<ol class="org-ol">
<li>long code<br  />
</li>
<li>object detection libraries and packages tend to be fragile in their nature as custom layers and loss methods are used.<br  />
</li>
</ol>

<p>
TFOD API install:<br  />
</p>
<ul class="org-ul">
<li>download code<br  />
</li>
</ul>
<p>
<a href="https://github.com/tensorflow/models">https://github.com/tensorflow/models</a><br  />
</p>


<ul class="org-ul">
<li>install protobuf<br  />
</li>
</ul>
<p>
<a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md#testing-the-installation">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md#testing-the-installation</a><br  />
</p>


<p>
if not<br  />
</p>
<div class="org-src-container">

<pre class="src src-sh">pip install protobuf
</pre>
</div>


<ul class="org-ul">
<li>add python path<br  />
</li>
</ul>
<p>
add the following to ~/.bashrc<br  />
</p>
<div class="org-src-container">

<pre class="src src-sh"><span style="color: #ff8A65;">export</span> <span style="color: #ffcc80;">PYTHONPATH</span>=$<span style="color: #ffcc80;">PYTHONPATH</span>:~/models/research:~/models/research/slim  <span style="color: #b0bec5;"># </span><span style="color: #b0bec5;">change the directory</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-9-3" class="outline-3">
<h3 id="sec-9-3"><span class="section-number-3">9.3</span> Training Your Faster R-CNN</h3>
<div class="outline-text-3" id="text-9-3">
</div><div id="outline-container-sec-9-3-1" class="outline-4">
<h4 id="sec-9-3-1"><span class="section-number-4">9.3.1</span> A TensorFlow Annotation Class</h4>
<div class="outline-text-4" id="text-9-3-1">
<p>
When working with the TFOD API, we need to build a dataset consisting of both the images and their associated bounding boxes.<br  />
What makes up "data point" for object detection? According to the TFOD API, we need to supply a number of attributes, including:<br  />
</p>
<ul class="org-ul">
<li>The TensorFlow-encoded image<br  />
</li>
<li>The width and height of the image<br  />
</li>
<li>The file encoding of the image (i.e., JPG, PNG, etc.)<br  />
</li>
<li>The filename<br  />
</li>
<li>A list of bounding box coordinates, normalized in the range [0, 1], for the image<br  />
</li>
<li>A list of class labels for each bounding box<br  />
</li>
<li>A flag used to encode if the bounding box is "difficult" or not (you’ll almost always want to leave this value as "0", or "not difficult" so TensorFlow trains on it — the difficult flag is a remnant of the VOC challenge).<br  />
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-9-3-2" class="outline-4">
<h4 id="sec-9-3-2"><span class="section-number-4">9.3.2</span> A Critical Pre-Training Step</h4>
<div class="outline-text-4" id="text-9-3-2">
<pre class="example">
One of the biggest mistakes I see deep learning developers, students, and researchers make is rushing and not double-checking their work when building a dataset.
Be warned: rushing will only cause you problems, especially in the context of object detection.
</pre>
</div>
</div>

<div id="outline-container-sec-9-3-3" class="outline-4">
<h4 id="sec-9-3-3"><span class="section-number-4">9.3.3</span> Configuring the Faster R-CNN</h4>
<div class="outline-text-4" id="text-9-3-3">
<p>
Training our Faster R-CNN on the LISA dataset is a four step process:<br  />
</p>
<ol class="org-ol">
<li>Download the pre-trained Faster R-CNN so we can fine-tune the network<br  />
</li>
<li>Download the sample TFOD API configuration file and modify it to point to our record files<br  />
</li>
<li>Start the training process and monitor<br  />
</li>
<li>Export the frozen model graph after training is complete<br  />
</li>
</ol>

<p>
step1:<br  />
download faster_rcnn_resnet101_coco (<a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md</a>)<br  />
move it to experiments/training<br  />
untar faster_rcnn_resnet101_coco<br  />
</p>

<p>
step2:<br  />
download configuration files( (<a href="https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs">https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs</a>)<br  />
(faster_rcnn_resnet101_coco.config<br  />
faster_rcnn_resnet101_kitti.config<br  />
faster_rcnn_resnet101_pets.config<br  />
faster_rcnn_resnet101_voc07.config)<br  />
mv faster_rcnn_resnet101_pets.config faster_rcnn_lias.config (Pets configuration requires fewer changes than the COCO configuration)<br  />
</p>

<pre class="example">
num_classes: 37 # -&gt; 3

num_steps: 200000 # -&gt; 50000


fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt" -&gt; "/home/hack/PycharmProjects/computer_vision/ic16_training_a_faster_rcnn_from_scratch/experiments/training/faster_rcnn_resnet101_coco_2018_01_28/model.ckpt"
# The model.cpkt file is the base filename in which the TFOD API uses to derive the other three files.


train_input_reader: {
  tf_record_input_reader {
    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-00010" -&gt;  "/home/hack/PycharmProjects/computer_vision/ic16_training_a_faster_rcnn_from_scratch/records/training.record"
  }
  label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt" -&gt; "/home/hack/PycharmProjects/computer_vision/ic16_training_a_faster_rcnn_from_scratch/records/classes.pbtxt"
}

eval_config: {
  metrics_set: "coco_detection_metrics"
  num_examples: 955  # the total number of bounding boxes in the testing set.
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-00010" -&gt;  "/home/hack/PycharmProjects/computer_vision/ic16_training_a_faster_rcnn_from_scratch/records/testing.record"
  }
  label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt" -&gt; "/home/hack/PycharmProjects/computer_vision/ic16_training_a_faster_rcnn_from_scratch/records/classes.pbtxt"
  shuffle: false
  num_readers: 1
}
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-sec-10" class="outline-2">
<h2 id="sec-10"><span class="section-number-2">10</span> Signle Shot Detectors(SSDs) (SSD: Single Shot MultiBox Detector)</h2>
<div class="outline-text-2" id="text-10">
<p>
Two problems in Faster R-CNN:<br  />
</p>
<ol class="org-ol">
<li>the framework is complex<br  />
</li>
<li>7-10 FPS, not sufficient for real-time performance<br  />
</li>
</ol>

<p>
The SSD object detector is entirely end-to-end, contains no complex moving parts, and is capable of super real-time performance.<br  />
</p>
</div>
<div id="outline-container-sec-10-1" class="outline-3">
<h3 id="sec-10-1"><span class="section-number-3">10.1</span> Understanding SSD</h3>
<div class="outline-text-3" id="text-10-1">
</div><div id="outline-container-sec-10-1-1" class="outline-4">
<h4 id="sec-10-1-1"><span class="section-number-4">10.1.1</span> Movtivation</h4>
<div class="outline-text-4" id="text-10-1-1">
<ol class="org-ol">
<li>prior to the discorvery of training the entire R-CNN architecture end-to-end, RPN is introduced a tedious pre-training process<br  />
</li>
<li>training takes too long<br  />
</li>
<li>inference time is too slow<br  />
</li>
</ol>

<p>
Single Shot implies that both localization and dectection are performed in a sigle forward pass of the network during inference time.<br  />
</p>

<p>
Unlike R-CNNs that require refetching pixels from the original image or slices from the feature map, SSDs instead continue to propagate the feature maps forward, connecting the feature maps in a novel way such that objects of various sizes and scales can be detected. According to Liu et al, the fundamental improvement in speed of SSDs comes from eliminating bounding box proposals and subsampling of pixels or features.<br  />
</p>

<p>
Dectector means:<br  />
</p>
<ol class="org-ol">
<li>localize the object<br  />
</li>
<li>lable the object<br  />
</li>
</ol>
</div>
</div>

<div id="outline-container-sec-10-1-2" class="outline-4">
<h4 id="sec-10-1-2"><span class="section-number-4">10.1.2</span> Architecture</h4>
<div class="outline-text-4" id="text-10-1-2">

<div class="figure">
<p><img src="pics/c17_ssd.png" alt="c17_ssd.png" /><br  />
</p>
</div>

<p>
Tow important components:<br  />
</p>
<ol class="org-ol">
<li>progressively reduce the volume size in deeper layer<br  />
</li>
<li>each of the CONV layers connects to the final dectection layer<br  />
</li>
</ol>

<p>
The connections is important:<br  />
</p>
<ol class="org-ol">
<li>allows the network to detect and localize at varying scales<br  />
</li>
<li>this scale localization happens in a forward pass (make it fast)<br  />
</li>
</ol>
</div>
</div>


<div id="outline-container-sec-10-1-3" class="outline-4">
<h4 id="sec-10-1-3"><span class="section-number-4">10.1.3</span> MultiBox, Priors and Fixed Priors</h4>
<div class="outline-text-4" id="text-10-1-3">
<p>
The SSD framework uses a modified version of Szegedy et al.’s MultiBox algorithm (Scalable High Quality Object Detection) for bounding box proposals.<br  />
</p>

<p>
The MultiBox algorithm starts with priors. The priors are fixed size bounding boxes whose dimensions have been pre-computed based on the dimensions and locations of the ground-truth bounding boxes for each class in the dataset. We call these a "prior" as we’re relying on Bayesian statistical inference of where object locations will appear in an image. The priors are selected such that their Intersection over Union (IoU) is greater than 50% with ground-truth objects.<br  />
</p>


<p>
It turns out that this method of computing priors is better than randomly selecting coordinates from the input image; however, the problem is that we now need to pre-train the MultiBox predictor, undermining our goal of training a complete deep learning-based object detector end-to-end. The solution is fixed priors.<br  />
</p>



<div class="figure">
<p><img src="pics/c17_ssd2.png" alt="c17_ssd2.png" /><br  />
</p>
</div>

<p>
For each predicted bounding box we are also computing the probability of all class labels inside the region rather than keeping only the bounding box with the largest probability across all classes. Computing, and retaining, the probability for the bounding boxes in a class-wise manner enables us to detect potentially overlapping objects as well.<br  />
</p>
</div>
</div>

<div id="outline-container-sec-10-1-4" class="outline-4">
<h4 id="sec-10-1-4"><span class="section-number-4">10.1.4</span> Training</h4>
<div class="outline-text-4" id="text-10-1-4">
<p>
loss:<br  />
<img src="pics/c17_loss.png" alt="c17_loss.png" /><br  />
</p>

<p>
tradoff:<br  />
more default bounding boxes -&gt; increased accuracy, decreased speed<br  />
more CONV layers -&gt; increased accuracy, decreased speed<br  />
</p>


<p>
Hard negative mining: (cited)<br  />
</p>
<pre class="example">
After the matching step, most of the default boxes are nega
tives, especially when the number of possible default boxes is large. This introduces a
significant imbalance between the positive and negative training examples. Instead of
using all the negative examples, we sort them using the highest confidence loss for each
default box and pick the top ones so that the ratio between the negatives and positives is
at most 3:1. We found that this leads to faster optimization and a more stable training.
</pre>


<p>
Optimizer: (original)<br  />
SGD<br  />
</p>

<p>
During prediction time, non-maxima suppression is used class-wise, yielding the final predictions from the network.<br  />
</p>
</div>
</div>
</div>


<div id="outline-container-sec-10-2" class="outline-3">
<h3 id="sec-10-2"><span class="section-number-3">10.2</span> Summary</h3>
<div class="outline-text-3" id="text-10-2">
<p>
The primary criticism of SSDs is that they tend to not work well for small objects, mainly because small objects may not appear on all feature maps — the more an object appears on a feature map, the more likely that the MultiBox algorithm can detect it.<br  />
</p>

<p>
If you are trying to detect objects that are small relative to the size of the input image you should consider using Faster R-CNN instead.<br  />
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Created: 2019-12-09 Mon 15:27</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
