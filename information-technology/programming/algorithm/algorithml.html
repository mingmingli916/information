<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-03-05 Tue 23:04 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>algorithm</title>
<meta name="author" content="Mingming Li" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">algorithm</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org7efc2e7">1. <span class="done DONE">DONE</span> Preface</a></li>
<li><a href="#orga38b4c5">2. <span class="done DONE">DONE</span> Foundations</a>
<ul>
<li><a href="#org8318f5f">2.1. The Role of Algorithms in Computing</a>
<ul>
<li><a href="#orgaa3358e">2.1.1. what is algorithm?</a></li>
<li><a href="#orgd8d7245">2.1.2. instance of a problem</a></li>
<li><a href="#org973c9f5">2.1.3. correct algorithms</a></li>
<li><a href="#orgcd69ec3">2.1.4. problem list</a></li>
<li><a href="#orgc98bfec">2.1.5. two characteristics of many algorithms</a></li>
<li><a href="#org8b18306">2.1.6. data structure</a></li>
<li><a href="#org229472b">2.1.7. the core technique</a></li>
<li><a href="#org1c569b8">2.1.8. hard problems</a></li>
<li><a href="#orgaf29159">2.1.9. NP-complete problem</a></li>
<li><a href="#org92ff4e9">2.1.10. algorithm efficiency</a></li>
<li><a href="#org0f7cf5b">2.1.11. algorithms as a technology</a></li>
</ul>
</li>
<li><a href="#org932bbac">2.2. Getting Started</a>
<ul>
<li><a href="#org5889fe1">2.2.1. loop invariant</a></li>
<li><a href="#orgd76a1bb">2.2.2. psudocode conventions</a></li>
<li><a href="#org6f35b16">2.2.3. analyzing algorithms</a></li>
<li><a href="#orgeb6e4a9">2.2.4. model</a></li>
<li><a href="#org0dc7188">2.2.5. algorithm vs RAM</a></li>
<li><a href="#org54ec593">2.2.6. RAM model</a></li>
<li><a href="#orgf76c379">2.2.7. core idea in modeling</a></li>
<li><a href="#org91c4e54">2.2.8. analysis of a algorithm</a></li>
<li><a href="#org0699adc">2.2.9. worst-case analysis</a></li>
<li><a href="#org8806a44">2.2.10. abstraction</a></li>
<li><a href="#org2b79346">2.2.11. designing algorithms</a></li>
</ul>
</li>
<li><a href="#org615da87">2.3. Growth of Functions</a>
<ul>
<li><a href="#orgef986be">2.3.1. Asymptotic notation</a></li>
<li><a href="#orgc0e1e24">2.3.2. Standard notations and common functions</a></li>
</ul>
</li>
<li><a href="#org585b9f7">2.4. Divide-and-Conquer</a>
<ul>
<li><a href="#org1c39466">2.4.1. The master method for solving recurrences</a></li>
</ul>
</li>
<li><a href="#orgc01d836">2.5. Probabilistic Analysis and Randomized Algorithms</a>
<ul>
<li><a href="#orge776912">2.5.1. Indicator random variables</a></li>
<li><a href="#orgd416a67">2.5.2. Randomized alogrithms</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org45b786a">3. Sorting and Order Statistics</a>
<ul>
<li><a href="#orgecf9b6c">3.1. <span class="done DONE">DONE</span> Heapsort</a>
<ul>
<li><a href="#orgc782974">3.1.1. Heaps</a></li>
<li><a href="#org7be7e99">3.1.2. Maintaining the heap property</a></li>
<li><a href="#org22d97ca">3.1.3. Building a heap</a></li>
<li><a href="#orgd592394">3.1.4. The heapsort algorithm</a></li>
<li><a href="#org9d3a62d">3.1.5. Priority queues</a></li>
</ul>
</li>
<li><a href="#org2e87f9e">3.2. Quicksort</a></li>
<li><a href="#org07c80c8">3.3. Sorting in Linear Time</a>
<ul>
<li><a href="#org23bbe07">3.3.1. Counting sort</a></li>
<li><a href="#org53a3b56">3.3.2. Radix sort</a></li>
<li><a href="#org56a4caa">3.3.3. Bucket sort</a></li>
<li><a href="#orgfbfb79c">3.3.4. Medians and Order Statistics</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org057b81b">4. Data Structures</a>
<ul>
<li><a href="#orga3c6bb7">4.1. Elementary Data Structure</a>
<ul>
<li><a href="#org99f0703">4.1.1. Stacks and queues</a></li>
<li><a href="#orgbd435cb">4.1.2. Linked lists</a></li>
<li><a href="#org1b5c7b9">4.1.3. Representing rooted trees</a></li>
</ul>
</li>
<li><a href="#org4e42bb9">4.2. Hash Tables</a>
<ul>
<li><a href="#org3e61ff1">4.2.1. Direct-address tables</a></li>
<li><a href="#org60954fa">4.2.2. Hash Tables</a></li>
<li><a href="#org481d7e5">4.2.3. Hash Functions</a></li>
<li><a href="#orgcade9cc">4.2.4. Open addressing</a></li>
</ul>
</li>
<li><a href="#org8bd3179">4.3. Binary Search Trees</a>
<ul>
<li><a href="#org5bf5a7f">4.3.1. What is a binary search tree?</a></li>
<li><a href="#org77b1753">4.3.2. Querying a binary search time</a></li>
<li><a href="#org96a4989">4.3.3. Insertion and deletion</a></li>
</ul>
</li>
<li><a href="#org4ca21b2">4.4. Red-Black Trees</a>
<ul>
<li><a href="#org5a038d8">4.4.1. Properties of red-black trees</a></li>
<li><a href="#org5a63b57">4.4.2. Rotations</a></li>
<li><a href="#org538c670">4.4.3. Insertion</a></li>
<li><a href="#org6f8d8c9">4.4.4. Deletion</a></li>
</ul>
</li>
<li><a href="#org9fcddd7">4.5. Augmenting Data Structures</a>
<ul>
<li><a href="#org11e6fb8">4.5.1. Dynamic order statistics</a></li>
<li><a href="#org28f6dfb">4.5.2. How to augment a data structure</a></li>
<li><a href="#org0abb1d0">4.5.3. Interval trees</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org4a14ba7">5. Advanced Design and Analysis Techniques</a>
<ul>
<li><a href="#orgb43f21b">5.1. Dynamic Programming</a>
<ul>
<li><a href="#orga5ff064">5.1.1. Rod cutting</a></li>
<li><a href="#orgad8290a">5.1.2. Matrix-chain multiplication</a></li>
<li><a href="#org0208238">5.1.3. Elements of dynamic programming</a></li>
<li><a href="#org423dc18">5.1.4. Longest Common Subsequence</a></li>
<li><a href="#orgd15678f">5.1.5. Optimal binary search tress</a></li>
</ul>
</li>
<li><a href="#orgd6dffd9">5.2. Greedy Algorithms</a>
<ul>
<li><a href="#orge369bc2">5.2.1. An activity-selection problem</a></li>
<li><a href="#orgb0da82f">5.2.2. Element of the greedy strategy</a></li>
<li><a href="#org250cc92">5.2.3. Huffman codes</a></li>
</ul>
</li>
<li><a href="#orgc29a26f">5.3. Amortized Analysis</a>
<ul>
<li><a href="#org299a708">5.3.1. Aggregate analysis</a></li>
<li><a href="#org07be9dd">5.3.2. The accounting method</a></li>
<li><a href="#org479b343">5.3.3. The potential method</a></li>
<li><a href="#org113282d">5.3.4. Dynamic tables</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgb873635">6. Advanced Data Structure</a>
<ul>
<li><a href="#orge230218">6.1. B-Tree</a>
<ul>
<li><a href="#orgb22aaae">6.1.1. Definition of B-trees</a></li>
<li><a href="#orgfb34bcd">6.1.2. Basic operation on B-trees</a></li>
<li><a href="#orgde88cd3">6.1.3. Deleting a key from B-tree</a></li>
</ul>
</li>
<li><a href="#orgf0e0075">6.2. Fibonacci Heaps</a>
<ul>
<li><a href="#org6c66cd8">6.2.1. Structure of Fibonacci heaps</a></li>
<li><a href="#org7f7e992">6.2.2. Mergeable-heap-operations</a></li>
<li><a href="#org20dde18">6.2.3. Decreasing a key and deleting a node</a></li>
</ul>
</li>
<li><a href="#org0309565">6.3. van Emde Boas Tress</a>
<ul>
<li><a href="#org14b24e3">6.3.1. Preliminary approaches</a></li>
<li><a href="#orgd53ba0a">6.3.2. A recursive structure</a></li>
<li><a href="#org04fd270">6.3.3. The van Emde Boas tree</a></li>
</ul>
</li>
<li><a href="#orgcd678b0">6.4. Data Structure for Disjoint Sets</a>
<ul>
<li><a href="#org9913497">6.4.1. Disjoint-set operations</a></li>
<li><a href="#orgf4a81dc">6.4.2. Linked-list representation of disjoint sets</a></li>
<li><a href="#orgf8afb49">6.4.3. Disjoint-set forests</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org7c30f2d">7. Graph Algorithms</a>
<ul>
<li><a href="#org36e0a29">7.1. Elementary Graph Algorithms</a>
<ul>
<li><a href="#org28e0678">7.1.1. Representations of graphs</a></li>
<li><a href="#orge2e1cc6">7.1.2. Breadth-first search</a></li>
<li><a href="#org9a035a3">7.1.3. Depth-first search</a></li>
<li><a href="#org8a1645c">7.1.4. Topological sort</a></li>
<li><a href="#org80b1856">7.1.5. Strongly connected components</a></li>
</ul>
</li>
<li><a href="#org27d527f">7.2. Minimum Spanning Tress</a>
<ul>
<li><a href="#org658d1d3">7.2.1. Growing a minimum spanning tree</a></li>
<li><a href="#orgc5c953f">7.2.2. The algorithms of Kruskal and Prim</a></li>
</ul>
</li>
<li><a href="#orgc27ede2">7.3. Single-Source Shortest Paths</a>
<ul>
<li><a href="#org68c15ad">7.3.1. The Bellman-Ford algorithm</a></li>
<li><a href="#org8d7680a">7.3.2. Single-source shortest paths in directed acyclic graphs</a></li>
<li><a href="#org5796959">7.3.3. Dijkstra's algorithm</a></li>
<li><a href="#org4916669">7.3.4. Difference constraints and shortest paths</a></li>
</ul>
</li>
<li><a href="#org30f2efa">7.4. All-Pair Shortest Paths</a>
<ul>
<li><a href="#org1030c3b">7.4.1. Shortest paths and matrix multiplication</a></li>
<li><a href="#orgfc32d3b">7.4.2. The Floyd-Warshall algorithm (based on vertices)</a></li>
<li><a href="#orgb0704f7">7.4.3. Johnson's algorithm for sparse graphs</a></li>
</ul>
</li>
<li><a href="#orgb015d29">7.5. Maximum Flow</a>
<ul>
<li><a href="#org4287a92">7.5.1. Flow networks</a></li>
<li><a href="#org49d945b">7.5.2. The Ford-Fulkerson method</a></li>
<li><a href="#org50091fc">7.5.3. Maximum bipartite matching</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org6a9f022">8. Selected Topics</a>
<ul>
<li><a href="#orga1e919f">8.1. Multithreaded Algorithms</a>
<ul>
<li><a href="#org8d05dc3">8.1.1. The basics of dynamic multithreading</a></li>
<li><a href="#orgb1ee2ad">8.1.2. Multithreaded matrix multiplication</a></li>
<li><a href="#org9536363">8.1.3. Multithreaded merge sort</a></li>
<li><a href="#org8eeffbc">8.1.4. Analysis of multithreaded merging</a></li>
<li><a href="#org8d807a2">8.1.5. Multithreaded merge sort</a></li>
<li><a href="#org8cb5074">8.1.6. Analysis of multithreaded merge sort</a></li>
</ul>
</li>
<li><a href="#org5f6747f">8.2. Matrix Operations</a>
<ul>
<li><a href="#org11c13df">8.2.1. Solving systems of linear equations</a></li>
<li><a href="#org02bf121">8.2.2. Inverting matrics</a></li>
</ul>
</li>
<li><a href="#orgdd178f4">8.3. Polynomials and FFT</a>
<ul>
<li><a href="#org05876db">8.3.1. Representing polynomials</a></li>
<li><a href="#orgadb8bbc">8.3.2. The DFT and FFT</a></li>
</ul>
</li>
<li><a href="#orgd05fdf4">8.4. Number-Theoretic Algorithms</a>
<ul>
<li><a href="#orgf0685a6">8.4.1. Elementary number-theoretic notions</a></li>
<li><a href="#orgb0bbeed">8.4.2. Greatest common divisor</a></li>
<li><a href="#orgedeb458">8.4.3. Modular arithmetic</a></li>
<li><a href="#org76c90d1">8.4.4. Sovling modular linear equations</a></li>
<li><a href="#org7afbf8b">8.4.5. <span class="todo TODO">TODO</span> The Chinese remainder theorem</a></li>
<li><a href="#org625bdf8">8.4.6. Power of an element</a></li>
<li><a href="#org8740ead">8.4.7. The RSA public-key cryptosystem</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>


<div id="outline-container-org7efc2e7" class="outline-2">
<h2 id="org7efc2e7"><span class="section-number-2">1.</span> <span class="done DONE">DONE</span> Preface</h2>
<div class="outline-text-2" id="text-1">
<p>
Algorithms lies at the heart of computering.<br />
</p>

<p>
Website: <a href="http://mitpress.mit.edu/algorithms/">http://mitpress.mit.edu/algorithms/</a><br />
</p>

<p>
Psudocode: present algorithm clearly and succintly, without idiosyncrasies of a particular programming language obscuring the its essence.<br />
</p>
</div>
</div>

<div id="outline-container-orga38b4c5" class="outline-2">
<h2 id="orga38b4c5"><span class="section-number-2">2.</span> <span class="done DONE">DONE</span> Foundations</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org8318f5f" class="outline-3">
<h3 id="org8318f5f"><span class="section-number-3">2.1.</span> The Role of Algorithms in Computing</h3>
<div class="outline-text-3" id="text-2-1">
</div>
<div id="outline-container-orgaa3358e" class="outline-4">
<h4 id="orgaa3358e"><span class="section-number-4">2.1.1.</span> what is algorithm?</h4>
<div class="outline-text-4" id="text-2-1-1">
<p>
input -&gt; algorithm -&gt; output<br />
An algorithm is a sequence of computational steps that transform the input into the output.<br />
An algorithm describes a specific computational procedure for achieving the input/output relationship.<br />
</p>
</div>
</div>

<div id="outline-container-orgd8d7245" class="outline-4">
<h4 id="orgd8d7245"><span class="section-number-4">2.1.2.</span> instance of a problem</h4>
<div class="outline-text-4" id="text-2-1-2">
<p>
the input needed to compute a solution to the problem.<br />
</p>
</div>
</div>

<div id="outline-container-org973c9f5" class="outline-4">
<h4 id="org973c9f5"><span class="section-number-4">2.1.3.</span> correct algorithms</h4>
<div class="outline-text-4" id="text-2-1-3">
<p>
For every input instance, it halts out the correct output.<br />
</p>
</div>
</div>

<div id="outline-container-orgcd69ec3" class="outline-4">
<h4 id="orgcd69ec3"><span class="section-number-4">2.1.4.</span> problem list</h4>
<div class="outline-text-4" id="text-2-1-4">
<ul class="org-ul">
<li>Internet, finding good routes and finding pages<br /></li>
<li>Electronic commerce, public-key cryptography and digital singatures<br /></li>
<li>oil company, where to place its wells in order to maximize its profit, linear programming<br /></li>
<li>road map, shortest route<br /></li>
<li>two ordered sequences, find a longest common subsequence<br /></li>
<li>a mechanical design, each part may include instances of other parts, to list the parts in order so that each part appears before any part that uses it, topological sorting<br /></li>
<li>convex hull<br /></li>
</ul>
</div>
</div>

<div id="outline-container-orgc98bfec" class="outline-4">
<h4 id="orgc98bfec"><span class="section-number-4">2.1.5.</span> two characteristics of many algorithms</h4>
<div class="outline-text-4" id="text-2-1-5">
<ol class="org-ol">
<li>There are many candidate solutions, but finding the one that solve or the one is best is challenge.<br /></li>
<li>They have practical applications.<br /></li>
</ol>
</div>
</div>

<div id="outline-container-org8b18306" class="outline-4">
<h4 id="org8b18306"><span class="section-number-4">2.1.6.</span> data structure</h4>
<div class="outline-text-4" id="text-2-1-6">
<p>
a way to store and organize data in order to facilitate access and modifications.<br />
</p>

<p>
No single data structure works well for all purposes, and it is important to know the strengths and limitations of several of them.<br />
</p>
</div>
</div>

<div id="outline-container-org229472b" class="outline-4">
<h4 id="org229472b"><span class="section-number-4">2.1.7.</span> the core technique</h4>
<div class="outline-text-4" id="text-2-1-7">
<p>
learn the technique of algorithm design and analysis.<br />
</p>
</div>
</div>

<div id="outline-container-org1c569b8" class="outline-4">
<h4 id="org1c569b8"><span class="section-number-4">2.1.8.</span> hard problems</h4>
<div class="outline-text-4" id="text-2-1-8">
<p>
Like the NP-complete problem, there are problem that has no efficient solutions.<br />
Before you delve into the real problem, take a overview of it.<br />
</p>
</div>
</div>

<div id="outline-container-orgaf29159" class="outline-4">
<h4 id="orgaf29159"><span class="section-number-4">2.1.9.</span> NP-complete problem</h4>
<div class="outline-text-4" id="text-2-1-9">
<ol class="org-ol">
<li>no one knows whether or not efficient algorithms exist.<br /></li>
<li>a solution for one NP-complete probelm will works for all NP-complete problems<br /></li>
<li>several NP-complete problems are similar, but not identical to problems for which we do know of efficient algorithms.<br /></li>
</ol>
</div>
</div>

<div id="outline-container-org92ff4e9" class="outline-4">
<h4 id="org92ff4e9"><span class="section-number-4">2.1.10.</span> algorithm efficiency</h4>
<div class="outline-text-4" id="text-2-1-10">
<p>
Computers are not infinitely fast and memory is not free, thus the efficiency of a algorithm matters.<br />
<span class="timestamp-wrapper"><span class="timestamp">[2019-06-21 Fri]</span></span><br />
</p>
</div>
</div>
<div id="outline-container-org0f7cf5b" class="outline-4">
<h4 id="org0f7cf5b"><span class="section-number-4">2.1.11.</span> algorithms as a technology</h4>
<div class="outline-text-4" id="text-2-1-11">
<p>
Algorithms are at the core of most technologies.<br />
</p>


<p>
<span class="timestamp-wrapper"><span class="timestamp">[2019-06-22 Sat]</span></span><br />
</p>
</div>
</div>
</div>
<div id="outline-container-org932bbac" class="outline-3">
<h3 id="org932bbac"><span class="section-number-3">2.2.</span> Getting Started</h3>
<div class="outline-text-3" id="text-2-2">
</div>
<div id="outline-container-org5889fe1" class="outline-4">
<h4 id="org5889fe1"><span class="section-number-4">2.2.1.</span> loop invariant</h4>
<div class="outline-text-4" id="text-2-2-1">
<p>
Loop invariant is used to help us to understand why an algorithm is correct.<br />
</p>

<p>
The comparison of loop invariant and mathematical induction<br />
</p>
<!-- This HTML table template is generated by emacs 29.1 -->
<table border="1">
  <tr>
    <td align="left" valign="top">
      &nbsp;&nbsp;&nbsp;Loop&nbsp;Invariant&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
    <td align="left" valign="top">
      &nbsp;Mathematical&nbsp;Induction&nbsp;
    </td>
  </tr>
  <tr>
    <td align="left" valign="top">
      initialization&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
    <td align="left" valign="top">
      base&nbsp;case&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
  </tr>
  <tr>
    <td align="left" valign="top">
      maintenance&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
    <td align="left" valign="top">
      inductive&nbsp;step&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
  </tr>
  <tr>
    <td align="left" valign="top">
      termination&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
    <td align="left" valign="top">
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
  </tr>
</table>

<p>
Initialization: It is true prior to the first iteration of the loop.<br />
Maintanance:    If it is true before an iteration of the loop, it remains true before the next iteration.<br />
Termination:    When the loop terminates, the invariant gives us a useful property that helps show that the algorithm is correct.<br />
</p>
</div>
</div>
<div id="outline-container-orgd76a1bb" class="outline-4">
<h4 id="orgd76a1bb"><span class="section-number-4">2.2.2.</span> psudocode conventions</h4>
<div class="outline-text-4" id="text-2-2-2">
<pre class="example" id="orgd8bdefc">
INSERTION-SORT(T)

for j = 2 to A.length
    key = A[j]
    // Insert A[j] into the sorted sequence A[1..j-1].
    i = j - 1
    while i &gt; 0 and A[i] &gt; key
        // in place sort
        A[i + 1] = A[i]
	i = i - 1
    // when the loop terminate, i = 0
    A[i + 1] = key
</pre>


<ol class="org-ol">
<li>Indentation indicates block structures.<br /></li>
<li>A loop counter retains its value after exiting the loop. (deffer from C++, Java&#x2026;)<br /></li>
<li>Variable are local to the given procedure.<br /></li>
<li>The keyword <b>error</b> indicates that an error occurred.<br /></li>
</ol>
</div>
</div>




<div id="outline-container-org6f35b16" class="outline-4">
<h4 id="org6f35b16"><span class="section-number-4">2.2.3.</span> analyzing algorithms</h4>
<div class="outline-text-4" id="text-2-2-3">
<p>
analyzing an algorithm: predict the resources.<br />
resources: time and space (memory, communication bandwidth, computer hardware, computational time&#x2026;)<br />
</p>
</div>
</div>

<div id="outline-container-orgeb6e4a9" class="outline-4">
<h4 id="orgeb6e4a9"><span class="section-number-4">2.2.4.</span> model</h4>
<div class="outline-text-4" id="text-2-2-4">
<p>
Before analyzing an algorithm, there must be a model to measure the resource cost.<br />
</p>
</div>
</div>

<div id="outline-container-org0dc7188" class="outline-4">
<h4 id="org0dc7188"><span class="section-number-4">2.2.5.</span> algorithm vs RAM</h4>
<div class="outline-text-4" id="text-2-2-5">
<p>
The focus is algorithm, not the tedious hardware detail.<br />
To yield a clear insight into algorithm design and analysis, RAM model is simplified.<br />
</p>
</div>
</div>

<div id="outline-container-org54ec593" class="outline-4">
<h4 id="org54ec593"><span class="section-number-4">2.2.6.</span> RAM model</h4>
<div class="outline-text-4" id="text-2-2-6">
<!-- This HTML table template is generated by emacs 29.1 -->
<table border="1">
  <tr>
    <td rowspan="2" align="left" valign="top">
      instructions&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
    <td align="left" valign="top">
      arithmetic&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
    <td align="left" valign="top">
      movement&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
    <td align="left" valign="top">
      control&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
    <td rowspan="2" align="left" valign="top">
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      each&nbsp;instructions&nbsp;takes&nbsp;<br />
      a&nbsp;constant&nbsp;amount&nbsp;of&nbsp;&nbsp;&nbsp;&nbsp;<br />
      time&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
  </tr>
  <tr>
    <td align="left" valign="top">
      add,&nbsp;abstruct,&nbsp;&nbsp;<br />
      multiply,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      divide,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      remainder,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      floor,&nbsp;ceiling&nbsp;&nbsp;<br />
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
    <td align="left" valign="top">
      load,&nbsp;store&nbsp;copy<br />
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
    <td align="left" valign="top">
      conditional&nbsp;and&nbsp;<br />
      unconditional&nbsp;&nbsp;&nbsp;<br />
      branch,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      subroutine&nbsp;call,<br />
      return&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
  </tr>
  <tr>
    <td align="left" valign="top">
      data&nbsp;types&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
    <td colspan="3" align="left" valign="top">
      integer,&nbsp;floating&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
    <td align="left" valign="top">
      represented&nbsp;by&nbsp;clgn&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      bits&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
  </tr>
  <tr>
    <td align="left" valign="top">
      memory&nbsp;hierarchy&nbsp;
    </td>
    <td colspan="3" align="left" valign="top">
      do&nbsp;not&nbsp;model&nbsp;caches&nbsp;or&nbsp;virtual&nbsp;memory&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
    <td align="left" valign="top">
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
  </tr>
</table>

<p>
\(c\lg n\) explaination:<br />
</p>
<ul class="org-ul">
<li>\(\lg\) means \(\log_2\)<br /></li>
<li>2 as root because of the binary system<br /></li>
<li>\(c\ge1\) : each word can hold the value of n<br /></li>
<li>\(c\) to a constant: the word size does not grow arbitrarily<br /></li>
</ul>
</div>
</div>

<div id="outline-container-orgf76c379" class="outline-4">
<h4 id="orgf76c379"><span class="section-number-4">2.2.7.</span> core idea in modeling</h4>
<div class="outline-text-4" id="text-2-2-7">
<p>
show the important characteristcs of algorithms and suppress the tedious details.<br />
</p>
</div>
</div>



<div id="outline-container-org91c4e54" class="outline-4">
<h4 id="org91c4e54"><span class="section-number-4">2.2.8.</span> analysis of a algorithm</h4>
<div class="outline-text-4" id="text-2-2-8">
<p>
In general, the time grows with the size of the input,<br />
so it is traditional to describe the running time as the function of the size of its input.<br />
</p>

<!-- This HTML table template is generated by emacs 29.1 -->
<table border="1">
  <tr>
    <td align="left" valign="top">
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
    <td align="left" valign="top">
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
    <td align="left" valign="top">
      Examples&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
  </tr>
  <tr>
    <td align="left" valign="top">
      input&nbsp;size&nbsp;&nbsp;&nbsp;
    </td>
    <td align="left" valign="top">
      depends&nbsp;on&nbsp;the&nbsp;problem&nbsp;being&nbsp;studied&nbsp;
    </td>
    <td align="left" valign="top">
      number&nbsp;of&nbsp;items,&nbsp;total&nbsp;number&nbsp;of&nbsp;bits&nbsp;...&nbsp;
    </td>
  </tr>
  <tr>
    <td align="left" valign="top">
      running&nbsp;time&nbsp;
    </td>
    <td align="left" valign="top">
      the&nbsp;number&nbsp;of&nbsp;primitive&nbsp;operations&nbsp;&nbsp;&nbsp;
    </td>
    <td align="left" valign="top">
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
  </tr>
</table>

<p>
Assumption for simpler analysis:<br />
A constant amount of time is requried to execute each line of the pseudocode.<br />
</p>
</div>
</div>



<div id="outline-container-org0699adc" class="outline-4">
<h4 id="org0699adc"><span class="section-number-4">2.2.9.</span> worst-case analysis</h4>
<div class="outline-text-4" id="text-2-2-9">
<p>
Becuase the behavior of an algorithm may be different for each possible input,<br />
we need a means for summarizing that behavior in simple, easily understood formulas.<br />
</p>


<p>
the reason to analyze worst-case running time:<br />
</p>
<ol class="org-ol">
<li>give an upper bound on the running time<br /></li>
<li>worst case ocurrs fairly often<br /></li>
<li>the "average case" is often roughly as bad as the worst case<br /></li>
</ol>
</div>
</div>

<div id="outline-container-org8806a44" class="outline-4">
<h4 id="org8806a44"><span class="section-number-4">2.2.10.</span> abstraction</h4>
<div class="outline-text-4" id="text-2-2-10">
<p>
Use some simplifying abstraction to ease the analysis.<br />
</p>
<ol class="org-ol">
<li>ignore the actual cost of each statement, using the constants \(c_i\) to represent these costs.<br /></li>
<li>ignore the abstract costs \(c_i\) ( \(an^2 + bn + c\) )<br /></li>
<li>rate of growth or order of growth of the running time ( \(\Theta(n^2)\) ) (pronounced "theta of n-squared")<br /></li>
</ol>
</div>
</div>


<div id="outline-container-org2b79346" class="outline-4">
<h4 id="org2b79346"><span class="section-number-4">2.2.11.</span> designing algorithms</h4>
<div class="outline-text-4" id="text-2-2-11">
</div>
<ol class="org-ol">
<li><a id="org4bc98da"></a>incremental approch<br />
<div class="outline-text-5" id="text-2-2-11-1">
<p>
example: insertion-sort<br />
</p>
</div>
</li>
<li><a id="orgfad282a"></a>devide-and-conquer approch<br />
<div class="outline-text-5" id="text-2-2-11-2">
<p>
example: like merge-sort<br />
</p>

<ol class="org-ol">
<li>divide the problem into a number of subproblems<br /></li>
<li>conquer the subproblems<br /></li>
<li>combine the solution<br /></li>
</ol>
</div>
</li>
</ol>
</div>
</div>


<div id="outline-container-org615da87" class="outline-3">
<h3 id="org615da87"><span class="section-number-3">2.3.</span> Growth of Functions</h3>
<div class="outline-text-3" id="text-2-3">
<p>
Althoght we can sometimes determine the exact running time of an algorithm, the extra procision is not usually worth the effort of computing it.<br />
</p>


<p>
When we look at input sizes large enought to make only the order of growth of the running time relevant, we are studying the <code>asymptotic efficiency of algorithms</code>.<br />
</p>
</div>
<div id="outline-container-orgef986be" class="outline-4">
<h4 id="orgef986be"><span class="section-number-4">2.3.1.</span> Asymptotic notation</h4>
<div class="outline-text-4" id="text-2-3-1">
</div>
<ol class="org-ol">
<li><a id="org57179aa"></a>$&Theta;$-notation<br />
<div class="outline-text-5" id="text-2-3-1-1">
\begin{equation}

\Theta(g(n)) = \{f(n): \text{there exist positive constant}\ c_1, c_2 \text{and} \ n_0 \text{such that} \
0 \le c_1 g(n) \le f(n) \le c_2 g(n)\  \text{for all} \ n \ge n_0 \}
\end{equation}

<p>
Because \(\Theta(g(n))\) is a set, we could write "\(f(n) \in \Theta(g(n))\) " to indicate that \(f(n)\) is a member of \(\Theta(g(n))\) . Instead, we will usually write "\(f(n) = \Theta(g(n))\) " to express the same notion.<br />
</p>

<p>
Since any constant is a degree-0 polynomial, we can express any constant function as \(\Theta(n^0)\) , or \(\Theta(1)\) .<br />
</p>
</div>
</li>
<li><a id="org0e7514e"></a>O-notation<br />
<div class="outline-text-5" id="text-2-3-1-2">
\begin{equation}
O(g(n)) = \{f(n): \text{there exist positive constants} \ c \text{and} \ n_0 \ \text{such that} \
0 \le f(n) \le cg(n) \ \text{for all} \ n \ge n_0 \}
\end{equation}

<p>
O-notation to the worst case ==&gt; to every input<br />
$&Theta;$-notation to the worst case =/=&gt; to every input<br />
</p>
</div>
</li>
<li><a id="org3365d2f"></a>$&Omega;$-notation<br />
<div class="outline-text-5" id="text-2-3-1-3">
\begin{equation}
\Omega(g(n)) = \{f(n): \text{there exist positive constants} \ c \text{and} \ n_0 \ \text{such that} \
0 \le cg(n) \le f(n) \ \text{for all} \ n \ge n_0 \}
\end{equation}
</div>
</li>
<li><a id="org82ac078"></a>Theorem<br />
<div class="outline-text-5" id="text-2-3-1-4">
<p>
For any two functions \(f(n)\) and \(g(n)\), we have \(f(n) = \Theta(g(n))\) if and only if<br />
\(f(n) = O(g(n))\) and \(f(n) = \Omega(g(n))\)<br />
</p>
</div>
</li>
<li><a id="org2bb00e5"></a>o-notation<br />
<div class="outline-text-5" id="text-2-3-1-5">
<p>
an upper bound that is not asymptotically tight.<br />
</p>

\begin{equation}
o(g(n)) = \{f(n): \text{for any positive constant} \ c > 0, \text{there exist a constant} \ n_0 > 0 \ \text{such that} \
0 \le f(n) < cg(n) \ \text{for all} \ n \ge n_0 \}
\end{equation}


<p>
or<br />
</p>
\begin{equation}
\lim_{n\rightarrow \infty}\frac{f(n)}{g(n)} = 0
\end{equation}
</div>
</li>

<li><a id="org9629347"></a>$&omega;$-notation<br />
<div class="outline-text-5" id="text-2-3-1-6">
<p>
a lower bound that is not asymptotically tight.<br />
</p>

\begin{equation}
\omega(g(n)) = \{f(n): \text{for any positive constant} \ c > 0, \text{there exist a constant} \ n_0 > 0 \ \text{such that} \
0 \le cg(n) < f(n) \ \text{for all} \ n \ge n_0 \}
\end{equation}


<p>
or<br />
</p>
\begin{equation}
\lim_{n\rightarrow \infty}\frac{f(n)}{g(n)} = \infty
\end{equation}
</div>
</li>

<li><a id="orgc809a97"></a>comparing functions<br />
<ol class="org-ol">
<li><a id="orgebee774"></a>Transitivity<br />
<div class="outline-text-6" id="text-2-3-1-7-1">
<p>
\[
f(n) = \Theta(g(n)) \quad \text{and} \quad g(n) = \Theta(h(n)) \Rightarrow f(n) = \Theta(h(n))
\]<br />
The same to \(O, \Omega, o, \omega\) .<br />
</p>
</div>
</li>

<li><a id="org191df8a"></a>Reflexivity<br />
<div class="outline-text-6" id="text-2-3-1-7-2">
<p>
\[f(n) = \Theta(f(n))\]<br />
\[f(n) = O(f(n)) \]<br />
\[f(n) = \Omega(f(n)) \]<br />
</p>
</div>
</li>

<li><a id="org726037c"></a>Symmetry<br />
<div class="outline-text-6" id="text-2-3-1-7-3">
<p>
\[ f(n) = \Theta(g(n)) \quad \text{if and only if} \quad g(n) = \Theta(f(n)) \]<br />
</p>
</div>
</li>
<li><a id="org9fdeb3e"></a>Transpose symmetry<br />
<div class="outline-text-6" id="text-2-3-1-7-4">
<p>
\[ f(n) = O(g(n)) \quad \text{if and only if} \quad g(n) = \Omega(f(n)) \]<br />
\[ f(n) = o(g(n)) \quad \text{if and only if} \quad g(n) = \omega(f(n)) \]<br />
</p>
</div>
</li>
</ol>
</li>
</ol>
</div>
<div id="outline-container-orgc0e1e24" class="outline-4">
<h4 id="orgc0e1e24"><span class="section-number-4">2.3.2.</span> Standard notations and common functions</h4>
</div>
</div>
<div id="outline-container-org585b9f7" class="outline-3">
<h3 id="org585b9f7"><span class="section-number-3">2.4.</span> Divide-and-Conquer</h3>
<div class="outline-text-3" id="text-2-4">
<p>
A recurrence is an equation or inequality that describes a function in terms of its value on smaller inputs.<br />
</p>

<p>
For example:<br />
</p>
\begin{equation}
T(n)=
\begin{cases}
\Theta(1) & \mathrm{if} \quad n=1 \\
2T(n/2) + \Theta(n) & \mathrm{if} \quad n > 1
\end{cases}
\end{equation}
</div>

<div id="outline-container-org1c39466" class="outline-4">
<h4 id="org1c39466"><span class="section-number-4">2.4.1.</span> The master method for solving recurrences</h4>
<div class="outline-text-4" id="text-2-4-1">
\begin{equation}
T(n)=aT(n/b)+f(n)
\end{equation}
<p>
where \(a\ge 1\) and \(b>1\) are constants and f(n) is an asymptotically positive function.<br />
</p>

<p>
Then \(T(n)\) has the following asymptotic bounds:<br />
</p>
<ol class="org-ol">
<li>If \(f(n) = O(n^{\log_b(a-\epsilon)})\) for some constant \(\epsilon>0\), then \(T(n)=\Theta(n^{\log_ba})\).<br /></li>
<li>If \(f(n) = \Theta(n^{\log_ba})\), then \(T(n)=\Theta(n^{\log_ba}\lg n)\).<br /></li>
<li>If \(f(n) = O(n^{\log_b(a+\epsilon)})\) for some constant \(\epsilon>0\), and if \(af(n/b)\le cf(n)\) for some constant \(c<1\) and all sufficiently large \(n\), then \(T(n)=\Theta(f(n))\).<br /></li>
</ol>

<p>
Intuitively, the larger of the two functions determines the solution to the recurrence.<br />
</p>


<p>
Note:<br />
Beyond this intuition, you need to be aware of some technicalities. In the first case, not only must \(f(n)\) be smaller than \(n^{\log_ba}\) , it must be polynomially smaller. In the third case, not only must \(f(n)\) be larger than \(n^{\log_ba}\) , it also must be polynomially larger and in addition satisfy the "regularity" condition that \(af(n/b)\le cf(n)\)<br />
</p>
</div>
</div>
</div>

<div id="outline-container-orgc01d836" class="outline-3">
<h3 id="orgc01d836"><span class="section-number-3">2.5.</span> Probabilistic Analysis and Randomized Algorithms</h3>
<div class="outline-text-3" id="text-2-5">
<p>
Probabilistic analysis is the use of probability in the analysis of problems.<br />
</p>

<p>
In order to perform a probabilistic analysis, we must use knowledge of, or make assumptions about, the distribution of the inputs. Then we analyze our algorithm, computing an average-case running time, where we take the average over the distribution of the possible inputs. Thus we are, in effect, averaging the running time over all possible inputs. When reporting such a running time, we will refer to it as the average-case running time.<br />
</p>

<p>
we call an algorithm randomized if its behavior is determined not only by its input but also by values produced by a random-number generator.<br />
</p>

<p>
In general, we discuss the average-case running time when the probability distribution is over the inputs to the algorithm, and we discuss the expected running time when the algorithm itself makes random choices.<br />
</p>
</div>

<div id="outline-container-orge776912" class="outline-4">
<h4 id="orge776912"><span class="section-number-4">2.5.1.</span> Indicator random variables</h4>
<div class="outline-text-4" id="text-2-5-1">
<p>
In order to analyze many algorithms, we use indicator random variables. Indicator random variables provide a convenient method for converting between probabilities and expectations.<br />
Suppose we are given a sample space S and an event A. Then the indicator random variable \(I\{A\}\) associated with event A is defined as<br />
</p>
\begin{equation}
I\{A\}=
\begin{cases}
1 \quad \mathrm{if} \ A \ \mathrm{occurs} \\
0 \quad \mathrm{if} \ A \ \mathrm{does\ not\ occurs}
\end{cases}
\end{equation}

<p>
Lemma:<br />
Given a sample space S and an event A in the sample space S, let \(X_A = I\{A\}\). Then \(E[X_A]=Pr\{A\}\).<br />
</p>
</div>
</div>

<div id="outline-container-orgd416a67" class="outline-4">
<h4 id="orgd416a67"><span class="section-number-4">2.5.2.</span> Randomized alogrithms</h4>
<div class="outline-text-4" id="text-2-5-2">
<p>
Most of the time, we do not have knowledge in the input distribution. Instead of assuming a distribution of inputs, we impose a distribution.<br />
</p>
</div>

<ol class="org-ol">
<li><a id="orgc82bedb"></a>Randomly permuting arrays<br />
<div class="outline-text-5" id="text-2-5-2-1">
<pre class="example" id="org369e2a6">
PERMUTE-BY-SORTING(A)
let P[1..n] be a new array
for i = 1 to n
    P[i] = RANDOM(1, n^3)  # priority
sort A, using P as sort keys
</pre>

<pre class="example" id="org8b574a8">
RANDOMIZE-IN-PLACE(A)
n = A.length
for i = 1 to n
    swap A[i] with A[RANDOM(i, n)]
</pre>
</div>
</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-org45b786a" class="outline-2">
<h2 id="org45b786a"><span class="section-number-2">3.</span> Sorting and Order Statistics</h2>
<div class="outline-text-2" id="text-3">
<p>
sorting problem:<br />
</p>

<p>
Input: A sequence of n numbers \(a_1, a_2, ..., a_n\).<br />
Output: A permutation (reordering) \(a'_1,a_2',...,a_n'\) of the input sequence such that \(a_1' \le a_2' \le ... \le a_n'\).<br />
</p>

<p>
In practice, the numbers to be sorted are rarely isolated values. Each is usually part of a collection of data called a record. Each record contains a key, which is the value to be sorted. The remainder of the record consists of satellite data, which are usually carried around with the key.<br />
</p>
</div>

<div id="outline-container-orgecf9b6c" class="outline-3">
<h3 id="orgecf9b6c"><span class="section-number-3">3.1.</span> <span class="done DONE">DONE</span> Heapsort</h3>
<div class="outline-text-3" id="text-3-1">
</div>
<div id="outline-container-orgc782974" class="outline-4">
<h4 id="orgc782974"><span class="section-number-4">3.1.1.</span> Heaps</h4>
<div class="outline-text-4" id="text-3-1-1">
<p>
The (binary) heap data stucture is an array object that can view as nearly complete tree. Each node of the tree corresponds to an element of the array. The tree is completely filled on all levels except possibly the lowest, which is filled from the left up to a point.<br />
</p>

<p>
An array A that represents a heap is an object with two attributes:<br />
</p>
<ol class="org-ol">
<li>A.length, which gives the number of elements in the array<br /></li>
<li>A.heap-size, which represents how many elements in the heap are stored with array A<br /></li>
</ol>


<div id="org297285f" class="figure">
<p><img src="images/c6_heap.png" alt="c6_heap.png" /><br />
</p>
</div>

<p>
There are two kinds of binary heaps: max-heaps and min-heaps. In both kinds, the values in the nodes satisfy a heap property.<br />
</p>

<p>
In a max-heap, the max-heap property is that for every node i other than root, \(A[PARENT(i)] \ge A[i]\).<br />
In a min-heap, \(A[PARENT(i)] \le A[i]\).<br />
</p>
</div>
</div>

<div id="outline-container-org7be7e99" class="outline-4">
<h4 id="org7be7e99"><span class="section-number-4">3.1.2.</span> Maintaining the heap property</h4>
<div class="outline-text-4" id="text-3-1-2">
<p>
In order to maintain the max-heap property, we call the procedure MAX-HEAPIFY. Its inputs are an array A and an index i into the array. When it is called, MAX-HEAPIFY assumes that the binary trees rooted at LEFT(i) and RIGHT(i) are max-heaps, but that A[i] be smaller than its children, thus violating the max-heap property.<br />
</p>


<div id="org0fe48fb" class="figure">
<p><img src="images/c6_max_heapify.png" alt="c6_max_heapify.png" /><br />
</p>
</div>


<div id="orgc983943" class="figure">
<p><img src="images/c6_max_heapify_fig.png" alt="c6_max_heapify_fig.png" /><br />
</p>
</div>
</div>
</div>

<div id="outline-container-org22d97ca" class="outline-4">
<h4 id="org22d97ca"><span class="section-number-4">3.1.3.</span> Building a heap</h4>
<div class="outline-text-4" id="text-3-1-3">

<div id="org2affc5a" class="figure">
<p><img src="images/c6_build_max_heap.png" alt="c6_build_max_heap.png" /><br />
</p>
</div>
</div>
</div>


<div id="outline-container-orgd592394" class="outline-4">
<h4 id="orgd592394"><span class="section-number-4">3.1.4.</span> The heapsort algorithm</h4>
<div class="outline-text-4" id="text-3-1-4">

<div id="org588087d" class="figure">
<p><img src="images/c6_build_max_heap_fig.png" alt="c6_build_max_heap_fig.png" /><br />
</p>
</div>



<div id="org704ac7e" class="figure">
<p><img src="images/c6_heapsort.png" alt="c6_heapsort.png" /><br />
</p>
</div>


<div id="orgbfb74d5" class="figure">
<p><img src="images/c6_heapsort_fig.png" alt="c6_heapsort_fig.png" /><br />
</p>
</div>

<pre class="example" id="org68827d2">
Similar bubble sort, but reducing the number of comparison.
</pre>
</div>
</div>
<div id="outline-container-org9d3a62d" class="outline-4">
<h4 id="org9d3a62d"><span class="section-number-4">3.1.5.</span> Priority queues</h4>
<div class="outline-text-4" id="text-3-1-5">
<p>
A priority queue is a data structure for maintaining a set S of elements, each with an associated value called a <b>key</b>.<br />
</p>

<p>
A max-priority queue supports the following operations:<br />
</p>
<ul class="org-ul">
<li>INSERT(S,x) inserts the elements x into the set S, which is equivalent to the operations \(S=S\cup \{x\}\).<br /></li>
<li>MAXIMUM(S) returns the element of S with the largest key.<br /></li>
<li>EXTRACT-MAX(S) removes and returns the element of S with the largest key.<br /></li>
<li>INCREASE-KEY(S,x,k) increase the value of element x's key to the new value k, which is assumed to be at least as large as x's current key value.<br /></li>
</ul>


<p>
When we use a heap to implement a priority queue, therefore, we often need to store a handle to the corresponding application object in each heap element.<br />
</p>



<div id="orga8d0667" class="figure">
<p><img src="images/c6_heap_maximum.png" alt="c6_heap_maximum.png" /><br />
</p>
</div>


<div id="org3ea3c0c" class="figure">
<p><img src="images/c6_heap_extract_max.png" alt="c6_heap_extract_max.png" /><br />
</p>
</div>


<div id="org6a0b1cd" class="figure">
<p><img src="images/c6_heap_increase_key.png" alt="c6_heap_increase_key.png" /><br />
</p>
</div>


<div id="org209531f" class="figure">
<p><img src="images/c6_max_heap_insert.png" alt="c6_max_heap_insert.png" /><br />
</p>
</div>

<pre class="example" id="orgc93aaf2">
the insert key is first set to negative infinity in order to be able to increase the key (any key is greater than negative infinity)
</pre>
</div>
</div>
</div>
<div id="outline-container-org2e87f9e" class="outline-3">
<h3 id="org2e87f9e"><span class="section-number-3">3.2.</span> Quicksort</h3>
<div class="outline-text-3" id="text-3-2">

<div id="org65c9ba4" class="figure">
<p><img src="images/c7_quicksort.png" alt="c7_quicksort.png" /><br />
</p>
</div>


<div id="orgabcfddf" class="figure">
<p><img src="images/c7_quicksort_fig.png" alt="c7_quicksort_fig.png" /><br />
</p>
</div>


<div id="org8b2c7b1" class="figure">
<p><img src="images/c7_randomized_quicksort.png" alt="c7_randomized_quicksort.png" /><br />
</p>
</div>
</div>
</div>

<div id="outline-container-org07c80c8" class="outline-3">
<h3 id="org07c80c8"><span class="section-number-3">3.3.</span> Sorting in Linear Time</h3>
<div class="outline-text-3" id="text-3-3">
<p>
comparison sorts: the sorted order they determine is based only on comparison between the input elements.<br />
</p>

<p>
Any comparison sort must make \(\Omega(n\lg n)\) comparisons in the worst case to sort n elements.<br />
</p>
</div>

<div id="outline-container-org23bbe07" class="outline-4">
<h4 id="org23bbe07"><span class="section-number-4">3.3.1.</span> Counting sort</h4>
<div class="outline-text-4" id="text-3-3-1">
<p>
Counting sort assume that each of the input elements is a integer in the range 0 to k, for some integer k. When \(k=O(n)\), the sort runs in \(\Theta(n)\) time.<br />
</p>


<div id="org105e36b" class="figure">
<p><img src="images/c8_counting_sort.png" alt="c8_counting_sort.png" /><br />
</p>
</div>


<div id="orgbba843f" class="figure">
<p><img src="images/c8_counting_sort_fig.png" alt="c8_counting_sort_fig.png" /><br />
</p>
</div>
</div>
</div>

<div id="outline-container-org53a3b56" class="outline-4">
<h4 id="org53a3b56"><span class="section-number-4">3.3.2.</span> Radix sort</h4>
<div class="outline-text-4" id="text-3-3-2">
<p>
The following procedure assumes that each element in the n-element array A has d digits, where digit 1 is the lowest-order digit and d is the highest-order digit.<br />
</p>


<div id="orga42ad6b" class="figure">
<p><img src="images/c8_radix_sort.png" alt="c8_radix_sort.png" /><br />
</p>
</div>


<div id="org629e848" class="figure">
<p><img src="images/c8_radix_sort_fig.png" alt="c8_radix_sort_fig.png" /><br />
</p>
</div>

<p>
Given n d-digit numbers in which each digit can take on up to k possible values, RADIX-SORT correctly sorts these numbers in \(\Theta(d(n+k))\) time if the stable sort it uses takes \(\Theta(n+k)\) time.<br />
</p>
</div>
</div>

<div id="outline-container-org56a4caa" class="outline-4">
<h4 id="org56a4caa"><span class="section-number-4">3.3.3.</span> Bucket sort</h4>
<div class="outline-text-4" id="text-3-3-3">
<p>
Bucket sort assumes that the input is generated by a random process that distributes elements uniformly and independently over the interval [0,1).<br />
Bucket sort divides the interval [0,1) into n equal-sized subintervals, or buckets, and then distributes the n inputs elements into the buckets.<br />
</p>


<div id="org555386e" class="figure">
<p><img src="images/c8_bucket_sort.png" alt="c8_bucket_sort.png" /><br />
</p>
</div>


<div id="org9479d8f" class="figure">
<p><img src="images/c8_bucket_sort_fig.png" alt="c8_bucket_sort_fig.png" /><br />
</p>
</div>
</div>
</div>




<div id="outline-container-orgfbfb79c" class="outline-4">
<h4 id="orgfbfb79c"><span class="section-number-4">3.3.4.</span> Medians and Order Statistics</h4>
<div class="outline-text-4" id="text-3-3-4">
<p>
The ith order statistic of a set of n elements is the ith smallest element. For example, the minimum of a set of elements is the first order statistics (i=1), and the maximum is the nth order statistics (i=n).<br />
A median is the "halfway point" of the set. When n is odd, the median is unique, ocurring at \(i=(n+1)/2\). When n is even, there are two median, ocurring at \(i=\lfloor(n+1)/2\rfloor\) (the lower median) and \(i=\lceil(n+1)/2\rceil\)<br />
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org057b81b" class="outline-2">
<h2 id="org057b81b"><span class="section-number-2">4.</span> Data Structures</h2>
<div class="outline-text-2" id="text-4">
<p>
In a typical implementation of a dynamic set, each element is represented by an object whose attributes can be examined and manipulated if we have a pointer to the object.<br />
</p>

<p>
Operations on a dynamic set can be grouped into two categories: queries, which simply return information about the set, and modifying operations, which change the set.<br />
</p>

<p>
SEARCH(S,k)<br />
A query that, given a set S and a key value k, returns a pointer x to an element in S such that x.key = k, or NIL if on such element belongs to S.<br />
</p>

<p>
INSERT(S,x)<br />
A modifying operation that augements the set S with the element pointed to by x.<br />
</p>

<p>
DELETE(S,x)<br />
A modifying operation that, given a pointer x to an element in the set S, removes x from S.<br />
</p>

<p>
MINIMUM(S)<br />
A query on a totally ordered set S that returns a pointer to the element of S with the smallest key.<br />
</p>

<p>
MAXIMUM(S)<br />
A query on a totally ordered set S that returns a pointer to the element of S with the largest key.<br />
</p>

<p>
SUCCESSOR(S,x)<br />
A query that, given an element x whose key is from a totally ordered set S, returns a pointer to the next larger element in S, or NIL if x is the maximum element.<br />
</p>

<p>
PREDECESSOR(S,x)<br />
A query that, given an element x whose key is from a totally ordered set S, returns a pointer to the next smaller element in S, or NIL if x is the minimum element.<br />
</p>
</div>

<div id="outline-container-orga3c6bb7" class="outline-3">
<h3 id="orga3c6bb7"><span class="section-number-3">4.1.</span> Elementary Data Structure</h3>
<div class="outline-text-3" id="text-4-1">
</div>
<div id="outline-container-org99f0703" class="outline-4">
<h4 id="org99f0703"><span class="section-number-4">4.1.1.</span> Stacks and queues</h4>
<div class="outline-text-4" id="text-4-1-1">
<p>
Stacks and queues are dynamic sets in which the element removed from the set by the DELETE operation is prespecified. In a stack, the element deleted from the set is the one most recently inserted: the stack implements a last-in, first-out, or LIFO, policy. Similarly, in a queue, the element deleted is always the one that has been in the set for the longest time: the queue implements a first-in, first-out, or FIFO, policy.<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org2f6ddf2"></a>Stacks<br />
<div class="outline-text-5" id="text-4-1-1-1">
<p>
The INSERT operation on a stack is often called PUSH , and the DELETE operation, which does not take an element argument, is often called POP.<br />
</p>

<p>
We can implement a stack of at most n elements with an array S[1..n]. The array has an attribute S.top that indexes the most recently inserted element. The stack consists of elements S[1..S.top], where S[ 1 ] is the element at the bottom of the stack and S[S.top] is the element at the top.<br />
</p>


<div id="org6596fdc" class="figure">
<p><img src="images/c10_stack_fig.png" alt="c10_stack_fig.png" /><br />
</p>
</div>

<p>
When S.top = 0, the stack contains no elements and is empty. If we attempt to pop an empty stack, we say the stack underflows. If S.top exceeds n, the stack overflows.<br />
</p>


<div id="orga6a324f" class="figure">
<p><img src="images/c10_stack.png" alt="c10_stack.png" /><br />
</p>
</div>
</div>
</li>

<li><a id="orgcf6760a"></a>Queues<br />
<div class="outline-text-5" id="text-4-1-1-2">
<p>
We call the INSERT operation on a queue ENQUEUE , and we call the DELETE operation DEQUEUE.<br />
</p>

<p>
The queue has a head and a tail. When an element is enqueued, it takes its place at the tail of the queue, takes a place at the end of the line. The element dequeued is always the one at the head of the queue.<br />
</p>


<div id="orgee228fc" class="figure">
<p><img src="images/c10_queue_fig.png" alt="c10_queue_fig.png" /><br />
</p>
</div>


<div id="org7b3e5ec" class="figure">
<p><img src="images/c10_queue.png" alt="c10_queue.png" /><br />
</p>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgbd435cb" class="outline-4">
<h4 id="orgbd435cb"><span class="section-number-4">4.1.2.</span> Linked lists</h4>
<div class="outline-text-4" id="text-4-1-2">
<p>
A linked list is a data structure in which the objects are arranged in a linear order.<br />
</p>

<pre class="example" id="orgc9b52d7">
Unlike an array, however, in which the linear order is determined by the array
indices, the order in a linked list is determined by a pointer in each object.
</pre>


<div id="org93e2734" class="figure">
<p><img src="images/c10_linked_list.png" alt="c10_linked_list.png" /><br />
</p>
</div>


<div id="orgbd7e195" class="figure">
<p><img src="images/c10_linked_list_search.png" alt="c10_linked_list_search.png" /><br />
</p>
</div>


<div id="org83a8c37" class="figure">
<p><img src="images/c10_linked_list_insert.png" alt="c10_linked_list_insert.png" /><br />
</p>
</div>


<div id="org49faca1" class="figure">
<p><img src="images/c10_linked_list_delete.png" alt="c10_linked_list_delete.png" /><br />
</p>
</div>

<p>
The code for LIST-DELETE would be simpler if we could ignore the boundary conditions at the head and the tail of the list:<br />
</p>


<div id="org74ce6a2" class="figure">
<p><img src="images/c10_linked_list_delete2.png" alt="c10_linked_list_delete2.png" /><br />
</p>
</div>

<p>
A sentinel is a dummy object that allows us to simplify boundary conditions.<br />
</p>


<div id="orgbb1e74d" class="figure">
<p><img src="images/c10_linked_list_2.png" alt="c10_linked_list_2.png" /><br />
</p>
</div>


<div id="org70fdb8c" class="figure">
<p><img src="images/c10_linked_list_search_2.png" alt="c10_linked_list_search_2.png" /><br />
</p>
</div>


<div id="org00f1c8d" class="figure">
<p><img src="images/c10_linked_list_insert_2.png" alt="c10_linked_list_insert_2.png" /><br />
</p>
</div>

<p>
Sentinels rarely reduce the asymptotic time bounds of data structure operations, but they can reduce constant factors.<br />
</p>

<p>
selection probelm:<br />
Input: A set of n (distinct) numbers and an interger i, with \(1\le i\ge n\).<br />
Output: The elements \(x \in A\) that is larger than exactly i-1 other elements of A.<br />
</p>
</div>


<ol class="org-ol">
<li><a id="org7adf7ab"></a>Minimum and maximum<br />
<div class="outline-text-5" id="text-4-1-2-1">
<pre class="example" id="org88e0f39">
MINIMUM(A)
    min = A[1]
    for i = 2 to A.length
        if min &gt; A[i]
            min = A[i]
    return min
</pre>


<pre class="example" id="org30fca23">
MIN_MAX(A)
    # init the min and max and start index
    if A.lenght is odd
        min = A[1]
        max = A[1]
        start = 2
    else
        if A[1] &gt; A[2]
            min = A[2]
            max = A[1]
        else
            min = A[1]
            max = A[2]
        start = 3

    for i = start to A.length by 2
        if A[i] &gt; A[i + 1]
            if A[i] &gt; max
                max = A[i]
            if A[i + 1] &lt; min
                min = A[i + 1]
        else
            if A[i + 1] &gt; max
                max = A[i + 1]
            if A[i] &lt; min
                min = A[i]
    
    return min, max
</pre>
</div>
</li>

<li><a id="org8c39d92"></a>Selection in expected linear time<br />
<div class="outline-text-5" id="text-4-1-2-2">

<div id="org7b5c16a" class="figure">
<p><img src="images/c9_randomized_select.png" alt="c9_randomized_select.png" /><br />
</p>
</div>
</div>
</li>

<li><a id="orge1270c0"></a>Selection in worst-case linear time<br />
<div class="outline-text-5" id="text-4-1-2-3">

<div id="org99900dc" class="figure">
<p><img src="images/c9_select.png" alt="c9_select.png" /><br />
</p>
</div>
</div>
</li>
</ol>
</div>



<div id="outline-container-org1b5c7b9" class="outline-4">
<h4 id="org1b5c7b9"><span class="section-number-4">4.1.3.</span> Representing rooted trees</h4>
<div class="outline-text-4" id="text-4-1-3">

<div id="orgb57b9eb" class="figure">
<p><img src="images/c10_rooted_tree.png" alt="c10_rooted_tree.png" /><br />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org4e42bb9" class="outline-3">
<h3 id="org4e42bb9"><span class="section-number-3">4.2.</span> Hash Tables</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Many applications require a dynamic set that supports only the dictionary operations INSERT, SEARCH, and DELETE.<br />
</p>

<p>
Directly addressing into an ordinary array makes effective use of our ability to examine an arbitrary position in an array in O(1) time.<br />
</p>

<p>
When the number of keys actually stored is small relative to the total number of possible keys, hash tables become an effective alternative to directly addressing an array, since a hash table typically uses an array of size proportional to the number of keys actually stored. Instead of using the key as an array index directly, the array index is computed from the key.<br />
</p>

<pre class="example" id="org648f4af">
Hashing is an extremely effective and practicaltechnique: 
the basic dictionary operations require only O(1) time on the average.
</pre>
</div>


<div id="outline-container-org3e61ff1" class="outline-4">
<h4 id="org3e61ff1"><span class="section-number-4">4.2.1.</span> Direct-address tables</h4>
<div class="outline-text-4" id="text-4-2-1">
<p>
Direct addressing is a simple technique that works well when the universe U of keys is reasonably small.<br />
</p>


<p>
To represent the dynamic set, we use an array, or direct-address table, denoted by T[0..m-1], in which each position, or slot, corresponds to a key in the universe U.<br />
</p>


<div id="orga358955" class="figure">
<p><img src="images/c11_direct_address_table.png" alt="c11_direct_address_table.png" /><br />
</p>
</div>
</div>
</div>

<div id="outline-container-org60954fa" class="outline-4">
<h4 id="org60954fa"><span class="section-number-4">4.2.2.</span> Hash Tables</h4>
<div class="outline-text-4" id="text-4-2-2">
<p>
The downside of direct addressing is obvious:<br />
</p>
<ol class="org-ol">
<li>if the universe U is large, storing a table T of size |U| may be impractical, or even impossible.<br /></li>
<li>the set K of keys actually stored may be so small relative to U that most of the space allocated for T would be wasted.<br /></li>
</ol>


<p>
When the set K of keys stored in a dictionary is much smaller than the universe U of all possible keys, a hash table reduces the storage requirement to \(\Theta(|K|)\) while maintains the benefit that searching for an element in the hash table still requires only O(1) time.<br />
</p>


<p>
we use a hash function h to compute the slot from the key k.<br />
</p>
\begin{equation}
h: U \rightarrow \{0,1,...,m-1\}
\end{equation}


<div id="orgb7277ca" class="figure">
<p><img src="images/c11_hash_table.png" alt="c11_hash_table.png" /><br />
</p>
</div>
</div>

<ol class="org-ol">
<li><a id="orgec530aa"></a>Collision resolution by chaining<br />
<div class="outline-text-5" id="text-4-2-2-1">
<p>
In chaining, we place all the elements that hash to the same slot into the same linked list.<br />
</p>


<div id="org2209048" class="figure">
<p><img src="images/c11_hash_table_chaining.png" alt="c11_hash_table_chaining.png" /><br />
</p>
</div>


<div id="org8ca148d" class="figure">
<p><img src="images/c11_hash_table_chaining_psudo.png" alt="c11_hash_table_chaining_psudo.png" /><br />
</p>
</div>


<p>
Given a hash table T with m slots that stores n elements, we define the load factor \(\alpha\) for T as n/m, that is, the average number of elements stored in a chain.<br />
</p>

<p>
simple uniform hashing:<br />
Any given element is equally likely to hash into any of the m slots, independently of where any other element has hashed to.<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org481d7e5" class="outline-4">
<h4 id="org481d7e5"><span class="section-number-4">4.2.3.</span> Hash Functions</h4>
<div class="outline-text-4" id="text-4-2-3">
<p>
A good hash function satisfies (approximately) the assumption of simple uniform hashing: each key is equally likely to hash to any of the m slots, independently of where any other key has hashed to.<br />
</p>

<p>
In practice, we can often employ heuristic techniques to create a hash function that performs well.<br />
</p>

<p>
A good approach derives the hash value in a way that we expect to be independent of any patterns that might exist in the data.<br />
</p>

<p>
Most hash functions assume that the universe of keys is the set \(N={0,1,2,...}\) of natural numbers. Thus, if the keys are not natural numbers, we find a way to interpret them as natural numbers.<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org9b5a421"></a>The division method<br />
<div class="outline-text-5" id="text-4-2-3-1">
<p>
h(k) = k mod m<br />
</p>

<pre class="example" id="orgec58097">
m should not be a power of 2, since if m = 2^p , then h(k) is just the p lowest-order bits of k.
A prime not too close to an exact power of 2 is often a good choice for m. 
</pre>
</div>
</li>
<li><a id="org5175269"></a>The multiplication method<br />
<div class="outline-text-5" id="text-4-2-3-2">
<p>
\(h(k) = \lfloor m(kA\ \mathbb{mod}\ 1) \rfloor\)<br />
\(A \approx (\sqrt{5} - 1)/2\)<br />
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgcade9cc" class="outline-4">
<h4 id="orgcade9cc"><span class="section-number-4">4.2.4.</span> Open addressing</h4>
</div>
</div>

<div id="outline-container-org8bd3179" class="outline-3">
<h3 id="org8bd3179"><span class="section-number-3">4.3.</span> Binary Search Trees</h3>
<div class="outline-text-3" id="text-4-3">
</div>
<div id="outline-container-org5bf5a7f" class="outline-4">
<h4 id="org5bf5a7f"><span class="section-number-4">4.3.1.</span> What is a binary search tree?</h4>
<div class="outline-text-4" id="text-4-3-1">
<p>
A binary search tree is organized in a binary tree.<br />
</p>


<div id="org720366a" class="figure">
<p><img src="images/c12_binary_search_tree.png" alt="c12_binary_search_tree.png" /><br />
</p>
</div>

<p>
binary-search-tree property:<br />
Lets \(x\) be the node in a binary search tree. If \(y\) is a node in the left subtree of \(x\), then \(x.key \le x.key\). If \(y\) is a node in the right subtree of \(x\), then \(y.key \ge x.key\).<br />
</p>

<p>
The binary-search-tree property allows us to print out all the keys in a binary search tree in sorted order by a simple recursive algorithm, called an inorder tree walk. This algorithm is so named because it prints the key of the root of a subtree between printing the values in its left subtree and printing those in its right subtree.<br />
</p>



<div id="orge0511ea" class="figure">
<p><img src="images/c12_inorder_tree_walk.png" alt="c12_inorder_tree_walk.png" /><br />
</p>
</div>


<p>
If x is the root of an n-node subtree, then the call INODER-TREE-WALK(x) takes \(\Theta(n)\) time.<br />
</p>
</div>
</div>

<div id="outline-container-org77b1753" class="outline-4">
<h4 id="org77b1753"><span class="section-number-4">4.3.2.</span> Querying a binary search time</h4>
<div class="outline-text-4" id="text-4-3-2">
<p>
SEARCH, MINIMUM, MAXIMUM, SUCCESSOR, PREDECESSOR run in O(h) time. (h is the height)<br />
</p>


<div id="org500267f" class="figure">
<p><img src="images/c12_tree_search.png" alt="c12_tree_search.png" /><br />
</p>
</div>


<div id="org1e6cccc" class="figure">
<p><img src="images/c12_iterative_tree_search.png" alt="c12_iterative_tree_search.png" /><br />
</p>
</div>


<div id="org17ab0fa" class="figure">
<p><img src="images/c12_tree_minimum.png" alt="c12_tree_minimum.png" /><br />
</p>
</div>


<div id="org767b6d3" class="figure">
<p><img src="images/c12_tree_maximum.png" alt="c12_tree_maximum.png" /><br />
</p>
</div>


<div id="orgc2fa6a0" class="figure">
<p><img src="images/c12_tree_successor.png" alt="c12_tree_successor.png" /><br />
</p>
</div>


<div id="orgb1f0cf1" class="figure">
<p><img src="images/c12_search_fig.png" alt="c12_search_fig.png" /><br />
</p>
</div>

<pre class="example" id="orgc7590e9">
TREE-PREDECESSOR(x)
  if x.left != NIL
    return TREE-MINIMUM(x.left)
  y = x.p
  while y != NIL and x == y.left
    x = y
    y = y.p
  return y
</pre>

<pre class="example" id="org14885a5">
If a node in a binary search tree has two children, then its successor has no left child and its predecessor has no right child.
(Ohterwise it will be successor or predecessor)
</pre>
</div>
</div>

<div id="outline-container-org96a4989" class="outline-4">
<h4 id="org96a4989"><span class="section-number-4">4.3.3.</span> Insertion and deletion</h4>
<div class="outline-text-4" id="text-4-3-3">
<p>
INSERTION and DELETION run in O(h) time.<br />
</p>


<div id="orgd00086d" class="figure">
<p><img src="images/c12_tree_insert.png" alt="c12_tree_insert.png" /><br />
</p>
</div>


<div id="orgf820ab3" class="figure">
<p><img src="images/c12_tree_insert_fig.png" alt="c12_tree_insert_fig.png" /><br />
</p>
</div>


<p>
deletetion auxilary function:<br />
</p>


<div id="orgb67d145" class="figure">
<p><img src="images/c12_transplant.png" alt="c12_transplant.png" /><br />
</p>
</div>



<div id="org2a9fefe" class="figure">
<p><img src="images/c12_tree_delete_fig.jpeg" alt="c12_tree_delete_fig.jpeg" /><br />
</p>
</div>


<div id="org2ea5e77" class="figure">
<p><img src="images/c12_tree_delete.jpeg" alt="c12_tree_delete.jpeg" /><br />
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org4ca21b2" class="outline-3">
<h3 id="org4ca21b2"><span class="section-number-3">4.4.</span> Red-Black Trees</h3>
<div class="outline-text-3" id="text-4-4">
<p>
Red-black trees are one of many search-tree schemes that are "balanced" in order to guarantee that basic dynamic-set operations take \(O(\lg n)\) time in the worst case.<br />
</p>
</div>

<div id="outline-container-org5a038d8" class="outline-4">
<h4 id="org5a038d8"><span class="section-number-4">4.4.1.</span> Properties of red-black trees</h4>
<div class="outline-text-4" id="text-4-4-1">
<p>
A red-black tree is a binary search tree with one extra bit of storage per node: its color, which can be either RED or BLACK . By constraining the node colors on any simple path from the root to a leaf, red-black trees ensure that no such path is more than twice as long as any other, so that the tree is approximately balanced.<br />
</p>

<pre class="example" id="org57f758f">
Red-black properties:
1. Every node is either red or black.
2. The root is black.
3. Every leaf ( NIL ) is black.
4. If a node is red, then both its children are black.
5. For each node, all simple paths from the node to descendant leaves contain the same number of black nodes.
</pre>


<div id="org5b85999" class="figure">
<p><img src="images/c13_red_black_tree_fig.png" alt="c13_red_black_tree_fig.png" /><br />
</p>
</div>


<p>
As a matter of convenience in dealing with boundary conditions in red-black tree code, we use a single sentinel to represent NIL. We use the sentinel so that we can treat a NIL child of a node x as an ordinary node whose parent is x.<br />
</p>

<p>
<b>black-height</b> of the node: (denoted bh(x))<br />
the number of black nodes on any simple path from, but not including, a node x down to a leaf<br />
</p>



<p>
Lemma:<br />
A red-black tree with n internal nodes has height at most \(2\lg(n+1)\).<br />
</p>
</div>
</div>

<div id="outline-container-org5a63b57" class="outline-4">
<h4 id="org5a63b57"><span class="section-number-4">4.4.2.</span> Rotations</h4>
<div class="outline-text-4" id="text-4-4-2">
<p>
ratation: a local operation in a search tree that preserves the binary-search-tree property. (run in O(1) time)<br />
</p>


<div id="org6bffea5" class="figure">
<p><img src="images/c13_left_rotate.jpeg" alt="c13_left_rotate.jpeg" /><br />
</p>
</div>
</div>
</div>


<div id="outline-container-org538c670" class="outline-4">
<h4 id="org538c670"><span class="section-number-4">4.4.3.</span> Insertion</h4>
<div class="outline-text-4" id="text-4-4-3">

<div id="org28db747" class="figure">
<p><img src="images/c13_insert.png" alt="c13_insert.png" /><br />
</p>
</div>


<div id="org93c0411" class="figure">
<p><img src="images/c13_insert_fixup.png" alt="c13_insert_fixup.png" /><br />
</p>
</div>

<p>
To understand how RB-INSERT-FIXUP works, we break the code into 3 major steps.<br />
</p>
<ol class="org-ol">
<li>determine the violations of the red-black properties<br /></li>
<li>examine the overall goal of the while loop<br /></li>
<li>explore each of the three cases<br /></li>
</ol>


<div id="org516901b" class="figure">
<p><img src="images/c13_insert_fixup_fig.png" alt="c13_insert_fixup_fig.png" /><br />
</p>
</div>
</div>
</div>


<div id="outline-container-org6f8d8c9" class="outline-4">
<h4 id="org6f8d8c9"><span class="section-number-4">4.4.4.</span> Deletion</h4>
<div class="outline-text-4" id="text-4-4-4">

<div id="orgd52a01c" class="figure">
<p><img src="images/c13_rb_transplant.png" alt="c13_rb_transplant.png" /><br />
</p>
</div>


<div id="orgd635b8c" class="figure">
<p><img src="images/c13_rb_delete.png" alt="c13_rb_delete.png" /><br />
</p>
</div>

<pre class="example" id="org023b2d5">
y: as the node either removed from the tree or moved within the tree;
x: moves into node y's original position;
node y's color might change, the variable y-original-color stores y's color before any changes occur;
</pre>


<div id="org47d1098" class="figure">
<p><img src="images/c13_rb_delete_fixup.png" alt="c13_rb_delete_fixup.png" /><br />
</p>
</div>


<div id="org25b0c5a" class="figure">
<p><img src="images/c13_rb_delete_fixup_fig.png" alt="c13_rb_delete_fixup_fig.png" /><br />
</p>
</div>

<pre class="example" id="org0124016">
If y is red, the red-black properties still hold when y is removed or moved

If node y was black, three problems may arise:
1. y had been the root and a red child of y because the new root (property 2 violated)
2. if both x and x.p are red (property 4 violated)
3. moving y within the tree causes any simple path that privously contained y to have one fewer black node (property 5 violated)
</pre>

<p>
We can correct the violation of property 5 by saying that node x, now occupying y’s original position, has an "extra" black. That is, if we add 1 to the count of black nodes on any simple path that contains x, then under this interpretation, property 5 holds. When we remove or move the black node y, we “push” its blackness onto node x. The problem is that now node x is neither red nor black, thereby violating property 1.<br />
</p>
</div>
</div>
</div>


<div id="outline-container-org9fcddd7" class="outline-3">
<h3 id="org9fcddd7"><span class="section-number-3">4.5.</span> Augmenting Data Structures</h3>
<div class="outline-text-3" id="text-4-5">
</div>
<div id="outline-container-org11e6fb8" class="outline-4">
<h4 id="org11e6fb8"><span class="section-number-4">4.5.1.</span> Dynamic order statistics</h4>
<div class="outline-text-4" id="text-4-5-1">
<p>
An order-statistic tree T is simply a red-black tree with additional information stored in each node.<br />
</p>

<p>
<img src="images/c14_os_tree.png" alt="c14_os_tree.png" /><br />
Besides the usual red-black tree attributes x.key, x.color, x.p, x.left, and x.right in a node x, we have another attribute, x.size. This attribute contains the number of (internal) nodes in the subtree rooted at x (including x itself), that is, the size of the subtree.<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org0ef887c"></a>Retrieving an element with a given rank<br />
<div class="outline-text-5" id="text-4-5-1-1">

<div id="org95e0d79" class="figure">
<p><img src="images/c14_os_select.png" alt="c14_os_select.png" /><br />
</p>
</div>
</div>
</li>

<li><a id="orgfbcfdfc"></a>Determing the rank of an element<br />
<div class="outline-text-5" id="text-4-5-1-2">

<div id="org2e53aa2" class="figure">
<p><img src="images/c14_os_rank.png" alt="c14_os_rank.png" /><br />
</p>
</div>
</div>
</li>

<li><a id="org70b3e10"></a>Maintaing subtree sizes<br /></li>
</ol>
</div>

<div id="outline-container-org28f6dfb" class="outline-4">
<h4 id="org28f6dfb"><span class="section-number-4">4.5.2.</span> How to augment a data structure</h4>
<div class="outline-text-4" id="text-4-5-2">
<p>
We can break the process of augmenting a data structure into four steps:<br />
</p>
<ol class="org-ol">
<li>Choose an underlying data structure.<br /></li>
<li>Determine additional information to maintain in the underlying data structure.<br /></li>
<li>Verify that we can maintain the additional information for the basic modifying operations on the underlying data structure.<br /></li>
<li>Develop new operations.<br /></li>
</ol>

<pre class="example" id="orga2d57bd">
As with any prescriptive design method, you should not blindly follow the steps
in the order given. Most design work contains an element of trial and error, and
progress on all steps usually proceeds in parallel. There is no point, for example, in
determining additional information and developing new operations (steps 2 and 4)
if we will not be able to maintain the additional information efficiently. Neverthe-
less, this four-step method provides a good focus for your efforts in augmenting
a data structure, and it is also a good way to organize the documentation of an
augmented data structure.
</pre>


<p>
Theroem (augmenting a red-black tree)<br />
</p>
<pre class="example" id="orge26fa0e">
Let f be an attribute that augments a red-black tree T of n nodes, and suppose that
the value of f for each node x depends on only the information in nodes x, x.left,
and x.right, possibly including x.left.f and x.right.f. Then, we can maintain the
values of f in all nodes of T during insertion and deletion without asymptotically
affecting the O(lgn) performance of these operations.
</pre>
</div>
</div>

<div id="outline-container-org0abb1d0" class="outline-4">
<h4 id="org0abb1d0"><span class="section-number-4">4.5.3.</span> Interval trees</h4>
<div class="outline-text-4" id="text-4-5-3">
<p>
A closed interval is an ordered pair of real numbers \([t_1, t_2]\) with \(t_1 \le t_2\) . The interval \([t_1, t_2]\) represents the set \(\{t \in \mathbb{R}: t_1 \le t \le t_2\}\).<br />
</p>

<p>
Any two intervals \(i\) and \(i'\) satisfy the interval trichotomy; that is, exactly one of the following three properties holds:<br />
</p>
<ol class="org-ol">
<li>\(i\) and \(i'\) overlap<br /></li>
<li>\(i\) is to the left of \(i'\)<br /></li>
<li>\(i\) is to the right of \(i'\)<br /></li>
</ol>

<p>
An interval tree is a red-black tree that maintains a dynamic set of elements, with each element x containing an interval x.int.<br />
</p>

<p>
Interval trees support the following operations:<br />
<img src="images/c14_interval_tree_op.png" alt="c14_interval_tree_op.png" /><br />
</p>


<div id="orge906e11" class="figure">
<p><img src="images/c14_interval_tree_fig.png" alt="c14_interval_tree_fig.png" /><br />
</p>
</div>


<div id="orgf03a2cc" class="figure">
<p><img src="images/c14_interval_search.png" alt="c14_interval_search.png" /><br />
</p>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-org4a14ba7" class="outline-2">
<h2 id="org4a14ba7"><span class="section-number-2">5.</span> Advanced Design and Analysis Techniques</h2>
<div class="outline-text-2" id="text-5">
<p>
Dynamic programming typically applies to optimization problems in which we make a set of choices in order to arrive at an optimal solution. As we make each choice, subproblems of the same form often arise. Dynamic programming is effective when a given subproblem may arise from more than one partial set of choices; the key technique is to store the solution to each such subproblem in case it should reappear.<br />
</p>

<p>
Greedy algorithms typically apply to optimization problems in which we make a set of choices in order to arrive at an optimal solution. The idea of a greedy algorithm is to make each choice in a locally optimal manner.<br />
</p>

<p>
We use amortized analysis to analyze certain algorithms that perform a sequence of similar operations. Instead of bounding the cost of the sequence of operations by bounding the actual cost of each operation separately, an amortized analysis provides a bound on the actual cost of the entire sequence. One advantage of this approach is that although some operations might be expensive, many others might be cheap.<br />
</p>
</div>
<div id="outline-container-orgb43f21b" class="outline-3">
<h3 id="orgb43f21b"><span class="section-number-3">5.1.</span> Dynamic Programming</h3>
<div class="outline-text-3" id="text-5-1">
<p>
Dynamic programming solves problems by combining the solutions to subproblems. ("programming" in this context refers to a tabular method, not to writing computer code.)<br />
Dynamic programming applies when the subproblems overlap.<br />
A dynamic programming algorithm solve each subproblem just once and then save its answer in a table, thereby avoiding the work of recomputing the answer every time is solves each subproblem.<br />
</p>

<p>
We typically apply dynamic programming to optimization problems.<br />
</p>

<p>
When developing a dynamic programming algorithm, we follow a sequence of four steps:<br />
</p>
<ol class="org-ol">
<li>characterize the structure of an optimal solution<br /></li>
<li>recursivly define the value of an optimal solution<br /></li>
<li>compute the value of an optimal solution, typically in a bottom-up fashion<br /></li>
<li>construct an optimal solution from computed information<br /></li>
</ol>
</div>
<div id="outline-container-orga5ff064" class="outline-4">
<h4 id="orga5ff064"><span class="section-number-4">5.1.1.</span> Rod cutting</h4>
<div class="outline-text-4" id="text-5-1-1">
<p>
The rod-cutting problem:<br />
Given a rod of length n inches and a table of price \(p_i\) for \(i=1,2,...,n\), determine the maximum revenue \(r_n\) obtainable by cutting up the rod and selling the pieces.<br />
</p>

<p>
We can cut up a rod of length \(n\) in \(2^{n-1}\) different ways.<br />
</p>


<div id="org464e473" class="figure">
<p><img src="images/c15_rod_cutting1.png" alt="c15_rod_cutting1.png" /><br />
</p>
</div>


<div id="org9b46242" class="figure">
<p><img src="images/c15_rod_cutting2.png" alt="c15_rod_cutting2.png" /><br />
</p>
</div>

<p>
If an optimal solution cuts the rod into k pieces, for some \(1\le k \le n\), then an optimal decomposition<br />
\(n = i_1 + i_2 + ... + i_k\)<br />
of the rod into pieces of lengths \(i_1,i_2,...,i_k\) provides maximum corresponding revenue<br />
\(r_n = p_{i_1} + p_{i_2} + ... + p_{i_k}\).<br />
</p>

<p>
Frame the values \(r_n\) for \(n\ge 1\) in terms of optimal revenues from shorter rods:<br />
</p>
\begin{equation}
r_n = \max(p_n, r_1+r_{n-1}, r_2+r_{n-2},...,r_{n-1}+r_1)
\end{equation}


<p>
To solve the original problem of size \(n\), we solve smaller problems of the same type, but of smaller sizes. The overall optimal solution incorporates optimal solution to the two related subproblems, maximizing revenue from each of those two pieces.<br />
</p>

<p>
We say that the rod-cutting problem exhibits <b>optimal substructure</b>: optimal solutions to a problem incorporate optimal solutions to related subproblems, which we may solve independently.<br />
</p>


<p>
In a related, but slightly simpler, way to arrange a recursive structure for the rod-cutting problem, we view a decomposition as consisting of a first piece of length \(i\) cut off the left-hand end, and then a right-hand remainder of length \(n-i\). Only the remainder, and not the first piece, may be further divided. We thus obtain the following simpler version:<br />
</p>
\begin{equation}
r_n = \max_{1\le i \le n}(p_i + r_{n-i}).
\end{equation}

<p>
In this formulation, an optimal solution embodies the solution to only one related subproblem—the remainder—rather than two.<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org91d4433"></a>Recursive top-down implementation<br />
<div class="outline-text-5" id="text-5-1-1-1">

<div id="org7e13ea7" class="figure">
<p><img src="images/c15_cut_rod.png" alt="c15_cut_rod.png" /><br />
</p>
</div>


<pre class="example" id="org06abfa1">
Why is CUT-ROD so inefficient? 
The problem is that CUT-ROD calls itself recursively over and over 
again with the same parameter values; it solves the same subproblems 
repeatedly.
</pre>
</div>
</li>
<li><a id="org94443b4"></a>Using dynamic programming for optimal rod cutting<br />
<div class="outline-text-5" id="text-5-1-1-2">
<p>
The dynamic-programming method works as follows. Having observed that a naive recursive solution is inefficient because it solves the same subproblems repeatedly, we arrange for each subproblem to be solved only once, saving its solution. Dynamic programming thus uses additional memory to save computation time; it serves an example of a time-memory trade-off. The savings may be dramatic: an exponential-time solution may be transformed into a polynomial-time solution. A dynamic-programming approach runs in polynomial time when the number of distinct subproblems involved is polynomial in the input size and we can solve each such subproblem in polynomial time.<br />
</p>

<p>
There are usually two equivalent ways to implement a dynamic-programming approach:<br />
</p>
<ol class="org-ol">
<li>top-down with memoization<br /></li>
<li>bottom-up method<br /></li>
</ol>

<p>
These two approaches yield algorithms with the same asymptotic running time, except in unusual circumstances where the top-down approach does not actually recurse to examine all possible subproblems. The bottom-up approach often has much better constant factors, since it has less overhead for procedure calls.<br />
</p>


<div id="orgae07e1b" class="figure">
<p><img src="images/c15_memoized_cut_rod.png" alt="c15_memoized_cut_rod.png" /><br />
</p>
</div>


<div id="orgdae09bf" class="figure">
<p><img src="images/c15_memoized_cut_rod_aux.png" alt="c15_memoized_cut_rod_aux.png" /><br />
</p>
</div>


<div id="org22f2859" class="figure">
<p><img src="images/c15_bottom_up_cut_rod.png" alt="c15_bottom_up_cut_rod.png" /><br />
</p>
</div>
</div>
</li>
<li><a id="orgbe77a95"></a>Reconstructing a solution<br />
<div class="outline-text-5" id="text-5-1-1-3">
<p>
Our dynamic-programming solutions to the rod-cutting problem return the value of an optimal solution, but they do not return an actual solution: a list of piece sizes. We can extend the dynamic-programming approach to record not only the optimal value computed for each subproblem, but also a choice that led to the optimal value. With this information, we can readily print an optimal solution.<br />
</p>


<div id="org5ce0452" class="figure">
<p><img src="images/c15_extended_bottom_up_cut_rod.png" alt="c15_extended_bottom_up_cut_rod.png" /><br />
</p>
</div>


<div id="orgb06646f" class="figure">
<p><img src="images/c15_print_cut_rod_solution.png" alt="c15_print_cut_rod_solution.png" /><br />
</p>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgad8290a" class="outline-4">
<h4 id="orgad8290a"><span class="section-number-4">5.1.2.</span> Matrix-chain multiplication</h4>
<div class="outline-text-4" id="text-5-1-2">
<p>
Given a sequence (chain) \(\{A_1,A_2,...,A_n\}\) of n matrices to be multiplied, and we wish to compute the product \(A_1A_2...A_n\)<br />
</p>


<div id="org7322dd8" class="figure">
<p><img src="images/c15_matrix_multiply.png" alt="c15_matrix_multiply.png" /><br />
</p>
</div>


<p>
matrix-chain multiplication problem:<br />
given a chine \(\{A_1,A_2,...,A_n\}\) of n matrices, where for \(i=1,2,...,n\), matrix \(A_i\) has dimension \(p_{i-1}\times p_i\), fully parenthesize of the product \(A_1A_2...A_n\) in a way that minimize the number of scalar multiplication.<br />
</p>


<div id="orgd42745b" class="figure">
<p><img src="images/c15_matrix_chain_order.png" alt="c15_matrix_chain_order.png" /><br />
</p>
</div>


<div id="orge631e89" class="figure">
<p><img src="images/c15_print_optimal_parens.png" alt="c15_print_optimal_parens.png" /><br />
</p>
</div>


<div id="orgd076adc" class="figure">
<p><img src="images/c15_memoized_matrix_chain.png" alt="c15_memoized_matrix_chain.png" /><br />
</p>
</div>
</div>
</div>


<div id="outline-container-org0208238" class="outline-4">
<h4 id="org0208238"><span class="section-number-4">5.1.3.</span> Elements of dynamic programming</h4>
<div class="outline-text-4" id="text-5-1-3">
<p>
When to apply dynamic programming?<br />
When the problem has the two properties:<br />
</p>
<ol class="org-ol">
<li>optimal substructure<br /></li>
<li>overlapping subproblems<br /></li>
</ol>

<p>
optimal substructure: If an optimal solution to the problem contains within it optimal solutions to subproblems.<br />
</p>
<pre class="example" id="orgdf7cbda">
Note: The subproblems should be independent.
</pre>
<p>
overlapping subproblems: When a recursive algorithm revisits the same problem repeatedly, we say that the optimization problem has overlapping subproblems.<br />
</p>
</div>
</div>

<div id="outline-container-org423dc18" class="outline-4">
<h4 id="org423dc18"><span class="section-number-4">5.1.4.</span> Longest Common Subsequence</h4>
<div class="outline-text-4" id="text-5-1-4">
<p>
Given a sequence \(X = [x_1, x_2, ..., x_m]\), another sequence \(Z = [z_1,z_2,...,z_k]\) is a subsequence of X if there exists a strictly increasing sequence \([i_1,i_2,...,i_k]\) of indices of X such that for all \(j = 1, 2, ..., k\), we have \(x_{i_j} = z_j\).<br />
</p>

<p>
Given two sequences X and Y, we say that a sequence Z is a common subsequence of X and Y if Z is a subsequence of both X and Y.<br />
</p>

<p>
In the longest-common-subsequence problem, we are given two sequences  \(X = [x_1, x_2, ..., x_m]\) and \(Y = [y_1,y_2,...,y_n]\) and wish to find a maximum length common subsequence of X and Y.<br />
</p>



<div id="org68bad6c" class="figure">
<p><img src="images/c15_theorem.png" alt="c15_theorem.png" /><br />
</p>
</div>

<p>
This leads to the following solution:<br />
<img src="images/c15_lcs_equation.png" alt="c15_lcs_equation.png" /><br />
</p>



<div id="org2514788" class="figure">
<p><img src="images/c15_lcs_length.png" alt="c15_lcs_length.png" /><br />
</p>
</div>



<div id="orgdbebc4d" class="figure">
<p><img src="images/c15_lcs_vis.png" alt="c15_lcs_vis.png" /><br />
</p>
</div>


<div id="org1a18933" class="figure">
<p><img src="images/c15_print_lcs.png" alt="c15_print_lcs.png" /><br />
</p>
</div>
</div>
</div>


<div id="outline-container-orgd15678f" class="outline-4">
<h4 id="orgd15678f"><span class="section-number-4">5.1.5.</span> Optimal binary search tress</h4>
<div class="outline-text-4" id="text-5-1-5">
<p>
Suppose that we are designing a program to translate text from English to French. For each occurrence of each English word in the text, we need to look up its French equivalent. We could perform these lookup operations by building a binary search tree with n English words as keys and their French equivalents as satellite data. We want words that occur frequently in the text to be placed nearer the root.<br />
</p>


<div id="orgdb6f73c" class="figure">
<p><img src="images/c15_obs1.png" alt="c15_obs1.png" /><br />
</p>
</div>


<div id="orgb96efc2" class="figure">
<p><img src="images/c15_obs2.png" alt="c15_obs2.png" /><br />
</p>
</div>


<div id="orgb276210" class="figure">
<p><img src="images/c15_recursion.png" alt="c15_recursion.png" /><br />
</p>
</div>

<p>
where<br />
</p>


<div id="org1a9ce97" class="figure">
<p><img src="images/c15_wij.png" alt="c15_wij.png" /><br />
</p>
</div>


<div id="org6a7c6dd" class="figure">
<p><img src="images/c15_optimal_bst.png" alt="c15_optimal_bst.png" /><br />
</p>
</div>
</div>
</div>
</div>



<div id="outline-container-orgd6dffd9" class="outline-3">
<h3 id="orgd6dffd9"><span class="section-number-3">5.2.</span> Greedy Algorithms</h3>
<div class="outline-text-3" id="text-5-2">
<p>
A greedy algorithm always make the choice that looks best at moment. That is, it makes a locally optimal choice in the hope that this choice will lead to globally optimal choice.<br />
</p>
</div>

<div id="outline-container-orge369bc2" class="outline-4">
<h4 id="orge369bc2"><span class="section-number-4">5.2.1.</span> An activity-selection problem</h4>
<div class="outline-text-4" id="text-5-2-1">
<p>
We have a set \(S={a_1,a_2,...,a_n}\) of n proposed activities that wish to use a resource which can serve only one activity at a time. Each activity \(a_i\) has a start time \(s_i\) and a finish time \(f_i\), where \(0\le s_i < f_i < \infty\). Activitis \(a_i\) and \(a_j\) are compatible if the intervals \([s_i,f_i)\) and $[s<sub>j,f</sub><sub>j</sub>) do not overlap. In the activity-selection problem, we wish to select a maximum-size subset of mutually compatible activitis.<br />
</p>

<p>
Theorem<br />
Consider any nonempty subproblem \(S_k\), and let \(a_m\) be an activity in \(S_k\) with the earliest finish time. Then \(a_m\) is included in some maximum-size subset of mutually compatible activity of \(S_k\).<br />
</p>


<div id="orgcbac69e" class="figure">
<p><img src="images/c16-recursive-as.png" alt="c16-recursive-as.png" /><br />
</p>
</div>


<div id="orga197c49" class="figure">
<p><img src="images/c16-greedy-as.png" alt="c16-greedy-as.png" /><br />
</p>
</div>
</div>
</div>


<div id="outline-container-orgb0da82f" class="outline-4">
<h4 id="orgb0da82f"><span class="section-number-4">5.2.2.</span> Element of the greedy strategy</h4>
<div class="outline-text-4" id="text-5-2-2">
<p>
steps:<br />
</p>
<ol class="org-ol">
<li>Case the optimization problem as one in which we make a choice and are left with one subproblem to solve.<br /></li>
<li>Prove that there is always an optimal solution to the original problem that makes the greedy choice, so that the greedy choice is always safe.<br /></li>
<li>Demoenstrate optimal substructure by showing that, having madethe greedy choice, what remains is a subproblem with the property that if we combine an optimal solution with the greedy choice we have made, we arrive an optimal solution to the original problem.<br /></li>
</ol>

<p>
greedy-choice proerty:<br />
we can assemble a globally optimal solution by making locally optimal (greedy) choices.<br />
</p>
</div>
</div>

<div id="outline-container-org250cc92" class="outline-4">
<h4 id="org250cc92"><span class="section-number-4">5.2.3.</span> Huffman codes</h4>
<div class="outline-text-4" id="text-5-2-3">
<p>
the problem of designing a binary character code in which each character is represented by a unique binary string, which we call a <b>codeword</b>.<br />
</p>

<p>
prefix-free code (prefix code):<br />
no codeword is also a prefix of some other codeword.<br />
</p>
<p>
Encodeing is always simple for any binary character code: we just concatenate the codewords representing each character of the file.<br />
</p>

<p>
Prefix codes are desirable becuase they simplify decoding. The decoding process needs a convenient representation for the prefix code so that we can easily pick off the inital codeword. A binary tree whose leaves are the given characters provides one such representation. We interpret the binary codeword for a character as the simple path from the root to the character.<br />
</p>


<div id="org0785623" class="figure">
<p><img src="images/c16-prefix-code-tree.png" alt="c16-prefix-code-tree.png" /><br />
</p>
</div>

<p>
Huffman invented a greedy algorithm that constructs an optimal prefix code called a Huffman code.<br />
<img src="images/c16-huffman.png" alt="c16-huffman.png" /><br />
</p>

<p>
Lemma<br />
Let C be an alphabet in which each character \(c\in C\) has frequency \(c.freq\). Let x and y be two characters in C having the lowest frequencies. Then there exists an optimal prefix code for C in which the codewords for x and y have the same length and differ only in the last bit.<br />
</p>

<p>
Lemma<br />
Let C be a given alphabet with frequency \(c.freq\) defined for each character \(c\in C\). Let x and y be two characters in C with minimum frequency. Let \(C'\) be the alphabet C with the characters x and y removed and a new character z added, so that \(C' = C - \{x,y\} \cup \{z\}\). Define f for C' as for C, except that \(z.freq = x.freq + y.freq\). Let T' be any tree representing an optimal prefix code for the alphabet C'. The the tree T, obtained from T' by replacing the leaf node for z with an internal node having x and y as children, represents an optimal prefix code for the alphabet C.<br />
</p>
</div>
</div>
</div>
<div id="outline-container-orgc29a26f" class="outline-3">
<h3 id="orgc29a26f"><span class="section-number-3">5.3.</span> Amortized Analysis</h3>
<div class="outline-text-3" id="text-5-3">
<p>
In an amortized analysis, we average the time required to perform a sequence of data-structure operations over all the operations performed. Amortized analysis differs from average-case analysis in that probability is not involved; an amortized analysis guarantees the average performance of each operation in the worst case.<br />
</p>

<p>
The core is that: the individual operation is not independent in the algorithm analysis.<br />
</p>
</div>
<div id="outline-container-org299a708" class="outline-4">
<h4 id="org299a708"><span class="section-number-4">5.3.1.</span> Aggregate analysis</h4>
<div class="outline-text-4" id="text-5-3-1">
<p>
In aggregate analysis, we show that for all \(n\), a sequence of n operations takes worst-case time \(T(n)\) in total. In the worst case, the average cost, or amortized cost, per operation is therefore \(T(n)/n\). Note that this amortized cost applies to each operation, even when there are several types of operations in the sequence.<br />
</p>
</div>
</div>
<div id="outline-container-org07be9dd" class="outline-4">
<h4 id="org07be9dd"><span class="section-number-4">5.3.2.</span> The accounting method</h4>
<div class="outline-text-4" id="text-5-3-2">
<p>
In the accounting method of amortized analysis, we assign differing charges to different operations, with some operations charged more or less than they actually cost. We call the amount we charge an operation its amortized cost. When an operation’s amortized cost exceeds its actual cost, we assign the difference to specific objects in the data structure as credit. Credit can help pay for later operations whose amortized cost is less than their actual cost.<br />
</p>

<p>
We must choose the amortized costs of operations carefully. If we want to show that in the worst case the average cost per operation is small by analyzing with amortized costs, we must ensure that the total amortized cost of a sequence of operations provides an upper bound on the total actual cost of the sequence.<br />
</p>
</div>
</div>
<div id="outline-container-org479b343" class="outline-4">
<h4 id="org479b343"><span class="section-number-4">5.3.3.</span> The potential method</h4>
<div class="outline-text-4" id="text-5-3-3">
<p>
The potential method of amortized analysis represents the prepaid work as "potential energy" or just "potential", which can be released to pay for future operations.<br />
</p>

\begin{equation}
\hat{c}_i = c_i + \Phi(D_i) - \Phi(D_{i-1}).
\end{equation}
<p>
where \(c_i\) is the acutual cost of the ith opration, \(D_i\) is the data structure that results after applying the ith operation to data structure \(D_{i-1}\), \(\hat{c_i}\) is the amortized cost of the ith operation.<br />
</p>
</div>
</div>
<div id="outline-container-org113282d" class="outline-4">
<h4 id="org113282d"><span class="section-number-4">5.3.4.</span> Dynamic tables</h4>
<div class="outline-text-4" id="text-5-3-4">
</div>
<ol class="org-ol">
<li><a id="org355f0be"></a>Table expansion<br />
<div class="outline-text-5" id="text-5-3-4-1">

<div id="org0bfcdf2" class="figure">
<p><img src="images/17-table-insert.png" alt="17-table-insert.png" /><br />
</p>
</div>


\begin{equation}
c_i =
\begin{cases}
i \quad \mbox{if}\ i-1 \ \mbox{is an exact power of 2}. \\
1 \quad \mbox{otherwise}.
\end{cases}
\end{equation}

\begin{equation}
\Phi(T)=
\begin{cases}
2\cdot T.num - T.size  \quad & \mbox{if}\ \alpha(T) \ge 1/2. \\
T.size/2 - T.num  & \mbox{if}\ \alpha(T) < 1/2.
\end{cases}
\end{equation}
<p>
where \(\Phi\) is the potential function, T is an object representing the table, T.num contains the number of items in the table, T.size gives the total number of slots in the table, and \(\alpha\) is the load factor.<br />
</p>
</div>
</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orgb873635" class="outline-2">
<h2 id="orgb873635"><span class="section-number-2">6.</span> Advanced Data Structure</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-orge230218" class="outline-3">
<h3 id="orge230218"><span class="section-number-3">6.1.</span> B-Tree</h3>
<div class="outline-text-3" id="text-6-1">
<p>
We have red-black tree already, so why we design B-tree?<br />
The point is the huge gap in read/write speed between primary memory (or main memory) and secondary storage.<br />
</p>

<p>
The primary memory normally consists of silicon memory chips. The secondary storage bases on magnetic disks. The spedd of main memory is about \(10^5\) times faster than that of secondary storage.<br />
</p>


<div id="orga59586e" class="figure">
<p><img src="images/18-disk.png" alt="18-disk.png" /><br />
</p>
</div>


<div id="orgcd6b13f" class="figure">
<p><img src="images/18-b-tree.png" alt="18-b-tree.png" /><br />
</p>
</div>


<p>
In order to amortize the time spent waiting for mechanical movements, disks access not just one item but several at a time. Information is divided into a number of equal-sized <b>pages</b> of bits that appear consecutively within tracks, and each disk read or write is of one or more entire pages.<br />
</p>
</div>
<div id="outline-container-orgb22aaae" class="outline-4">
<h4 id="orgb22aaae"><span class="section-number-4">6.1.1.</span> Definition of B-trees</h4>
<div class="outline-text-4" id="text-6-1-1">
<p>
<img src="images/18-b-tree-d1.png" alt="18-b-tree-d1.png" /><br />
<img src="images/18-b-tree-d2.png" alt="18-b-tree-d2.png" /><br />
</p>

<p>
<b>Theorem</b><br />
<img src="images/18-theorem1.png" alt="18-theorem1.png" /><br />
</p>
</div>
</div>
<div id="outline-container-orgfb34bcd" class="outline-4">
<h4 id="orgfb34bcd"><span class="section-number-4">6.1.2.</span> Basic operation on B-trees</h4>
<div class="outline-text-4" id="text-6-1-2">
<p>
<img src="images/18-b-tree-search.png" alt="18-b-tree-search.png" /><br />
x is the pointer to a node, k is the key needed to be searched for.<br />
</p>

<p>
<img src="images/18-b-tree-create.png" alt="18-b-tree-create.png" /><br />
ALLOCATE-NODE allocates one disk page to be used as a new node in O.1/ time.<br />
<img src="images/18-b-tree-split-child.png" alt="18-b-tree-split-child.png" /><br />
</p>
<pre class="example" id="org22a6e10">
NOTE: There is a bug the line 11, 12, 14, 15, that is the case of x.n = 0. There should be a if condition of x.n &gt; 0.
</pre>
<p>
<img src="images/18-b-tree-insert.png" alt="18-b-tree-insert.png" /><br />
<img src="images/18-b-tree-insert-nonfull.png" alt="18-b-tree-insert-nonfull.png" /><br />
<img src="images/18-b-tree-insert-fig.png" alt="18-b-tree-insert-fig.png" /><br />
</p>
</div>
</div>
<div id="outline-container-orgde88cd3" class="outline-4">
<h4 id="orgde88cd3"><span class="section-number-4">6.1.3.</span> Deleting a key from B-tree</h4>
<div class="outline-text-4" id="text-6-1-3">
<p>
The procedure B-TREE-DELETE deletes the key k from the subtree rooted at x. We design this procedure to guarantee that whenever it calls itself recursively on a node x, the number of keys in x is at least the minimum degree t. Note that this condition requires one more key than the minimum required by the usual B-tree conditions, so that sometimes a key may have to be moved into a child node before recursion descends to that child. This strengthened condition allows us to delete a key from the tree in one downward pass without having to "back up".<br />
</p>

<p>
<img src="images/18-delete1.png" alt="18-delete1.png" /><br />
<img src="images/18-delete2.png" alt="18-delete2.png" /><br />
<img src="images/18-delete3.png" alt="18-delete3.png" /><br />
</p>


<p>
<img src="images/18-delete-fig1.png" alt="18-delete-fig1.png" /><br />
<img src="images/18-delete-fig2.png" alt="18-delete-fig2.png" /><br />
</p>
</div>
</div>
</div>
<div id="outline-container-orgf0e0075" class="outline-3">
<h3 id="orgf0e0075"><span class="section-number-3">6.2.</span> Fibonacci Heaps</h3>
<div class="outline-text-3" id="text-6-2">
<p>
The Fibonacci heap data structure serves a dual purpose. First, it supports a set of operations that constitutes what is known as a "mergeable heap". Second, several Fibonacci-heap operations run in constant amortized time, which makes this data structure well suited for applications that invoke these operations frequently.<br />
</p>


<div id="orgdfc7f11" class="figure">
<p><img src="images/19-mergeable-heaps.png" alt="19-mergeable-heaps.png" /><br />
</p>
</div>


<div id="org0d376f9" class="figure">
<p><img src="images/19-binary-fibonacci.png" alt="19-binary-fibonacci.png" /><br />
</p>
</div>

<p>
If UNION operation is not needed, ordinary binary heaps work fairly well. (The above running in Fibonacci heaps are amortized time bounds)<br />
</p>


<p>
<b>Fibonacci heaps in theory and practice</b><br />
From a theoretical standpoint, Fibonacci heaps are especially desirable when the number of EXTRACT-MIN and D ELETE operations is small relative to the number of other operations performed. From a practical point of view, however, the constant factors and programming complexity of Fibonacci heaps make them less desirable than ordinary binary (or k-ary) heaps for most applications, except for certain applications that manage large amounts of data.<br />
</p>
</div>
<div id="outline-container-org6c66cd8" class="outline-4">
<h4 id="org6c66cd8"><span class="section-number-4">6.2.1.</span> Structure of Fibonacci heaps</h4>
<div class="outline-text-4" id="text-6-2-1">

<div id="org6a69364" class="figure">
<p><img src="images/c19-fibonacci.jpg" alt="c19-fibonacci.jpg" /><br />
</p>
</div>

<ol class="org-ol">
<li>A Fibonacci heap is a collection of rooted trees that are min-heap ordered.<br /></li>
<li>Each node x contains a pointer x.p to its parent and a pointer x.child to any one of its children.<br /></li>
<li>The children of x are linked together in a circular, doubly linked list, which we call the child list of x.<br /></li>
</ol>


<p>
Circular, doubly linked lists have two advantages for use in Fibonacci heaps:<br />
</p>
<ol class="org-ol">
<li>we can insert a node into any location or remove a node from anywhere in a circular, doubly linked list in O(1) time.<br /></li>
<li>given two such lists, we can concatenate them (or "splice" them together) into one circular, doubly linked list in O(1) time.<br /></li>
</ol>


<ol class="org-ol">
<li>\(x.degree\): the number of childern in the child list of node x<br /></li>
<li>\(x.mark\): indicates whether node x has lost a child since the last time \(x\) was made the child of another node.<br /></li>
<li>\(x.left\): point to the left siblings in the child list<br /></li>
<li>\(x.right\): point to the right siblings in the child list<br /></li>
<li>\(H.n\): the number of nodes currently in \(H\)<br /></li>
</ol>
</div>
<ol class="org-ol">
<li><a id="org3f97346"></a>Potential function<br />
<div class="outline-text-5" id="text-6-2-1-1">
<p>
For a given Fibonacci heap H,<br />
</p>
\begin{equation}
\Phi(H)=t(H)+2m(H).
\end{equation}

<p>
where \(t(H)\) is the number of trees in the root list of H, \(m(H)\) is the number of marked nodes in H.<br />
</p>

<pre class="example" id="org11d44ca">
The boolean-valued attribute x:mark indicates whether
node x has lost a child since the last time x was made the child of another node.

The roots of all the trees in a Fibonacci heap are linked together using their
left and right pointers into a circular, doubly linked list called the root list of the
Fibonacci heap.
</pre>
</div>
</li>
</ol>
</div>
<div id="outline-container-org7f7e992" class="outline-4">
<h4 id="org7f7e992"><span class="section-number-4">6.2.2.</span> Mergeable-heap-operations</h4>
<div class="outline-text-4" id="text-6-2-2">
</div>
<ol class="org-ol">
<li><a id="orgb30339e"></a>Inserting a node<br />
<div class="outline-text-5" id="text-6-2-2-1">

<div id="org6d6aa43" class="figure">
<p><img src="images/c19-fib-insert.png" alt="c19-fib-insert.png" /><br />
</p>
</div>


<div id="orge1ba2a3" class="figure">
<p><img src="images/c19-fib-insert-fig.png" alt="c19-fib-insert-fig.png" /><br />
</p>
</div>
</div>
</li>
<li><a id="org856174a"></a>Uniting two Fibonacci heaps<br />
<div class="outline-text-5" id="text-6-2-2-2">

<div id="org2cdcb9f" class="figure">
<p><img src="images/c19-fib-union.png" alt="c19-fib-union.png" /><br />
</p>
</div>
</div>
</li>
<li><a id="org9765e4f"></a>Extracting the minimum node<br />
<div class="outline-text-5" id="text-6-2-2-3">

<div id="orgaa168a0" class="figure">
<p><img src="images/c19-fib-extract-min.png" alt="c19-fib-extract-min.png" /><br />
</p>
</div>

<pre class="example" id="orge850632">
If z is NIL, then Fibonacci heap H is already empty and we are done.
If z is its own right sibling after line 6,then z was the only node on the root list and it had no children.
</pre>

<p>
Consolidating the root list consists of repeatedly executing the following steps until every root in the root list has a distinct degree value:<br />
</p>
<ol class="org-ol">
<li>Find two roots \(x\) and \(y\) in the root list with the same degree. Without loss of generality, let \(x.key \le y.key\).<br /></li>
<li>Link \(y\) to \(x\): remove \(y\) from the root list, and make \(y\) a child of \(x\) by calling the FIB-HEAP-LINK procedure. This procedure increments the attribute \(x.degree\) and clears the mark on y.<br /></li>
</ol>


<div id="orgd77f147" class="figure">
<p><img src="images/c19-consolidate.png" alt="c19-consolidate.png" /><br />
</p>
</div>


<div id="orgb7cfe48" class="figure">
<p><img src="images/c19-fib-extract-min-fig1.png" alt="c19-fib-extract-min-fig1.png" /><br />
</p>
</div>


<div id="org7ebd04d" class="figure">
<p><img src="images/c19-fib-extract-min-fig1.png" alt="c19-fib-extract-min-fig1.png" /><br />
</p>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-org20dde18" class="outline-4">
<h4 id="org20dde18"><span class="section-number-4">6.2.3.</span> Decreasing a key and deleting a node</h4>
<div class="outline-text-4" id="text-6-2-3">
</div>
<ol class="org-ol">
<li><a id="org5d7ea00"></a>Decreasing a key<br />
<div class="outline-text-5" id="text-6-2-3-1">

<div id="orgaeb5e31" class="figure">
<p><img src="images/c19-fib-decrease-key.png" alt="c19-fib-decrease-key.png" /><br />
</p>
</div>

<pre class="example" id="org4d5417d">
I do not know the function of the mark attribute.
</pre>

<pre class="example" id="org258a58a">
The figure helps to understand the algorithm process.
</pre>


<div id="orgbd804b8" class="figure">
<p><img src="images/c19-decrease-key-fig.png" alt="c19-decrease-key-fig.png" /><br />
</p>
</div>
</div>
</li>
<li><a id="org6e098ee"></a>Deleting a node<br />
<div class="outline-text-5" id="text-6-2-3-2">

<div id="org34e1767" class="figure">
<p><img src="images/c19-delete.png" alt="c19-delete.png" /><br />
</p>
</div>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-org0309565" class="outline-3">
<h3 id="org0309565"><span class="section-number-3">6.3.</span> van Emde Boas Tress</h3>
<div class="outline-text-3" id="text-6-3">
<p>
In the data structures like binary heaps, red-black trees, Fibonacci heaps, at least one important operation took \(O(\lg n)\) time, either worst case or amortized.<br />
Beacuse each of these data structures based tis decisions on comparing keys, leading to the \(\Omega(\lg n)\) limitation (on time).<br />
</p>

<p>
We can use addtional information (or limitation on information) to circumvent this limitation (on time).<br />
</p>
</div>

<div id="outline-container-org14b24e3" class="outline-4">
<h4 id="org14b24e3"><span class="section-number-4">6.3.1.</span> Preliminary approaches</h4>
<div class="outline-text-4" id="text-6-3-1">
</div>
<ol class="org-ol">
<li><a id="org46e6467"></a>Direct addressing<br />
<div class="outline-text-5" id="text-6-3-1-1">
<p>
To store a dynamic set of values from the universe \(\{0,1,2,...,u-1\}\), we maintain an array \(A[0..u-1]\) of \(u\) bits.<br />
INSERT, DELETE, and MEMBER operations in \(O(1)\) time.<br />
MINIMUM, MAXIMUM, SUCCESSOR, and PREDECESSOR in \(\Omega(u)\) time in the worst case.<br />
</p>
</div>
</li>

<li><a id="org9f8af22"></a>Superimposing a binary tree structure<br />
<div class="outline-text-5" id="text-6-3-1-2">

<div id="org7764e35" class="figure">
<p><img src="images/c20-binary-tree.png" alt="c20-binary-tree.png" /><br />
</p>
</div>

<p>
In the figure, each interval node contains a 1 if and only if any leaf in its subtree contains a 1.<br />
In other words, the ibt stored in an internal node is the logical-or of its two-children.<br />
</p>

<p>
Resulting in \(O(\lg n)\) time.<br />
</p>
</div>
</li>

<li><a id="orgf7a2e9f"></a>Superimpose a tree of constant height<br />
<div class="outline-text-5" id="text-6-3-1-3">
\begin{equation}
u=2^{2k}
\end{equation}



<div id="orgbb884a1" class="figure">
<p><img src="images/c20-degree-tree.png" alt="c20-degree-tree.png" /><br />
</p>
</div>

<p>
We impose a tree of degree \(\sqrt{u}\).<br />
\(summary[i]\) contains a 1 if and only if the subarray \(A[i\sqrt{u}..(i+1)\sqrt{u}-1]\) contains a 1.<br />
We call this $\sqrt{u}$-bit subarray of A the ith cluster.<br />
</p>

<p>
Resulting in \(O(\sqrt{u})\) time.<br />
</p>
</div>
</li>
</ol>
</div>


<div id="outline-container-orgd53ba0a" class="outline-4">
<h4 id="orgd53ba0a"><span class="section-number-4">6.3.2.</span> A recursive structure</h4>
<div class="outline-text-4" id="text-6-3-2">
<p>
For simplicity, we assume that \(u=2^{2^k}\) for some integer \(k\).<br />
</p>

<p>
Consider the recurrence:<br />
</p>
\begin{equation}
T(u) = T(\sqrt{u}) + O(1)
\end{equation}

<p>
The above equation has the solutionn \(T(u)=O(\lg\lg u)\).<br />
</p>

\begin{equation}
high(x) = \lfloor x/\sqrt{u} \rfloor
\end{equation}
\begin{equation}
low(x) = x \mod \sqrt{u}
\end{equation}
\begin{equation}
index(x,y) = x \sqrt{u} + y
\end{equation}

<p>
We have identity \(x=index(high(x), low(x))\).<br />
</p>
</div>


<ol class="org-ol">
<li><a id="orged44b86"></a>Proto van Emde Boas structures<br />
<div class="outline-text-5" id="text-6-3-2-1">
<p>
For the universe \(\{0,1,2,...,u-1\}\), we define a <b>proto van Emde Boas structure</b>, or <b>proto-vEB structure</b>.<br />
</p>

<p>
Each proto-vEB(u) structure contains an attribute \(u\) giving its universe size.<br />
In addition, it contains the following:<br />
<img src="images/c20-proto-vEB.png" alt="c20-proto-vEB.png" /><br />
</p>


<div id="org86bde0f" class="figure">
<p><img src="images/c20-proto-vEB-fig.png" alt="c20-proto-vEB-fig.png" /><br />
</p>
</div>


<div id="org6020758" class="figure">
<p><img src="images/c20-proto-vEB-fig-all.png" alt="c20-proto-vEB-fig-all.png" /><br />
</p>
</div>
</div>
</li>

<li><a id="org1cdc5fb"></a>Operations on a proto van Emde Boas structure<br />
<ol class="org-ol">
<li><a id="org8e32c3f"></a>Determining whether a value is in the set<br />
<div class="outline-text-6" id="text-6-3-2-2-1">

<div id="org2071d2e" class="figure">
<p><img src="images/c20-member.png" alt="c20-member.png" /><br />
</p>
</div>

<p>
\(O(\lg \lg u)\)<br />
</p>
</div>
</li>

<li><a id="org056ad65"></a>Finding the minimum element<br />
<div class="outline-text-6" id="text-6-3-2-2-2">

<div id="org635e030" class="figure">
<p><img src="images/c20-minimum.png" alt="c20-minimum.png" /><br />
</p>
</div>

<p>
Although querying the summary information allows us to quickly find the cluster containing the minimum element, because this procedure makes two recursive calls on \(proto-vEB(\sqrt{u})\) structures,<br />
it does not run in \(O(\lg\lg u)\) time in the worst case. (\(\Theta(\lg u)\) instead)<br />
</p>
</div>
</li>

<li><a id="org947cc1d"></a>Finding the successor<br />
<div class="outline-text-6" id="text-6-3-2-2-3">

<div id="org412c9ce" class="figure">
<p><img src="images/c20-successor.png" alt="c20-successor.png" /><br />
</p>
</div>

<p>
\(\Theta(\lg u \lg\lg u)\)<br />
</p>
</div>
</li>

<li><a id="org5eb4719"></a>Inserting an element<br />
<div class="outline-text-6" id="text-6-3-2-2-4">

<div id="org3a6a1fe" class="figure">
<p><img src="images/c20-insert.png" alt="c20-insert.png" /><br />
</p>
</div>


<p>
\(\Theta(\lg u)\)<br />
</p>
</div>
</li>

<li><a id="org69639e0"></a>Deleting an element<br />
<div class="outline-text-6" id="text-6-3-2-2-5">
<pre class="example" id="org7528d05">
PROTO-vEB-DELETE(V,x)
    if V.u == 2
        V.A[x] = 0
    else
        PROTO-vEB-DELETE(V.cluster[high(x)], low(x))
        if V.cluster[high(x)] are all 0
            PROTO-vEB-DELETE(V.summary, high(x))

</pre>
</div>
</li>
</ol>
</li>
</ol>
</div>

<div id="outline-container-org04fd270" class="outline-4">
<h4 id="org04fd270"><span class="section-number-4">6.3.3.</span> The van Emde Boas tree</h4>
<div class="outline-text-4" id="text-6-3-3">
<p>
The proto-vEB structure is close to what we need to achieve \(O(\lg\lg u)\) running times.<br />
It falls short because we have to recurse too many times in most the operations.<br />
We need to design a data structure that is similar to the proto-vEB structure but stores a little more information, thereby removing the need for the recursion.<br />
</p>

<p>
If u is an odd power of 2 (\(u=2^{2k+1}\) for some integer \(k\ge 0)\) &#x2013; then we will divide the \(\lg u\) bits of a number into the most significant \(\lceil(\lg u)/2 \rceil\) bits and the least significant \(\lfloor (\lg u)/2 \rfloor\) bits.<br />
For convenience, we denote \(2^{\lceil (\lg u)/2 \rceil}\) by \(\sqrt[\uparrow]{u}\) and \(2^{\lfloor(\lg u)/2 \rfloor}\) by \(\sqrt[\downarrow]{u}\).<br />
</p>

\begin{equation}
\mathrm{high}(x) = \lfloor x/ \sqrt[\downarrow]{u} \rfloor,
\end{equation}
\begin{equation}
\mathrm{low}(x) = x \mod \sqrt[\downarrow]{u},
\end{equation}
\begin{equation}
\mathrm{index}(x,y) = x\sqrt[\downarrow]{u} + y
\end{equation}
</div>

<ol class="org-ol">
<li><a id="org89c1ae8"></a>van Emde Boas trees<br />
<div class="outline-text-5" id="text-6-3-3-1">
<p>
A vEB tree contains two attributes not found in a proto-vEB structure:<br />
</p>
<ol class="org-ol">
<li><code>min</code> stores the minimum element in the vEB tree<br /></li>
<li><code>max</code> stores the maximum element in the vEB tree<br /></li>
</ol>


<div id="orgfe7b454" class="figure">
<p><img src="images/c20-vemde-boas.png" alt="c20-vemde-boas.png" /><br />
</p>
</div>


<div id="orga75c743" class="figure">
<p><img src="images/c20-veb-fig.png" alt="c20-veb-fig.png" /><br />
</p>
</div>

<p>
The min and max attributes will turn out to be key to reducing the number of recursive calls within the operation on vEB trees.<br />
</p>
<ol class="org-ol">
<li>The MINIMUM and MAXIMUM operations do not even need to recurse, for they can just return the values of min or max.<br /></li>
<li>The SUCCESSOR operation can avoid making recursive call to determine whether the seccessor of a value x lies within high(x). That is becuase x's successor lies within its cluster if and only if x is strictly less than the max attribute of its cluster. A symmetric argument holds for PREDECESSOR and min.<br /></li>
<li>We can tell whether a vEB tree has no elements, exactly one element, or at least two elements in constant time from its min and max values.<br /></li>
<li>If we know that a vEB tree is empty, we can insert an element into it by updating only its min and max attributes.<br /></li>
</ol>


<p>
Before using a van Emde Boas tree, we must know the universe size \(u\), so that we can create a van Emde Boas tree of the appropriate size that initially represents an empty set.<br />
The total space requirement of a van Emde Boas tree is \(O(u)\), and it is straightforward to create an empty tree in \(O(u)\) time.<br />
</p>

<p>
Therefore, we might not want to use a van Emde Boas tree when we perform only a small number of operations, since the time to create the data structure would exceed the time saved in the individual operations.<br />
</p>
</div>
</li>

<li><a id="orgeae8980"></a>Operations on a van Emde Boas tree<br />
<ol class="org-ol">
<li><a id="org8279522"></a>Finding the minimum and maximum elements<br />
<div class="outline-text-6" id="text-6-3-3-2-1">
<pre class="example" id="orgee86b1a">
vEB-TREE-MINIMUM(V)
    return V.min
</pre>

<pre class="example" id="org8554acd">
vEB-TREE-MAXIMUM(V)
    return V.max
</pre>
</div>
</li>

<li><a id="orgb462e49"></a>Determining whether a value is in the set<br />
<div class="outline-text-6" id="text-6-3-3-2-2">
<pre class="example" id="orgd2030b4">
vEB-TREE-MEMBER(V,x)
    if x == V.min or x == V.max
        return TRUE
elseif V.u == 2
    return FALSE # a vEB(2) tree has no elements other than those in min and max, if it is the base case, return FALSE
else
    return vEB-TREE-MEMBER(V.cluster[high(x)],low(x))
</pre>
</div>
</li>

<li><a id="org30d7ddb"></a>Finding the successor and predecessor<br />
<div class="outline-text-6" id="text-6-3-3-2-3">
<p>
Comparing to proto-vEB, we can access the maximum value in a vEB tree quickly, we can avoid making two recursive calls, and instead make one recursive call on either a cluster or on the summary, but not on both.<br />
</p>


<div id="orga9c92c0" class="figure">
<p><img src="images/c20-veb-successor.png" alt="c20-veb-successor.png" /><br />
</p>
</div>

<pre class="example" id="orgfca4f5b">
line 1-4: find the successor of 0 and 1 is in the 2-element set or not
line 5: whether x is strictly less than the minimum element.
line 6: to determine whether the successor is in x's cluster
</pre>


<div id="org1a77abd" class="figure">
<p><img src="images/c20-veb-predecessor.png" alt="c20-veb-predecessor.png" /><br />
</p>
</div>


<pre class="example">
line 13-14: if x's predecessor is the minimum value in vEB tree, then the predecessor resides in no cluster at all.
</pre>
</div>
</li>

<li><a id="orgf656f78"></a>Inserting an element<br />
<div class="outline-text-6" id="text-6-3-3-2-4">
<p>
The VEB-TREE-INSERT procedure will make only one recursive call. How can we get away with just one?<br />
When we insert an element, either the cluster that it goes into already has another element or it does not.<br />
If the cluster already has another element, then the cluster number is already in the summary, and so we do not need to make that recursive call.<br />
If the cluster does not already have another element, then the element being inserted becomes the only element in the cluster, and we do not need to recurse to insert an element into an empty vEB tree.<br />
</p>


<div id="org9f86996" class="figure">
<p><img src="images/c20-veb-empty-tree-insert.png" alt="c20-veb-empty-tree-insert.png" /><br />
</p>
</div>



<div id="org0b65c0c" class="figure">
<p><img src="images/c20-veb-tree-insert.png" alt="c20-veb-tree-insert.png" /><br />
</p>
</div>

<pre class="example" id="org95b808f">
line 3: if x &lt; min, then x needs to become the new min
line 4: we don't want to lose the original min, so we need to insert it into one of V's clusters
</pre>


<div id="org888cc83" class="figure">
<p><img src="images/c20-veb-tree-delete.png" alt="c20-veb-tree-delete.png" /><br />
</p>
</div>

<pre class="example" id="org115ee24">
line 1: the tree contains only one element
line 4: test if it is the base case
line 9-22: assume that V has two or more elements and that u &gt;= 4
line 9: because the min is not stored in cluster, 
        if x == V.min, the other element should be the new min and this element should be deleted from the cluster.
line 13: the cluster is empty 
line 18: all of V's clusters are empty, then the onlyy ramaining element in V is min
line 21: x's cluster did not become empty due to x being deleted
</pre>
</div>
</li>
</ol>
</li>
</ol>
</div>
</div>

<div id="outline-container-orgcd678b0" class="outline-3">
<h3 id="orgcd678b0"><span class="section-number-3">6.4.</span> Data Structure for Disjoint Sets</h3>
<div class="outline-text-3" id="text-6-4">
</div>
<div id="outline-container-org9913497" class="outline-4">
<h4 id="org9913497"><span class="section-number-4">6.4.1.</span> Disjoint-set operations</h4>
<div class="outline-text-4" id="text-6-4-1">
<p>
A disjoint-set data structure maintains a collection \(\mathbb{S}=\{S_1,S_2,...,S_k\}\) of disjoint dynamic sets.<br />
We identify each set by a representative, which is some member of the set.<br />
</p>

<p>
wanted operations:<br />
</p>
<ol class="org-ol">
<li>MAKE-SET(x) creates a new set whose only member is x.<br /></li>
<li>UNION(x,y) unites the dynamic sets that contains x and y, say \(S_x\) and \(S_y\), into a new set that is the union of these two sets.<br /></li>
<li>FIND-SET(x) returns a pointer to the representative of the (unique) set containing x.<br /></li>
</ol>

<pre class="example" id="org8ed640a">
n: the number of MAKE-SET operations
m: the total number of MAKE-SET, UNION, and FIND-SET operations
</pre>
</div>
<ol class="org-ol">
<li><a id="org03abaa8"></a>An aaplication of disjoint-set data structures<br />
<div class="outline-text-5" id="text-6-4-1-1">
<p>
One of the many applications of disjoint-set data structures arises in determining the connected components of an undirected graph.<br />
</p>


<div id="org05508cf" class="figure">
<p><img src="images/c21-disjoint-set-application.png" alt="c21-disjoint-set-application.png" /><br />
</p>
</div>


<div id="orgfeeff06" class="figure">
<p><img src="images/c21-disjoint-set-application2.png" alt="c21-disjoint-set-application2.png" /><br />
</p>
</div>

<pre class="example" id="org426a486">
In an actual implementation of this connected-components algorithm, the repre-
sentations of the graph and the disjoint-set data structure would need to reference
each other. That is, an object representing a vertex would contain a pointer to
the corresponding disjoint-set object, and vice versa.
</pre>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgf4a81dc" class="outline-4">
<h4 id="orgf4a81dc"><span class="section-number-4">6.4.2.</span> Linked-list representation of disjoint sets</h4>
<div class="outline-text-4" id="text-6-4-2">

<div id="orgfd77255" class="figure">
<p><img src="images/c21-linked-list.png" alt="c21-linked-list.png" /><br />
</p>
</div>


<pre class="example" id="orgcb9f496">
With this linked-list representation, both MAKE-SET and FIND-SET are easy, requiring O(1) time.
</pre>
</div>

<ol class="org-ol">
<li><a id="orgb649915"></a>A simple implementation of union<br />
<div class="outline-text-5" id="text-6-4-2-1">
<p>
Just append x's list to y's list, ignoring which set is longer.<br />
We use the tail pointer for x’s list to quickly find where to append y’s list.<br />
</p>
</div>
</li>


<li><a id="org6ef901b"></a>A weighted-union heuristic<br />
<div class="outline-text-5" id="text-6-4-2-2">
<p>
Suppose each list also includes the length of the list and that we always append the shorter list onto the longer, breaking ties arbitrarily.<br />
With this simple weighted-union heuristic, a single UNION operation can still take \(\Omega(n)\) time if both sets have \(\Omega(n)\) members.<br />
</p>

<p>
Theorem 21.1<br />
Using the linked-list representation of disjoint sets and the weighted-union heuristic, a sequence of \(m\) MAKE-SET, UNION, and FIND-SET operations, \(n\) of which are MAKE-SET operations, take \(O(m+n\lg n)\) time.<br />
</p>
</div>
</li>
</ol>
</div>


<div id="outline-container-orgf8afb49" class="outline-4">
<h4 id="orgf8afb49"><span class="section-number-4">6.4.3.</span> Disjoint-set forests</h4>
<div class="outline-text-4" id="text-6-4-3">
<p>
In a disjoint-set forest, we represent sets by rooted trees, with each node containing one memeber and each tree representing one set.<br />
</p>


<div id="orgfa0c756" class="figure">
<p><img src="images/c21-disjoint-set-forest.png" alt="c21-disjoint-set-forest.png" /><br />
</p>
</div>

<pre class="example" id="org8383584">
We perform a FIND-SET operation by following parent pointers until we find the root of the tree.
The nodes visited on this simple path toward the root consitute the find path.
</pre>
</div>

<ol class="org-ol">
<li><a id="org03b660d"></a>Heuristic to improve the running time<br />
<div class="outline-text-5" id="text-6-4-3-1">
<p>
A sequence of \(n-1\) UNION operations may create a tree that is just a linear chain of \(n\) nodes. By using two <b>heuristics</b>, however, we can achieve a running time that is almost linear in the total number of operations \(m\).<br />
</p>

<p>
heuristics:<br />
</p>
<dl class="org-dl">
<dt>union by rank</dt><dd>make the root of the tree with fewer nodes point to the root of the tree with more nodes.<br /></dd>
<dt>path compression</dt><dd>during FIND-SET operations, make each node on the find path point directly to the root.<br /></dd>
</dl>

<pre class="example" id="org168f23d">
For each node, we maintain a rank, which is a upper bound on the height of the node.
Pat compression does not change any ranks.
</pre>



<div id="orga683c1e" class="figure">
<p><img src="images/c21-compression.png" alt="c21-compression.png" /><br />
</p>
</div>
</div>
</li>


<li><a id="org9ab9908"></a>Pseudocode for disjoint-set forests<br />
<div class="outline-text-5" id="text-6-4-3-2">
<pre class="example" id="orgdffe846">
MAKE-SET(x)
  x.p = x
  x.rank = 0
</pre>

<pre class="example" id="orgc37be4c">
UNION(x,y)
  LINK(FIND-SET(x), FIND-SET(y))

LINK(x,y)
  if x.rank &gt; y.rank
    y.p = x
else
  x.p = y  # this idea
  if x.rank == y.rank
      y.rank = y.rank + 1
</pre>

<pre class="example" id="org9c2e7d1">
FIND-SET(x)
  if x != x.p
    x.p = FIND-SET(x.p)
return x.p
</pre>
</div>
</li>

<li><a id="orgd882e9e"></a>Effect of the heuristics on the running time<br />
<div class="outline-text-5" id="text-6-4-3-3">
<p>
Separately, either union by rank or path compression improves the running time of the operations on disjoint-set forests, and the improvement is even greater when we use the two heuristics together.<br />
When we use both union by rank and path compression, the worst-case running time is \(O(m\alpha(n)\), where \(\alpha(n)\) is a very slowly growing function.<br />
In any conceivable application of a disjoint-set data structure, \(\alpha(n) \le 4\); thus, we can view the running time as linear in \(m\) in all practical situations.<br />
</p>
</div>
</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org7c30f2d" class="outline-2">
<h2 id="org7c30f2d"><span class="section-number-2">7.</span> Graph Algorithms</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-org36e0a29" class="outline-3">
<h3 id="org36e0a29"><span class="section-number-3">7.1.</span> Elementary Graph Algorithms</h3>
<div class="outline-text-3" id="text-7-1">
<p>
Keys:<br />
</p>
<ol class="org-ol">
<li>representing a graph<br /></li>
<li>searching a graph<br /></li>
</ol>

<p>
Searching a graph meas systematically following the edges of the graph so as to visit the vertices of the graph.<br />
</p>
</div>

<div id="outline-container-org28e0678" class="outline-4">
<h4 id="org28e0678"><span class="section-number-4">7.1.1.</span> Representations of graphs</h4>
<div class="outline-text-4" id="text-7-1-1">
<p>
Two standard ways to represent a graph \(G=(V,E)\) :<br />
</p>
<ol class="org-ol">
<li>a collection of <b>adjacency</b> lists (for sparse graph)<br /></li>
<li>an <b>adjacency</b> matrix (dense graph or tell quickly if there is an edge connecting two given vertices.<br /></li>
</ol>


<div id="orgb44906c" class="figure">
<p><img src="images/22-graph.png" alt="22-graph.png" /><br />
</p>
</div>

<p>
The adjacency-list representation of a graph \(G=(V,E)\) consists of an array of adjacency of |V| lists, one for each vertex in V.<br />
</p>

<p>
For the adjacency-matrix representation of a graph \(G=(V,E)\), we assume that the vertices are numbered 1,2,&#x2026;,|V| in some arbitary manner. The the adjacency-matrix representation consists of a \(|V| \times |V|\) matrix \(A=(a_{ij})\) such that<br />
</p>
\begin{equation}
a_{ij}=
\begin{cases}
1 & \mbox{if}\ (i,j) \in E, \\
0 & \mbox{otherwise}
\end{cases}
\end{equation}
</div>
</div>


<div id="outline-container-orge2e1cc6" class="outline-4">
<h4 id="orge2e1cc6"><span class="section-number-4">7.1.2.</span> Breadth-first search</h4>
<div class="outline-text-4" id="text-7-1-2">
<p>
Breadth-first search is one of the simplest algorithms for searching a graph and the archetype for many important graph algorithms.<br />
</p>

<p>
Given a graph \(G=(V,E)\) and a distringuished <b>source</b> vertex s, breadth-first search systematically explores the edges of G to "discover" every vertex that is reachable from s.<br />
</p>


<p>
Breadth-first search is so named because it expands the frontier between discovered and undiscovered vertices uniformly across the breadth of the frontier. That is, the algorithm discovers all vertices at distance k from s before discovering any vertices at distance k+1.<br />
</p>

<p>
To keep track of progress, breadth-first search colors each vertex white, gray, or black. All vertices start out white and may later become gray and then black. A vertex is discovered the first time it is encountered during the search, at which time it becomes nonwhite. Gray and black vertices, therefore, have been discovered, but breadth-first search distinguishes between them to ensure that the search proceeds in a breadth-first manner. If \((u,v) \in E\) and vertex u is black, then vertex v is either gray or black; that is, all vertices adjacent to black vertices have been discovered. Gray vertices may have some adjacent white vertices; they represent the frontier between discovered and undiscovered vertices.<br />
</p>


<div id="org76d641d" class="figure">
<p><img src="images/22-bfs.png" alt="22-bfs.png" /><br />
</p>
</div>

<pre class="example" id="orgd2a9a47">
1. the input graph is represented using adjacent lists.
2. u.color represents the color of the vertex u
3. u.pi represents the predecessor of u
4. u.d represents the distance from the source s to vertex u
</pre>


<div id="orgee83163" class="figure">
<p><img src="images/22-bfs-fig.png" alt="22-bfs-fig.png" /><br />
</p>
</div>
</div>

<ol class="org-ol">
<li><a id="orga9802c1"></a>Analysis<br />
<div class="outline-text-5" id="text-7-1-2-1">
<pre class="example" id="orgb479bb2">
The running of BFS is O(V+E).
</pre>
</div>
</li>

<li><a id="org6b15e9e"></a>Shortest paths<br />
<div class="outline-text-5" id="text-7-1-2-2">
<p>
Define the <b>shortest-path distance</b> \(\delta(s,v)\) from s to v as the minimum number of edges in any path from the vectex s to vectex v; if there is no path from s to v, the \(\delta(s,v)=\infty\) . We call a path of length \(\delta(s,v)\) from s to v a <b>shortest path</b> from s to v.<br />
</p>

<p>
Lemmas:<br />
<img src="images/22-lemma1.png" alt="22-lemma1.png" /><br />
</p>


<div id="orga259cf0" class="figure">
<p><img src="images/22-lemma2.png" alt="22-lemma2.png" /><br />
</p>
</div>


<div id="org7615437" class="figure">
<p><img src="images/22-lemma3.png" alt="22-lemma3.png" /><br />
</p>
</div>


<div id="org8ee1afd" class="figure">
<p><img src="images/22-lemma4.png" alt="22-lemma4.png" /><br />
</p>
</div>

<p>
<img src="images/22-lemma5-1.png" alt="22-lemma5-1.png" /><br />
<img src="images/22-lemma5-2.png" alt="22-lemma5-2.png" /><br />
</p>
</div>
</li>


<li><a id="org3d3bf31"></a>Breadth-first trees<br />
<div class="outline-text-5" id="text-7-1-2-3">
<p>
The procedure BFS builds a breadth-first tree as it searches the graph. The tree corresponds to the \(\pi\) attributes.<br />
</p>

<p>
For a graph \(G=(V,E)\) with source s, we define the predecessor subgraph of G as \(G_\pi=(V_\pi,E_\pi)\), where<br />
</p>
\begin{equation}
V_\pi=\{v\in V: v.\pi \ne NIL\} \cup \{s\}
\end{equation}
<p>
and<br />
</p>
\begin{equation}
E_\pi=\{(v.\pi,v): v \in V_\pi - \{s\}\}
\end{equation}

<p>
The predecessor subgraph \(G_\pi\) is a breadth-first tree if \(V_\pi\) consists of the vertices reachable from s and, for all \(v\in V_\pi\), the subgraph \(G_\pi\) contains a unique simple path from s to v that is also a shortest path from s to v in G.<br />
</p>


<div id="org399f26a" class="figure">
<p><img src="images/22-lemma6.png" alt="22-lemma6.png" /><br />
</p>
</div>



<div id="org2001cf4" class="figure">
<p><img src="images/22-print-path.png" alt="22-print-path.png" /><br />
</p>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-org9a035a3" class="outline-4">
<h4 id="org9a035a3"><span class="section-number-4">7.1.3.</span> Depth-first search</h4>
<div class="outline-text-4" id="text-7-1-3">

<div id="orga383ec3" class="figure">
<p><img src="images/22-dfs.png" alt="22-dfs.png" /><br />
</p>
</div>

<pre class="example" id="org1f33ec0">
Each vertex is initially white, is grayed when it is discovered in the search, 
and is blackened when it is finished.

Depth-first search timestamps each vertex. Each vertex vhas two timestamps: 
the first timestamp v.d records when v is first discovered (and grayed), 
and the second timestamp v.f records when the
search finishes examining v’s adjacency list (and blackens v).

The variable time is a global variable for timestamping.
</pre>


<div id="org7e27230" class="figure">
<p><img src="images/22-dfs-fig.png" alt="22-dfs-fig.png" /><br />
</p>
</div>

<pre class="example" id="orgc9c96ab">
The result of depth-first search may depend upon the order in which lien 5 of DFS examines the vertices 
and upon the order in which line 4 of DFS-VISIT visits the neighbors of a vertex.
</pre>


<p>
The predecessor subgraph produced by a depth-first search may be composed of several trees. Therefore, we define the predecessor subgraph as follows:<br />
</p>
\begin{equation}
G_\pi=(V,E_\pi)
\end{equation}
<p>
where<br />
</p>
\begin{equation}
E_\pi=\{(v.\pi , v):v \in V\ \mbox{and}\ v.\pi \ne NIL\}
\end{equation}

<p>
The predecessor subgraph of a depth-first search forms a depth-first forest comprising several depth-first trees. The edges in \(E_\pi\) are tree edges.<br />
</p>

<p>
The running time of DFS is \(\Theta(V+E)\).<br />
</p>
</div>

<ol class="org-ol">
<li><a id="orgd114681"></a>Properties of depth-first search<br />
<div class="outline-text-5" id="text-7-1-3-1">
<p>
Depth-first search yields valuable information about the structure of a graph.<br />
</p>

<p>
(?)<br />
Perhaps the most basic property of depth-first search is that the predecessor subgraph \(G_\pi\) does indeed form a forest of trees, since the structure of the depth-first trees exactly mirrors the structure of recursive calls of DFS-VISIT.<br />
</p>

<p>
Another important property of depth-first search is that discovery and finishing times have parenthesis structure. If we represent the discovery of vertex u with a left parenthesis “(u” and represent its finishing by a right parenthesis “u)”, then the history of discoveries and finishings makes a well-formed expression in the sense that the parentheses are properly nested.<br />
</p>


<div id="org463d5c8" class="figure">
<p><img src="images/22-dfs-property.png" alt="22-dfs-property.png" /><br />
</p>
</div>


<div id="orgcd9285d" class="figure">
<p><img src="images/22-theorem7.png" alt="22-theorem7.png" /><br />
</p>
</div>


<div id="org797a0a4" class="figure">
<p><img src="images/22-corollary8.png" alt="22-corollary8.png" /><br />
</p>
</div>


<div id="orge0e6620" class="figure">
<p><img src="images/22-theorem9.png" alt="22-theorem9.png" /><br />
</p>
</div>
</div>
</li>

<li><a id="org12ac085"></a>Classification of edges<br />
<div class="outline-text-5" id="text-7-1-3-2">
<p>
Another interesting property of depth-first search is that the search can be used to classify the edges of the input graph $ G=(V,E)$. The type of each edge can provide important information about a graph.<br />
</p>


<div id="org210d138" class="figure">
<p><img src="images/22-classification-of-edges.png" alt="22-classification-of-edges.png" /><br />
</p>
</div>


<div id="org70e28d3" class="figure">
<p><img src="images/22-color-and-edge.png" alt="22-color-and-edge.png" /><br />
</p>
</div>


<div id="orgcddf53f" class="figure">
<p><img src="images/22-theorem10.png" alt="22-theorem10.png" /><br />
</p>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-org8a1645c" class="outline-4">
<h4 id="org8a1645c"><span class="section-number-4">7.1.4.</span> Topological sort</h4>
<div class="outline-text-4" id="text-7-1-4">
<p>
Directed acyclic graph is sometime called "dag".<br />
</p>

<p>
A topological sort of a dag \(G=(V,E)\) is a linear ordering of all its vertices such that if G contains an edge \((u,v)\), then u appears before v in the ordering.<br />
</p>

<pre class="example" id="org73936da">
TOPOLOGICAL-SORT(G)
1 call DFS(G) to compute finishing times v.f for each vectex v
2 as each vertex is finished, insert it into onto the front of a linked list
3 return the linked list of vertices
</pre>

<p>
The running time of TOPOLOGICAL-SORT is \(\Theta(V+E)\).<br />
</p>


<div id="org415073c" class="figure">
<p><img src="images/22-topological-sort.png" alt="22-topological-sort.png" /><br />
</p>
</div>

<p>
Lemma<br />
A directed graph G is a acyclic if and only if a depth-first search of G yields no back edges.<br />
</p>

<p>
Lemma<br />
TOPOLOGICAL-SORT produces a topological sort of the directed acyclic graph provided as its input.<br />
</p>
</div>
</div>


<div id="outline-container-org80b1856" class="outline-4">
<h4 id="org80b1856"><span class="section-number-4">7.1.5.</span> Strongly connected components</h4>
<div class="outline-text-4" id="text-7-1-5">
<p>
Strongly connected component of a directed graph \(G=(V,E)\) is a maximal set of vertices \(C\subseteq V\) such that for every pair of vertices u and v in C, we have both \(u\leadsto v\) and \(v\leadsto u\); that is, vertices u and v are reachable from each other.<br />
</p>


<div id="orge9b4137" class="figure">
<p><img src="images/22-scc.png" alt="22-scc.png" /><br />
</p>
</div>


<div id="org5f59c9e" class="figure">
<p><img src="images/22-scc-algorithm.png" alt="22-scc-algorithm.png" /><br />
</p>
</div>


<div id="org38dd2a0" class="figure">
<p><img src="images/22-lemma-13.png" alt="22-lemma-13.png" /><br />
</p>
</div>


<div id="org34654bd" class="figure">
<p><img src="images/22-lemma-14.png" alt="22-lemma-14.png" /><br />
</p>
</div>


<div id="org0199ad8" class="figure">
<p><img src="images/22-corollary-15.png" alt="22-corollary-15.png" /><br />
</p>
</div>


<div id="org0a12fb6" class="figure">
<p><img src="images/22-theorem-16.png" alt="22-theorem-16.png" /><br />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org27d527f" class="outline-3">
<h3 id="org27d527f"><span class="section-number-3">7.2.</span> Minimum Spanning Tress</h3>
<div class="outline-text-3" id="text-7-2">
<p>
Given a connected, undirected graph \(G=(V,E)\). For each edge \((u,v) \in E\), there is a weight \(w(u,v)\) specifying the cost to connect \(u\) and \(v\). We wish to find an acyclic subset \(T \subseteq E\) that connects all the vertices and whose total weight<br />
\[w(T) = \sum_{(u,v) \in T} w(u,v)\]<br />
is minimized. Since T is acyclic and connects all the vertices, it must form a tree, which we call a spanning tree since it "spance" the graph G.<br />
</p>



<div id="org161de22" class="figure">
<p><img src="images/23-minimum-spanning-tree.png" alt="23-minimum-spanning-tree.png" /><br />
</p>
</div>
</div>

<div id="outline-container-org658d1d3" class="outline-4">
<h4 id="org658d1d3"><span class="section-number-4">7.2.1.</span> Growing a minimum spanning tree</h4>
<div class="outline-text-4" id="text-7-2-1">
<p>
The two algorithms we consider here use a greedy approach to the problem, although they differ in how they apply this approach.<br />
</p>

<p>
The greedy strategy is captured by the following generic method, which grows the minimum spanning tree one edge at a time. The generic method manages a set of edges A, maintaining the following loop invariant:<br />
</p>
<pre class="example" id="org2ef44a7">
Prior to each iteration, A is a subset of some minimum spanning tree.
</pre>

<p>
At each step, we determine an edge \((u,v)\) we can add to A without violating this invariant, in the sense that \(A\cup \{(u,v)\}\) is also a subset of a minimum spanning tree. We call such an edge a <b>safe edge</b> for A, since we can add it safely to A while maintaining the invariant.<br />
</p>


<div id="org28a4fb9" class="figure">
<p><img src="images/23-generic-mst.png" alt="23-generic-mst.png" /><br />
</p>
</div>


<p>
Some definitions:<br />
</p>


<div id="orgdada8a7" class="figure">
<p><img src="images/23-cut.png" alt="23-cut.png" /><br />
</p>
</div>

<p>
A <b>cut</b> \((S,V-S)\) of an undirected graph \(G=(V,E)\) is a partition of \(V\).<br />
An edge \((u,v) \in E\) <b>crosses</b> the cut \((S,V-S)\) if one of its endpoints is in \(S\) and the other is in \(V-S\).<br />
A cut <b>respects</b> a set \(A\) of edges if on edge in \(A\) crosses the cut.<br />
An edge is a <b>light edge</b> crossing a cut if its weight is the minimum of any edge corssing the cut.<br />
</p>


<div id="org23d19a9" class="figure">
<p><img src="images/23-theorem-1.png" alt="23-theorem-1.png" /><br />
</p>
</div>
</div>
</div>

<div id="outline-container-orgc5c953f" class="outline-4">
<h4 id="orgc5c953f"><span class="section-number-4">7.2.2.</span> The algorithms of Kruskal and Prim</h4>
<div class="outline-text-4" id="text-7-2-2">
</div>
<ol class="org-ol">
<li><a id="org0945abb"></a>Kruskal's algorithm<br />
<div class="outline-text-5" id="text-7-2-2-1">
<p>
Kruskal's algorithm qualifies as a greedy algorithm becuase at each step it adds to the forest an edge of least possible weight.<br />
</p>



<div id="orgce1aab7" class="figure">
<p><img src="images/23-mst-kruskal.png" alt="23-mst-kruskal.png" /><br />
</p>
</div>


<div id="org7c0f6c2" class="figure">
<p><img src="images/23-mst-kruskal-fig.png" alt="23-mst-kruskal-fig.png" /><br />
</p>
</div>
</div>
</li>

<li><a id="orge5b9e35"></a>Prim's algorithm<br />
<div class="outline-text-5" id="text-7-2-2-2">
<p>
This strategy qualifies as greedy since at each step it adds to the tree an edge that contributes the minim amount possible to the tree's weight.<br />
</p>


<div id="orgfe0baf4" class="figure">
<p><img src="images/23-mst-prim.png" alt="23-mst-prim.png" /><br />
</p>
</div>


<p>
During execution of the algorithm, all vertices that are not in the tree reside in a min-priority queue Q based on a key attribute. For each vertex v, the attribute v.key is the minimum weight of any edge connecting v to a vertex in the tree; by convention, \(v.key = \infty\) if there is no such edge.<br />
</p>


<div id="orge1b5885" class="figure">
<p><img src="images/23-prim-fig.png" alt="23-prim-fig.png" /><br />
</p>
</div>



<div id="org629676e" class="figure">
<p><img src="images/23-prim-loop.png" alt="23-prim-loop.png" /><br />
</p>
</div>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-orgc27ede2" class="outline-3">
<h3 id="orgc27ede2"><span class="section-number-3">7.3.</span> Single-Source Shortest Paths</h3>
<div class="outline-text-3" id="text-7-3">
<p>
In a shortest-paths problem, we are given a weighted, directed graph \(G=(V,E)\), with weight function \(w: E \rightarrow \mathbb{R}\) mapping edges to real-valued weights. The weight \(w(p)\) of path \(p=\langle v_0,v_1,\dots,v_k \rangle\) is the sum of the weights of its consituent edges:<br />
</p>
\begin{equation}
w(p) = \sum_{i=1}^k w(v_{i-1},
\end{equation}

<p>
We define the shortest-path weight \(\delta(u,v)\) from \(u\) to \(v\) by<br />
</p>


<div id="orgc29f79b" class="figure">
<p><img src="images/24-shortest-path-weight.png" alt="24-shortest-path-weight.png" /><br />
</p>
</div>

<p>
A <b>shortest path</b> from vertex \(u\) to vertex \(v\) is then defined as any path \(p\) with weight \(w(p)=\delta(u,v)\)<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org063009a"></a>Optimal substructure of a shortest path<br />
<div class="outline-text-5" id="text-7-3-0-1">
<p>
Shortest-paths algorithms typically rely on the property that a shortest path between two vertices contains other shortest paths within it.<br />
</p>


<div id="orgbaaab58" class="figure">
<p><img src="images/24-lemma-1.png" alt="24-lemma-1.png" /><br />
</p>
</div>
</div>
</li>

<li><a id="org79d7844"></a>Negative-weight edges<br />
<div class="outline-text-5" id="text-7-3-0-2">
<p>
If there is a nagative-weight cycle on some path from \(s\) to \(v\), we define \(\delta(s,v)=-\infty\).<br />
</p>
</div>
</li>

<li><a id="org05f263e"></a>Cycles<br />
<div class="outline-text-5" id="text-7-3-0-3">
<p>
We can assume that when we are finding shortest paths, they have no cycles, i.e., they are simple paths. (only 0-weight cycles are possible)<br />
Since any acyclic path in a graph \(G=(V,E)\) contains at most \(|V|\) distinct vertices, it also contains at most \(|V|-1\) edges. Thus, we can restrict our attention to shortest paths of at most \(|V|-1\) edges.<br />
</p>
</div>
</li>

<li><a id="org7922a1b"></a>Representing shortest paths<br />
<div class="outline-text-5" id="text-7-3-0-4">
<p>
A shortest-paths tree rooted at \(s\) is a directed subgraph \(G'=(V',E')\), where \(V' \subseteq V\) and \(E' \subseteq E\), such that<br />
</p>
<ol class="org-ol">
<li>\(V'\) is the set of vertices reachable from \(s\) in \(G\)<br /></li>
<li>\(G'\) forms a rooted tree with root \(s\), and<br /></li>
<li>for all \(v \in V'\), the unique simple path from \(s\) to \(v\) in \(G'\) is a shortest path from \(s\) to \(v\) in \(G\).<br /></li>
</ol>

<p>
Shortest paths are not necessarily unique, and neighter are shortest-paths trees.<br />
</p>


<div id="org435222d" class="figure">
<p><img src="images/24-not-unique.png" alt="24-not-unique.png" /><br />
</p>
</div>
</div>
</li>

<li><a id="orgcde0ffd"></a>Relaxation<br />
<div class="outline-text-5" id="text-7-3-0-5">
<p>
For each vertex \(v \in V\), we maintain an attribute \(v.d\), which is an upper bound on the weight of a shortest path from source \(s\) to \(v\). We call \(v.d\) a <b>shortest-path estimate</b>.<br />
</p>


<div id="orgd8b618d" class="figure">
<p><img src="images/24-initialize-single-source.png" alt="24-initialize-single-source.png" /><br />
</p>
</div>

<p>
The process of <b>relaxing</b> an edge \((u,v)\) consists of testing whether we can improve the shortest path to \(v\) found so far by going through \(u\) and, if so, updating \(v.d\) and \(v.\pi\)<br />
</p>


<div id="orgdf8b06a" class="figure">
<p><img src="images/24-relax-original.png" alt="24-relax-original.png" /><br />
</p>
</div>


<div id="org98b1118" class="figure">
<p><img src="images/24-relax.png" alt="24-relax.png" /><br />
</p>
</div>


<div id="orge58a58c" class="figure">
<p><img src="images/24-relax-fig.png" alt="24-relax-fig.png" /><br />
</p>
</div>


<pre class="example" id="org0f3e8eb">
Relaxation is the only means by which shortest-path estimates and predecessors change.
</pre>
</div>
</li>

<li><a id="org4490cd5"></a>Properties of shortest paths and relaxation<br />
<div class="outline-text-5" id="text-7-3-0-6">

<div id="org2abb71a" class="figure">
<p><img src="images/24-properties.png" alt="24-properties.png" /><br />
</p>
</div>
</div>
</li>
</ol>

<div id="outline-container-org68c15ad" class="outline-4">
<h4 id="org68c15ad"><span class="section-number-4">7.3.1.</span> The Bellman-Ford algorithm</h4>
<div class="outline-text-4" id="text-7-3-1">

<div id="org2e92ea5" class="figure">
<p><img src="images/24-bellman-ford.png" alt="24-bellman-ford.png" /><br />
</p>
</div>


<div id="orgad74056" class="figure">
<p><img src="images/24-bellman-ford-fig.png" alt="24-bellman-ford-fig.png" /><br />
</p>
</div>


<pre class="example" id="orgef367a7">
The running time is O(VE).
</pre>
</div>
</div>
<div id="outline-container-org8d7680a" class="outline-4">
<h4 id="org8d7680a"><span class="section-number-4">7.3.2.</span> Single-source shortest paths in directed acyclic graphs</h4>
<div class="outline-text-4" id="text-7-3-2">

<div id="org3ee3ba7" class="figure">
<p><img src="images/24-dag.png" alt="24-dag.png" /><br />
</p>
</div>


<div id="org4688dcf" class="figure">
<p><img src="images/24-dag-fig.png" alt="24-dag-fig.png" /><br />
</p>
</div>

<p>
The running time is \(\Theta(V+E)\).<br />
</p>
</div>
</div>

<div id="outline-container-org5796959" class="outline-4">
<h4 id="org5796959"><span class="section-number-4">7.3.3.</span> Dijkstra's algorithm</h4>
<div class="outline-text-4" id="text-7-3-3">
<p>
Situation: all edge weights are nonnegative.<br />
</p>


<div id="org6abb32a" class="figure">
<p><img src="images/24-dijkstra.png" alt="24-dijkstra.png" /><br />
</p>
</div>


<div id="orgc8c3466" class="figure">
<p><img src="images/24-dijkstra-fig.png" alt="24-dijkstra-fig.png" /><br />
</p>
</div>


<pre class="example" id="org667c1cd">
The running time of Dijkstra’s algorithm depends on how we implement the min-priority queue.
</pre>
</div>
</div>

<div id="outline-container-org4916669" class="outline-4">
<h4 id="org4916669"><span class="section-number-4">7.3.4.</span> Difference constraints and shortest paths</h4>
</div>
</div>

<div id="outline-container-org30f2efa" class="outline-3">
<h3 id="org30f2efa"><span class="section-number-3">7.4.</span> All-Pair Shortest Paths</h3>
<div class="outline-text-3" id="text-7-4">
<p>
For convenience, we assume that the vertices are numbered \(1,2,...,|V|\), so that the input is an \(n\times n\) matrix \(W\) representing the edge weights of an n-vertex directed graph \(G=(V,E)\).<br />
That is, \(W=(w_{ij})\), where<br />
</p>
\begin{equation}
w_{ij}=
\begin{cases}
0 & \mathrm{if} \ i = j, \\
\mathrm{the\ weight\ of\ directed\ edge}\ (i,j) & \mathrm{if}\ i \ne j \ \mathrm{and}\ (i,j) \in E, \\
\infty & \mathrm{if}\ i\notin j \ \mathrm{and}\ (i,j) \notin E.
\end{cases}
\end{equation}

<p>
To solve the all pairs shortest-path problem on an input adjacency matrix, we need to compute not only the shortest-path weights but also a <b>predecssor matrix</b> \(\Pi=(\pi_{ij})\), where \(\pi_{ij}\) is the NIL if either \(i=j\) or there is on path from i to j, and otherwise \(\pi_ij\) is the predecessor of j on some shortest path from i.<br />
</p>

<p>
For each vertex \(i\in V\), we define the predecessor subgraph of G for i as \(G_{\pi,i} = (V_{\pi,i},E_{\pi,i})\), where<br />
</p>
\begin{equation}
V_{\pi,i}=\{j\in V : \pi_{ij} \ne NIL\} \cup \{i\}
\end{equation}
<p>
and<br />
</p>
\begin{equation}
E_{\pi,i}=\{ (\pi_{ij},j) : j \in V_{\pi,i} - \{i\} \}
\end{equation}


<div id="orgd12aa7f" class="figure">
<p><img src="images/print-all-pairs-shortest-path.png" alt="print-all-pairs-shortest-path.png" /><br />
</p>
</div>
</div>

<div id="outline-container-org1030c3b" class="outline-4">
<h4 id="org1030c3b"><span class="section-number-4">7.4.1.</span> Shortest paths and matrix multiplication</h4>
<div class="outline-text-4" id="text-7-4-1">
<p>
This part shows a dynamic-programming algorithm for the all-pair shortest-path problem.<br />
</p>
</div>

<ol class="org-ol">
<li><a id="orgb78de34"></a>The structure of the shortest path<br />
<div class="outline-text-5" id="text-7-4-1-1">
<p>
From the perspective the the edge: (all subpaths of a shortest path are shortest paths.)<br />
In order to compute the shortest path containing m edges, compute the shortest path containing m-1 edges first.<br />
Then<br />
</p>
\begin{equation}
\delta(i,j)=\delta(i,k) + w_{wj}
\end{equation}
</div>
</li>


<li><a id="org7ac9cd9"></a>A recursive solution to the all-pair shortest-paths problem<br />
<div class="outline-text-5" id="text-7-4-1-2">
<p>
Let \(l_{ij}^{(m)}\) be the minimum weight of any path from vertex i to vertex j that contains <b>at most</b> m edges.<br />
</p>

<p>
Initial(m=0):<br />
</p>
\begin{equation}
l_{ij}^{(0)} =
\begin{cases}
0 & \mathrm{if}\ i=j \\
\infty & \mathrm{if}\ i\ne j
\end{cases}
\end{equation} 

<p>
For \(m\ge i\):<br />
</p>
\begin{equation}
l_{ij}^{(m)} = \min\left(l_{ij}^{(m-1)}, \min_{1\le k \le n}\{l_{ik}^{(m-1)} + w_{kj}\}\right)
=\min_{1\le k \le n}\{l_{ik}^{(m-1)} + w_{kj}\}
\end{equation}
<p>
The latter equality follows since \(w_{jj}=0\) for all j.<br />
</p>

<p>
What are the actual shortest-path weights \(\delta(i,j)\)?<br />
(no nagative-weight cycles) There is a shortest path from i to j that is simple and thus contains at most n-1 edges.<br />
</p>
\begin{equation}
\delta(i,j) = l_{ij}^{(n-1)} = l_{ij}^{(n)} = l_{ij}^{(n+1)} = \cdots
\end{equation}
</div>
</li>

<li><a id="org3710bb8"></a>Computing the shortest-path weights bottom up<br />
<div class="outline-text-5" id="text-7-4-1-3">

<div id="org18eb7a1" class="figure">
<p><img src="images/c25-extend-shortest-paths.png" alt="c25-extend-shortest-paths.png" /><br />
</p>
</div>


<div id="orgdf02672" class="figure">
<p><img src="images/c25-sapsp.png" alt="c25-sapsp.png" /><br />
</p>
</div>
</div>
</li>

<li><a id="org098596a"></a>Improving the running time<br />
<div class="outline-text-5" id="text-7-4-1-4">

<div id="org820e5fe" class="figure">
<p><img src="images/c25-improving.png" alt="c25-improving.png" /><br />
</p>
</div>


<div id="org1fd2adf" class="figure">
<p><img src="images/c25-faster.png" alt="c25-faster.png" /><br />
</p>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgfc32d3b" class="outline-4">
<h4 id="orgfc32d3b"><span class="section-number-4">7.4.2.</span> The Floyd-Warshall algorithm (based on vertices)</h4>
<div class="outline-text-4" id="text-7-4-2">
</div>
<ol class="org-ol">
<li><a id="org2e23a78"></a>The structure of a shortest path<br />
<div class="outline-text-5" id="text-7-4-2-1">
<p>
The Floyd-Warshall algorithm considers the intermediate vertices of a shortest path.<br />
</p>


<div id="orgd7a16c4" class="figure">
<p><img src="images/c25-fw1.png" alt="c25-fw1.png" /><br />
</p>
</div>


<div id="org001727e" class="figure">
<p><img src="images/c25-fw2.png" alt="c25-fw2.png" /><br />
</p>
</div>

<p>
(Lemma 24.1: subpaths of shortest paths are shortest paths)<br />
</p>


<div id="org0805bbb" class="figure">
<p><img src="images/c25-fw-fig.png" alt="c25-fw-fig.png" /><br />
</p>
</div>
</div>
</li>


<li><a id="orga0048d6"></a>A recursive solution to the all-pair shortest-paths problem<br />
<div class="outline-text-5" id="text-7-4-2-2">
\begin{equation}
d_{ij}^{(0)} = w_{ij}
\end{equation}

\begin{equation}
d_{ij}^{(0)} =
\begin{cases}
w_{ij} & \mathrm{if}\ k=0 \\
\min(d_{ij}^{(k-1)}, d_{ik}^{(k-1)} + d_{kj}^{(k-1)}) & \mathrm{if} \ge 1
\end{cases}
\end{equation}
</div>
</li>

<li><a id="org895c527"></a>Computing the shortest-path weights bottom up<br />
<div class="outline-text-5" id="text-7-4-2-3">

<div id="orgcd536a3" class="figure">
<p><img src="images/c25-floyd-warshall.png" alt="c25-floyd-warshall.png" /><br />
</p>
</div>
</div>
</li>

<li><a id="org41ca0af"></a>Constucting a shortest path<br />
<div class="outline-text-5" id="text-7-4-2-4">
<p>
We compute a sequence of matrices \(\Pi^{(0)},\Pi^{(1)},...,\Pi^{(n)}\), where \(\Pi=\Pi^{(n)}\) and we define \(\pi_{ij}^{(k)}\) as the predecessor of vertex j on a shortest path from vertex i with all intermediate vertices in the set \(\{1,2,...,k\}\).<br />
</p>

\begin{equation}
\pi_{ij}^{(0)}=
\begin{cases}
NIL & \mathrm{if}\ i=j \ \mathrm{or}\ w_{ij}=\infty \\
i & \mathrm{if}\ i\ne j \ \mathrm{and}\ w_{ij}=\infty
\end{cases}
\end{equation}

\begin{equation}
\pi_{ij}^{(k)}=
\begin{cases}
\pi_{ij}^{(k-1)} & \mbox{if}\ d_{ij}^{(k-1)} \le d_{ij}^{(k-1)} + d_{kj}^{(k-1)} \\
\pi_{kj}^{(k-1)} & \mbox{if}\ d_{ij}^{(k-1)} > d_{ij}^{(k-1)} + d_{kj}^{(k-1)} \\
\end{cases}
\end{equation}
</div>
</li>

<li><a id="org7f2ab03"></a>Transitive closure of a directed graph<br />
<div class="outline-text-5" id="text-7-4-2-5">
<p>
For the purpose, we want to determine whether G contains a path from i to j for all vertex pairs \(i,j \in V\).<br />
We define the <b>transitive closure</b> of G as the graph \(G^\ast=(V,E^\ast)\) , where \(E^*=\{(i,j): \mbox{there is a path from vetex}\ i\ \mbox{to vetex}\ j\ \mbox{in}\ G\}\)<br />
</p>

<p>
Substitutes the logical operations \(\lor\) (logical OR) and \(land\) (logical AND) for the arithmetic operations min and + in the Floyd-Warshall algorithm.<br />
</p>


<div id="org4eb3468" class="figure">
<p><img src="images/c25-closure.png" alt="c25-closure.png" /><br />
</p>
</div>


<div id="org20715fa" class="figure">
<p><img src="images/c25-transitive-closure.png" alt="c25-transitive-closure.png" /><br />
</p>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgb0704f7" class="outline-4">
<h4 id="orgb0704f7"><span class="section-number-4">7.4.3.</span> Johnson's algorithm for sparse graphs</h4>
<div class="outline-text-4" id="text-7-4-3">
<p>
Johnson's algorithm finds shortest paths between all pairs in \(O(V^2\lg V + VE)\) time.<br />
The algorithm either returns a matrix of shortest-path weights for all pairs of vertices or reports that the input graph contains a negative-weight cycle.<br />
</p>

<p>
Johnson's algorithm uses the technique of reweighting, which works as follows.<br />
If all edge weights \(w\) in a graph \(G=(V,E)\) are nonnegative, we can find shortest paths between all pairs of vertices by running Dijkstra's algorithm once from each vertex;<br />
If G has negative-weight edges but no negative-weight cycles, we simply compute a new sort of nonnegative edge weights that allows us to use the same method.<br />
</p>

<p>
The new set of edge weights \(\hat{w}\) must satisfy two important properties:<br />
</p>
<ol class="org-ol">
<li>For all pairs of vertices \(u,v\in V\), a path p is a shortest path from u to v using weight function w if and only if p is also a shortest path from u to v using weight function \(\hat{w}\).<br /></li>
<li>For all edges \((u,v)\), the new weight \(\hat{w}(u,v)\) is nonnegative.<br /></li>
</ol>

<pre class="example" id="org6f9c00d">
Lemma (reweighting does not change shortest paths)
</pre>
<p>
Given a weighted directed graph \(G=(V,E)\) with weight function \(w: E \rightarrow \mathbb{R}\), let \(h:V \rightarrow \mathbb{R}\) be any function mapping vertices to real numbers.<br />
For each edge \((u,v)\in E\), define<br />
</p>
\begin{equation}
\hat{w}(u,v) = w(u,v) + h(u) - h(v).
\end{equation}
<p>
Let \(p=\langle v_0,v_1,...,v_k\rangle\) be any path from vertex \(v_0\) to vertex \(v_k\).<br />
Then p is a shortest path from \(v_0\) to \(v_k\) with weight function \(w\) if and only if it is a shortest path with weight function \(\hat{w}\).<br />
That is, \(w(p) = \delta(v_0,v_k)\) if and only if \(\hat{w} = \hat{\delta}(p) = \hat{\delta}(v_0,v_k)\).<br />
Furthermore, G has a negative-weight cycle using weight function w if and only if G has a negative-weight cycle using weight function \(\hat{w}\).<br />
</p>

<p>
(At first I think to find the \(\hat{w}\) is difficult, but after the Lemma, it shows that it is very easy to find a \(\hat{w}\).<br />
That is the charm of the math)<br />
</p>


<div id="orgb896295" class="figure">
<p><img src="images/c25-reweight.png" alt="c25-reweight.png" /><br />
</p>
</div>


<div id="org4c7f06e" class="figure">
<p><img src="images/c25-johnson.png" alt="c25-johnson.png" /><br />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgb015d29" class="outline-3">
<h3 id="orgb015d29"><span class="section-number-3">7.5.</span> Maximum Flow</h3>
<div class="outline-text-3" id="text-7-5">
</div>
<div id="outline-container-org4287a92" class="outline-4">
<h4 id="org4287a92"><span class="section-number-4">7.5.1.</span> Flow networks</h4>
<div class="outline-text-4" id="text-7-5-1">
<p>
A flow network \(G=(V,E)\) is a directed graph in which each edge \((u,v) \in E\) has a nonnegative capacity \(c(u,v) \ge 0\).<br />
(We further require that if E contains an edge \((u,v)\), then there is no edge \((u,v)\) in the reverse direction.<br />
</p>

<p>
A flow in G is a real-valued function \(f: V\times V \rightarrow \mathbb{R}\) that satisfies the following two properties:<br />
</p>
<ol class="org-ol">
<li>Capacity constraint: for all \(u,v \in V\), we require \(0\le f(u,v) \le c(u,v)\).<br /></li>
<li>Flow conservation: for all \(u\in V - \{s,t\}\), we require \(\sum\limits_{v \in V}f(v,u) = \sum\limits_{v\in V}f(u,v)\).<br /></li>
</ol>

<p>
We call the nonnegative quantity \(f(u,v)\) the flow from vertex u to vertex v.<br />
The value \(|f|\) of a flow \(f\) is defined as<br />
</p>
\begin{equation}
|f|=\sum_{v\in V} f(s,v) - \sum_{v\in V} f(v,s).
\end{equation}


<p>
In the maximum-flow problem, we are given a flow network G with source s and sink t, and we wish to find a flow of maximum value.<br />
</p>


<div id="org0915fee" class="figure">
<p><img src="images/c26-multi-source-sink.png" alt="c26-multi-source-sink.png" /><br />
</p>
</div>
</div>
</div>

<div id="outline-container-org49d945b" class="outline-4">
<h4 id="org49d945b"><span class="section-number-4">7.5.2.</span> The Ford-Fulkerson method</h4>
<div class="outline-text-4" id="text-7-5-2">

<div id="orgaabffee" class="figure">
<p><img src="images/c26-ford-fulkerson-method.png" alt="c26-ford-fulkerson-method.png" /><br />
</p>
</div>
</div>

<ol class="org-ol">
<li><a id="orgf1e40cd"></a>Residual networks<br />
<div class="outline-text-5" id="text-7-5-2-1">
<p>
Intuitively, given a flow network G and a flow f, the residual network \(G_f\) consists of edges with capacities that represent how can change the flow on edges of G.<br />
</p>

<p>
Let f be a flow in G, and consider a pair of vertices \(u,v \in V\).<br />
We define the residual capacity \(c_f(u,v)\) by<br />
</p>
\begin{equation}
c_f(u,v) =
\begin{cases}
c(u,v) - f(u,v) & \mathrm{if}\ (u,v) \in E, \\
f(v,u) & \mathrm{if}\ (v,u) \in E, \\
0 & \mathrm{otherwise}
\end{cases}
\end{equation}



<p>
Given a flow network \(G=(V,E)\) and a flow f, the residual network of G induced by f is \(G_f=(V,E)\), where<br />
</p>
\begin{equation}
E_f = \{ (u,v) \in V \times V : c_f(u,v) > 0 \}
\end{equation}


<p>
A flow in a residual network provides a roadmap for adding flow to the original flow network.<br />
If f is a flow in G and \(f^{'}\) is a flow in the corresponding residual network \(G_f\), we define \(f \uparrow f^{'}\), to be a function from \(V\times V\) to \(\mathbb{R}\), defined by<br />
</p>
\begin{equation}
(f \uparrow f^{'})(u,v) =
\begin{cases}
f(u,v) + f^{'}(u,v) - f^{'}(v,u) & \mathrm{if}\ (u,v) \in E, \\
0 & \mathrm{otherwise}
\end{cases}
\end{equation}


<div id="org9723713" class="figure">
<p><img src="images/c26-residual-network.png" alt="c26-residual-network.png" /><br />
</p>
</div>

<p>
Lemma<br />
</p>
\begin{equation}
|f \uparrow f^{'}| = |f| + |f^{'}|
\end{equation}
</div>
</li>

<li><a id="org7c309ae"></a>Augmenting paths<br />
<div class="outline-text-5" id="text-7-5-2-2">
<p>
Given a flow network \(G=(V,E)\) and a flow f, an augmenting path p is a simple path from s to t in the residual network \(G_f\).<br />
</p>

<p>
We call the maximum amount by which we can increase the flow on each edge in an augmenting path p the residual capacity of p, given by<br />
</p>
\begin{equation}
c_f(p) = \min\{ c_f(u,v) : (u,v) \mathrm{\ is\ on}\ p \}. 
\end{equation}



<div id="org7d6526c" class="figure">
<p><img src="images/c26-lemma-26-2.png" alt="c26-lemma-26-2.png" /><br />
</p>
</div>
</div>
</li>


<li><a id="org66c29d2"></a>Cuts of flow networks<br />
<div class="outline-text-5" id="text-7-5-2-3">
<p>
A cut (S,T) of flow network \(G=(V,E)\) is a partition of V into S and T=V-S such that \(s\in S\) and \(t\in T\).<br />
If f is a flow, then the net flow \(f(S,T)\) across the cut (S,T) is defined to be<br />
</p>
\begin{equation}
f(S,T) = \sum_{u\in S} \sum_{v\in T} f(u,v) - \sum_{u\in S} \sum_{v\in T} f(v,u).
\end{equation}

<p>
The capacity of the cut (S,T) is<br />
</p>
\begin{equation}
c(S,T) = \sum_{u\in S} \sum_{v\in T} c(u,v).
\end{equation}

<p>
A minimum cut of a network is a cut whose capacity is minimum over all cuts of the network.<br />
</p>


<div id="orgaefee7c" class="figure">
<p><img src="images/c26-lemma-26-4.png" alt="c26-lemma-26-4.png" /><br />
</p>
</div>


<div id="org3ad9024" class="figure">
<p><img src="images/c26-theorem.png" alt="c26-theorem.png" /><br />
</p>
</div>
</div>
</li>

<li><a id="orge371df9"></a>The basic Ford-Fulkerson algorithm<br />
<div class="outline-text-5" id="text-7-5-2-4">

<div id="org22a444b" class="figure">
<p><img src="images/c26-ford-fulkerson-algorithm.png" alt="c26-ford-fulkerson-algorithm.png" /><br />
</p>
</div>



<div id="org9835fdc" class="figure">
<p><img src="images/c26-ford-fulkerson-fig.png" alt="c26-ford-fulkerson-fig.png" /><br />
</p>
</div>


<div id="orge3b50d5" class="figure">
<p><img src="images/c26-ford-fulkerson-fig2.png" alt="c26-ford-fulkerson-fig2.png" /><br />
</p>
</div>


<p>
(If \(f^*\) denote a maximum flow in the integral network, then a staightforward inplementation of FORD-FULKERSON executes the while loop of line 3-8 at most \(|f^*|\) times, since the flow value increase by at least one unit in each iteration.<br />
When the capacities are integral and the optimal flow value \(|f^*|\) is small, the running time of the Ford-Fulkerson algorithm is good)<br />
</p>
</div>
</li>

<li><a id="org4c3a03e"></a>The Edmonds-Karp algorithm<br />
<div class="outline-text-5" id="text-7-5-2-5">
<p>
We can improve the bound on FORD-FULKERSON by finding the augmenting path p in line 3 with a breadth-first search.<br />
That is, we choose the augmenting path as a shortest path from s to t in the residual network, where each edge has unit distance (weight).<br />
We call the Ford-Fulkerson method so implemented the Edmonds-Karp algorithm.<br />
</p>


<div id="orgedec5b6" class="figure">
<p><img src="images/c26-lemma-7.png" alt="c26-lemma-7.png" /><br />
</p>
</div>


<div id="org0e8eb18" class="figure">
<p><img src="images/c26-lemma-8.png" alt="c26-lemma-8.png" /><br />
</p>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-org50091fc" class="outline-4">
<h4 id="org50091fc"><span class="section-number-4">7.5.3.</span> Maximum bipartite matching</h4>
<div class="outline-text-4" id="text-7-5-3">
<p>
Some combinatorial problems seem to on the surface to have little to do with flow network, but can in face be reduced to maximum-flow problem.<br />
</p>
</div>

<ol class="org-ol">
<li><a id="orgccc51b7"></a>The maximum-bipartite-matching problem<br />
<div class="outline-text-5" id="text-7-5-3-1">
<p>
Given an undirected graph \(G=(V,E)\), a matching is a subset of edges \(M \subset E\) such that for all vertices \(v \in V\), at most one edge of M is incident on v.<br />
We say that a vertex \(v \in V\) is matched by the matching M if some edge in M is incident on v; otherwise v is unmatched.<br />
A maximum matching is a matching of maximum cardinality, that is, a matching M that for any matching \(M^{'}\), we have \(|M| \ge |M^{'}|\).<br />
</p>


<div id="org2098232" class="figure">
<p><img src="images/c26-bipartite.png" alt="c26-bipartite.png" /><br />
</p>
</div>
</div>
</li>

<li><a id="orgfc3f5c4"></a>Finding a maximum bipartite matching<br />
<div class="outline-text-5" id="text-7-5-3-2">
<p>
We define the corresponding flow network \(G'=(V',E')\) for the bipartite graph G as follows.<br />
We let the source s and sink t to be new vertices not in V, and we let \(V'=V\cup \{s,t\}\).<br />
If the vertex patition of G is \(V=L\cup R\), the directed edges of \(G'\) are the edges of E, directed from L to R, along with \(|V|\) new directed edges:<br />
</p>
\begin{equation}
E'=\{ (s,u): u \in L \} \cup \{ (u,v): (u,v) \in E \} \cup \{ (v,t): v \in R \}.
\end{equation}
<p>
To complete the construction, we assign unit capacity to each eage in \(E'\).<br />
</p>

<p>
We say that a flow \(f\) on a flow network \(G=(V,E)\) is integer-valued if \(f(u,v)\) is an integer for all \((u,v) \in V \times V\).<br />
</p>


<div id="orge5af794" class="figure">
<p><img src="images/c26-lemma-9.png" alt="c26-lemma-9.png" /><br />
</p>
</div>


<div id="org4e96007" class="figure">
<p><img src="images/c26-theorem-10.png" alt="c26-theorem-10.png" /><br />
</p>
</div>


<div id="org38da06f" class="figure">
<p><img src="images/c26-corollary-11.png" alt="c26-corollary-11.png" /><br />
</p>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org6a9f022" class="outline-2">
<h2 id="org6a9f022"><span class="section-number-2">8.</span> Selected Topics</h2>
<div class="outline-text-2" id="text-8">
</div>
<div id="outline-container-orga1e919f" class="outline-3">
<h3 id="orga1e919f"><span class="section-number-3">8.1.</span> Multithreaded Algorithms</h3>
<div class="outline-text-3" id="text-8-1">
<p>
uniprocessor computer: in which only one instruction executes at a time.<br />
parallel computers: computers with multiple processing units.<br />
</p>

<p>
One common means of programming chip multiprocessors and other shared-memory parallel computers is by using static threading, which provides a software abstraction of "virtual processor", or <b>threads</b>, sharing a common memory.<br />
Although the operating system allows programmers to create and destroy threads, these operations are comparatively slow.<br />
Thus, for most applications, threads persist for the duration of a computation, which is why we call them "static".<br />
</p>

<p>
Dynamically partitioning the work among the threads so that each receives approximately the same load turns out to be a complicated undertaking. (Leading to concurrency platforms)<br />
concurrency platforms: which provide a layer of software that coodinates, schedules, and manages the parallel-computing resources.<br />
</p>
</div>

<ol class="org-ol">
<li><a id="orgcde262d"></a>Dynamic multithreaded programming<br />
<div class="outline-text-5" id="text-8-1-0-1">
<p>
One important class of concurrency platform is <b>dynamic multithreading</b>.<br />
Dynamic multithreading allows programmers to specify parallelism in application withou worrying about communication protocols, load balancing, and other vagaries of static-thread programing.<br />
Almost all of dynamic-multithreading enviroment support two features:<br />
</p>
<ol class="org-ol">
<li>nested parallelism<br /></li>
<li>parallel loops<br /></li>
</ol>

<p>
We can describe a multithreaded algorithm by adding to our pseudocode just three "concurrency" keywords: parallel, spwan, and sync.<br />
</p>
</div>
</li>
</ol>

<div id="outline-container-org8d05dc3" class="outline-4">
<h4 id="org8d05dc3"><span class="section-number-4">8.1.1.</span> The basics of dynamic multithreading</h4>
<div class="outline-text-4" id="text-8-1-1">
</div>
<ol class="org-ol">
<li><a id="org77f331a"></a>A model for multithreaded execution<br />
<div class="outline-text-5" id="text-8-1-1-1">
<p>
It helps to think of a multithreaded computation as a directed acyclic graph \(G=(V,E)\), called a computation dag.<br />
</p>

<pre class="example" id="org04f2d7d">
P-FIB(n)
  if n &lt;= 1
    return n
  else x = spawn P-FIB(n-1)
    y = P-FIB(n-2)
    sync
    return x + y
</pre>


<div id="org01e68e3" class="figure">
<p><img src="images/c27-p-fib-fig.jpg" alt="c27-p-fib-fig.jpg" /><br />
</p>
</div>


<p>
An ideal parallel computer consists of:<br />
</p>
<ol class="org-ol">
<li>a set of processors<br /></li>
<li>a sequentially consistent shared memeory<br /></li>
</ol>


<p>
Sequential consistency means that the shared memory produces the same results as if at each step, exactly one instruction from one the processors is executed.<br />
That is, the memory behaves as if the instructions were executed sequentially according to some global linear order that preserves the individual orders in which each processor issues its own instructions.<br />
</p>
</div>
</li>


<li><a id="orgccb5ce4"></a>Performance measures<br />
<div class="outline-text-5" id="text-8-1-1-2">
<p>
We can gauge the theoretical efficiency of a multithreaded algorithm by using two metrics: "work" and "span".<br />
The work of a multithreaded computation is the total time to execute the entire computation on one processor.<br />
In other words, the work is the sum of the times taken by each of the strands.<br />
The span is the longest time to execute the strands along any path in the dag.<br />
</p>


<p>
The work and span provide lower bounds on the running time \(T_P\) of a multithreaded computation on \(P\) processors:<br />
</p>
<ol class="org-ol">
<li>work law: \(T_P \ge \frac{T_1}{P}\)<br /></li>
<li>span law: \(T_P \ge T_\infty\) <br /></li>
</ol>


<p>
\(T_P\) denotes the running time on processor \(P\)<br />
\(T_1\) denotes the running time of total work, i.e. only running on one processor<br />
\(T_\infty\) denotes the running time on unlimited processors,<br />
</p>


<p>
We define the <b>speedup</b> of a computation on \(P\) processors by the ratio \(\frac{T_1}{T_P}\), which says how many time faster the computation is on \(P\) processors than on 1 processor.<br />
When \(T_1/T_P = \Theta(P)\), the speedup exhibits linear speedup, and when \(T_1/T_P = P\), we have perfect linear speedup.<br />
</p>

<p>
We define the parallel slackness of a multithreaded computation executed on an ideal parallel computer with \(P\) processors to be the ratio \((T_1/T_\infty)/P = T_1/(PT_=infty)\), which is the factor by which the parallelism of the computation exceeds the number of processors in the machine.<br />
</p>
</div>
</li>


<li><a id="orgdce40e0"></a>Scheduling<br />
<div class="outline-text-5" id="text-8-1-1-3">
<p>
A multithreaded scheduler must schedule the computation with no advance knowledge of when strands will spawn or when they will complete &#x2013; it must operate on-line.<br />
</p>

<p>
To keep our analysis simple, we investigate an on-line centralized scheduler, which knows the global state of the computation at any given time.<br />
greedy scheduler: assign as many strands to processors as possible in each time step.<br />
</p>

<p>
If at least P strands are ready to execute during a time step, we say that the step is a <b>complete step</b>, and a greedy scheduler assigns any P of the ready strands to processors.<br />
Otherwise, fewer than P strands are ready to execute, in which case we say that the step is an <b>incomplete step</b>, and the scheduler assigns each ready strand to its own processor.<br />
</p>

<hr />
<p>
Theorem 27.1<br />
On an ideal parallel computer with \(P\) processors, a greedy scheduler executes a multithreaded computation with work \(T_1\) and span \(T_\infty\) in time<br />
</p>
\begin{equation}
T_P \le T_1/P + T_\infty
\end{equation}
<hr />

<p>
complete situation: \(\le \lfloor T_1/P \rfloor\)<br />
incomplete situation: \(< T_\infty\)<br />
</p>

<hr />
<p>
Corollary 27.2<br />
The running time \(T_P\) of any multithreaded computation scheduled by a greedy scheduler on an ideal parallel computer with \(P\) processors is within a factor of 2 of optimal<br />
</p>
<hr />


<hr />
<p>
Corollary 27.3<br />
Let \(T_P\) be the running time of a multithreaded computation produced by a greedy scheduler on an ideal parallel computer with P processors,<br />
and let \(T_1\) and \(T_\infty\) be the work and span of the computation, respectively.<br />
Then, if \(P \ll T_1/T_\infty\), we have \(T_P \approx T_1/P\), or equivalently, a speedup of approximately \(P\).<br />
</p>
<hr />
</div>
</li>

<li><a id="orgd5a0eeb"></a>Analyzing multithreaded algorithms<br />
<div class="outline-text-5" id="text-8-1-1-4">
<p>
If two subcomputations are joined in series, their spans add to form the span of their composition, whereas if they are joined in parallel, the span of their composition is the maximum of the spans of the two subcomputations.<br />
</p>


<div id="org8354a49" class="figure">
<p><img src="images/c27-span.png" alt="c27-span.png" /><br />
</p>
</div>
</div>
</li>


<li><a id="org4467441"></a>Parallel loops<br />
<div class="outline-text-5" id="text-8-1-1-5">
<p>
For example:<br />
\[A=(a_{ij})_{n\times n}\]<br />
\[x=(x_j)_{n\times 1}\]<br />
The result of the multiplication is \(y\):<br />
\[y_i = \sum_{j=1}^{n} a_{ij}x_j\]<br />
</p>


<div id="orgd5386d0" class="figure">
<p><img src="images/c27-mat-vec.png" alt="c27-mat-vec.png" /><br />
</p>
</div>

<p>
A compiler can implement each parallel for loop as a divide-and-conquer subroutine using nested parallelism.<br />
For example: MAT-VEC-MAIN-LOOP(A,x,y,n,1,n)<br />
</p>


<div id="orgc718d49" class="figure">
<p><img src="images/c27-mat-vec-main-loop.png" alt="c27-mat-vec-main-loop.png" /><br />
</p>
</div>



<div id="org204e762" class="figure">
<p><img src="images/c27-mat-vec-main-loop-fig.png" alt="c27-mat-vec-main-loop-fig.png" /><br />
</p>
</div>
</div>
</li>


<li><a id="orgbc5c202"></a>Race conditions<br />
<div class="outline-text-5" id="text-8-1-1-6">
<p>
A multithreaded algorithm is deterministic if it always does the same thing on the same input, no matter how the instructions are scheduled on the multicore computer.<br />
It is nondeterministic if its behavior might vary from run to run.<br />
Often, a multithreaded algorithm that is intended to be deterministic fails to be, because it contains a "deterministic race".<br />
</p>

<p>
<b>Race conditions</b> are the bane (<i>bein</i>) of concurrency.<br />
</p>

<p>
A determinacy race occurs when two logically parallel instructions access the same memory location and at least on of the instructions performs a write.<br />
</p>


<pre class="example" id="orga43c72c">
RACE-EXAMPLE()
  x = 0
  parallel for i = 1 to 2
    x = x + 1
  print x
</pre>


<pre class="example" id="org4eda4db">
When a processor increments x, the operation is not indivisible, but is composed of a sequence of instructions:
1. Read x from memory into one of the processor's regitsters.
2. Increment the value in the register.
3. Write the value in the register back into x in memory.
</pre>



<div id="org4baca2f" class="figure">
<p><img src="images/c27-race-fig.png" alt="c27-race-fig.png" /><br />
</p>
</div>

<p>
The bug resides in that processor 1 haven't execute all its instructions tightly.<br />
</p>


<p>
Of course, many execution do not elicit the bug.<br />
For example, if the execution oder were \(\langle 1,2,3,7,4,5,6,8 \rangle\) or \(\langle 1,4,5,6,2,3,7,8 \rangle\), we would get the correct result.<br />
That’s the problem with determinacy races. Generally, most orderings produce correct results.<br />
But some orderings generate improper results when the instructions interleave.<br />
Consequently, races can be extremely hard to test for.<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgb1ee2ad" class="outline-4">
<h4 id="orgb1ee2ad"><span class="section-number-4">8.1.2.</span> Multithreaded matrix multiplication</h4>
<div class="outline-text-4" id="text-8-1-2">
</div>
<ol class="org-ol">
<li><a id="org306b6ba"></a>Multithreaded matrix multiplication<br />
<div class="outline-text-5" id="text-8-1-2-1">

<div id="org63a7b23" class="figure">
<p><img src="images/c27-p-square-matrix-multiply.png" alt="c27-p-square-matrix-multiply.png" /><br />
</p>
</div>

\begin{equation}
parallelism = \frac{T_1(n)}{T_\infty(n)} = \frac{\Theta(n^3)}{\Theta(n)} = \Theta(n^2)
\end{equation}
</div>
</li>

<li><a id="orgd7ea54d"></a>A divide-and-conquer multithreaded algorithm for matrix multiplication<br />
<div class="outline-text-5" id="text-8-1-2-2">
\begin{equation}
A = \begin{bmatrix}
A_{11} & A_{12} \\
A_{21} & A_{22}
\end{bmatrix},
B = \begin{bmatrix}
B_{11} & B_{12} \\
B_{21} & B_{22}
\end{bmatrix},
C = \begin{bmatrix}
C_{11} & C_{12} \\
C_{21} & C_{22}
\end{bmatrix}.
\end{equation}

\begin{equation}
\begin{bmatrix}
C_{11} & C_{12} \\
C_{21} & C_{22}
\end{bmatrix} =
\begin{bmatrix}
A_{11} & A_{12} \\
A_{21} & A_{22}
\end{bmatrix}
\begin{bmatrix}
B_{11} & B_{12} \\
B_{21} & B_{22}
\end{bmatrix}=
\begin{bmatrix}
A_{11}B_{11} & A_{12}B_{12} \\
A_{21}B_{21} & A_{22}B_{21}
\end{bmatrix} +
\begin{bmatrix}
A_{12}B_{21} & A_{12}B_{22} \\
A_{22}B_{21} & A_{22}B_{22}
\end{bmatrix}.
\end{equation}


<div id="org9a5d7bd" class="figure">
<p><img src="images/c27-p-matrix-multiply-recursive.png" alt="c27-p-matrix-multiply-recursive.png" /><br />
</p>
</div>

<p>
\(M_1(n) = 8M_1(n/2) + \Theta(n^2) = \Theta(n^3)\)<br />
\(M_{\infty}(n) = M_{\infty}(n/2) + \Theta(\lg n)\)<br />
</p>

<hr />
<p>
&Theta;(lg n) ?<br />
</p>
<hr />

\begin{equation}
parallelism = \frac{M_1(n)}{M_{\infty}(n)} = \Theta(n^3/ \lg^2 n) 
\end{equation}
</div>
</li>
</ol>
</div>

<div id="outline-container-org9536363" class="outline-4">
<h4 id="org9536363"><span class="section-number-4">8.1.3.</span> Multithreaded merge sort</h4>
<div class="outline-text-4" id="text-8-1-3">

<div id="orgfebd219" class="figure">
<p><img src="images/c27-multithreaded-merge-sort1.png" alt="c27-multithreaded-merge-sort1.png" /><br />
</p>
</div>

\begin{equation}
T_1(n) = 2T_1(n/2) + \Theta(n) = \Theta(n\lg n)
\end{equation}

\begin{equation}
T_{\infty}(n) = T_{\infty}(n/2) + \Theta(n) = \Theta(n)
\end{equation}

\begin{equation}
parallelism = \frac{M_1}{M_{\infty}} = \Theta(\lg n)
\end{equation}

<p>
The parallelism bottleneck is the serial MERGE procedure.<br />
Although merging might initially seem to be inherently serial, we can, in fact, fashion a multithreaded version of it by using nested parallelism.<br />
</p>


<div id="org29d2b36" class="figure">
<p><img src="images/c27-multithreaded-merge.png" alt="c27-multithreaded-merge.png" /><br />
</p>
</div>


<ul class="org-ul">
<li>merge \(T[p_1..r_1]\) and \(T[p_2..r_2]\) into \(A[p_3..r_3]\)<br /></li>
<li>\(n_1 = r_1 - p_1 + 1\)<br /></li>
<li>\(n_2 = r_2 - p_2 + 1\)<br /></li>
<li>\(n_3 = r_3 - p_3 + 1\)<br /></li>
<li>suppose that \(n_1 \ge n_2\)<br /></li>
<li>find the middle element \(x=T[q_1]\) of \(T[p_1..r_1]\)<br /></li>
<li>use binary search to find the index \(q_2\) in \(T[p_2..r_2]\)<br /></li>
<li>set \(q_3 = p_3 + (q_1 - p_1) + (q_2 - p_2)\)<br /></li>
<li>copy \(x\) into \(A[q_3]\)<br /></li>
<li>recursively merge \(T[p_1..q_1-1]\) with \(T[p_2..q_2-1]\) and place the result into \(A[p_3..q_3-1]\)<br /></li>
<li>recursively merge \(T[q_1+1..r_1]\) with \(T[q_2..r_2]\) and place the result into \(A[p_3..r_3]\)<br /></li>
</ul>


<p>
The base caes occurs when \(n_1 = n_2 =0\), in which case we have no work to do to merge the two empty subarrays.<br />
</p>



<div id="org324c794" class="figure">
<p><img src="images/c27-binary-search.png" alt="c27-binary-search.png" /><br />
</p>
</div>



<div id="orgba5d9b4" class="figure">
<p><img src="images/c27-p-merge.png" alt="c27-p-merge.png" /><br />
</p>
</div>
</div>
</div>


<div id="outline-container-org8eeffbc" class="outline-4">
<h4 id="org8eeffbc"><span class="section-number-4">8.1.4.</span> Analysis of multithreaded merging</h4>
<div class="outline-text-4" id="text-8-1-4">
<p>
For the span \(PM_{\infty}(n)\) of P-MERGE, the key is to understand that in the worst case, the maximum nubmer of elements in eighter of the recursive calls can be at most \(3n/4\).<br />
In the worst case, one of the two recursive calls merges \(\lfloor n_1/2 \rfloor\) elements of \(T[p_1..r_1]\) with all \(n_2\) elements of \(T[p_2..r_2]\),<br />
and hence the number of elements involved in calles is<br />
</p>
\begin{equation}
\lfloor n_1/2 \rfloor + n_2 \le n_1/2 + (n_2/2 + n_2/2) = (n_1 + n_2)/2 + n_2/2 \le n/2 + n/4 = 3n/4
\end{equation}


\begin{equation}
PM_{\infty}(n) = PM_{\infty}(3n/4) + \Theta(\lg n) = \Theta(\lg^2 n)
\end{equation}

<p>
Since each of the \(n\) elements must be copied from array \(T\) to array \(A\), we have \(PM_1(n) = \Omega(n)\).<br />
</p>


\begin{equation}
PM_1(n) = PM_1(\alpha n) + PM_1((1-\alpha)n) + O(\lg n) = O(n)
\end{equation}
<p>
where \(\alpha\) lies in the range \(1/4 \le \alpha \le 3/4\).<br />
</p>

\begin{equation}
PM_1(n) = \Theta(n)
\end{equation}

\begin{equation}
parallelism = \frac{PM_1(n)}{PM_{\infty}} = \Theta(n/\lg^2n)
\end{equation}
</div>
</div>


<div id="outline-container-org8d807a2" class="outline-4">
<h4 id="org8d807a2"><span class="section-number-4">8.1.5.</span> Multithreaded merge sort</h4>
<div class="outline-text-4" id="text-8-1-5">

<div id="org763c63b" class="figure">
<p><img src="images/c27-p-merge-sort.png" alt="c27-p-merge-sort.png" /><br />
</p>
</div>
</div>
</div>

<div id="outline-container-org8cb5074" class="outline-4">
<h4 id="org8cb5074"><span class="section-number-4">8.1.6.</span> Analysis of multithreaded merge sort</h4>
<div class="outline-text-4" id="text-8-1-6">
\begin{equation}
PMS_1(n) = 2PMS_1(n/2) + PM_1(n) = 2PMS_1(n/2) + \Theta(n) = \Theta(n\lg n)
\end{equation}

\begin{equation}
PMS_{\infty}(n) = PMS_{\infty}(n/2) + PMS_{\infty}(n) = PMS_{\infty}(n/2) + \Theta(\lg^2 n) = \Theta(ln^3 n)
\end{equation}

\begin{equation}
parallelism = \frac{PMS_1(n)}{PMS_{\infty}(n)} =\frac{\Theta(n \lg n)}{\Theta(\lg^3 n)} = \Theta(\frac{n}{lg^2 n})
\end{equation}


<pre class="example" id="org6f9fd24">
A good implementation in practice would sacrifice some parallelism by coarsening the base case 
in order to reduce the constants hidden by the asymptotic notation. 
The straightforward way to coarsen the base case is to switch to an ordinary serial sort, perhaps quicksort,
when the size of the array is sufficiently small.
</pre>
</div>
</div>
</div>


<div id="outline-container-org5f6747f" class="outline-3">
<h3 id="org5f6747f"><span class="section-number-3">8.2.</span> Matrix Operations</h3>
<div class="outline-text-3" id="text-8-2">
<p>
Due to the limited precision of floating-point representations in actual computers, round-off errors in numerical computations may become amplified over the course of a computation, leading to incorrect results; we call such computations numerically unstable.<br />
</p>
</div>

<div id="outline-container-org11c13df" class="outline-4">
<h4 id="org11c13df"><span class="section-number-4">8.2.1.</span> Solving systems of linear equations</h4>
<div class="outline-text-4" id="text-8-2-1">
<p>
The problem:<br />
</p>

\begin{equation}
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn} \\
\end{pmatrix}
\begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{pmatrix}
=
\begin{pmatrix}
b_1 \\
b_2 \\
\vdots \\
b_n
\end{pmatrix}
\end{equation}

<p>
or equivalently, letting \(A=(a_{ij})\), \(x=(x_i)\), and \(b=(b_i)\) as<br />
</p>
\begin{equation}
Ax = b
\end{equation}

<p>
If \(A\) is nonsigular, it possesses an inverse \(A^{-1}\), and<br />
</p>
\begin{equation}
x = A^{-1}b
\end{equation}

<pre class="example" id="orge7f8e8d">
This approch suffers in practice from numberical instability.
The approach LUP decomposition is numberically stable and 
has the further advantage of being faster in practice.
</pre>
</div>


<ol class="org-ol">
<li><a id="org5484872"></a>Overview of LUP decomposition<br />
<div class="outline-text-5" id="text-8-2-1-1">
<p>
The idea behind LUP decomposition is to find three \(n\times n\) matrices \(L\), \(U\), and \(P\) such that<br />
</p>
\begin{equation}
PA = LU
\end{equation}
<p>
where<br />
</p>
<ul class="org-ul">
<li>L is a unit lower-triangular matrix<br /></li>
<li>U is an upper triangular matrix<br /></li>
<li>P is a permutation matrix<br /></li>
</ul>

<pre class="example" id="org9cc51c8">
Every nonsigular matrix A possesses such a decompsition.
</pre>

\begin{equation}
LUx = Pb
\end{equation}

\begin{equation}
Ly = Pb
\end{equation}

\begin{equation}
Ux = y
\end{equation}
</div>
</li>

<li><a id="org32f3d25"></a>Forward and back substitution<br />
<div class="outline-text-5" id="text-8-2-1-2">
<p>
Forward substitution can solve the lower-triangular system (\(Ly = Pb)\) in \(\Theta(n^2)\) time.<br />
</p>


<div id="orgdfc3d47" class="figure">
<p><img src="images/c28-forward.png" alt="c28-forward.png" /><br />
</p>
</div>

<p>
The solution is:<br />
\(y_i = b_{\pi[i]} - \sum\limits_{j=1}^{i-1} l_{ij}y_j\)<br />
</p>

<p>
Back substitution can solve the upper-triangular system (\(Ux = y\)) in \(\Theta(n^2)\) time.<br />
</p>


<div id="orgec450f8" class="figure">
<p><img src="images/c28-back.png" alt="c28-back.png" /><br />
</p>
</div>

<p>
The solution is:<br />
\(x_i = \left ( y_i - \sum\limits_{j=i+1}^{n} u_{ij}x_j \right ) / u_{ii}\)<br />
</p>

<p>
The pseudocode for solving x:<br />
</p>


<div id="org08e9392" class="figure">
<p><img src="images/c28-lup-solve.png" alt="c28-lup-solve.png" /><br />
</p>
</div>
</div>
</li>


<li><a id="orgd464641"></a>Computing an LUP decomposition<br />
<div class="outline-text-5" id="text-8-2-1-3">
<p>
Suppose \(P = I_n\), then \(A=LU\), and we call the two matrices \(L\) and \(u\) an \(LU\) decomposition of A.<br />
</p>

<p>
We use a process known as Gaussian elimination to create an LU decomposition.<br />
How to do this?<br />
</p>
<ol class="org-ol">
<li>substracting multiples of the first equations<br /></li>
<li>substracting multiples of the second equations<br /></li>
<li>continue, until the system that remains has an upper-triangular form, i.e. the matrix \(U\)<br /></li>
<li>the matrix \(L\) is made up of the row multipliers<br /></li>
</ol>

<p>
The algorithm to implement the LU decomposition is recursive.<br />
</p>


<div id="org9e85945" class="figure">
<p><img src="images/c28-lu1.png" alt="c28-lu1.png" /><br />
</p>
</div>


<div id="org4c03aa9" class="figure">
<p><img src="images/c28-lu2.png" alt="c28-lu2.png" /><br />
</p>
</div>

<pre class="example" id="org4e3ed4a">
Take a_11 and 0s as the core to constructe the decomposition.
</pre>

<p>
\(A' - vw^{\mathrm{T}} / a_{11\)} is called the Schur complement of A with respect to \(a_{11}\).<br />
</p>

<pre class="example" id="orgb00e7b7">
If A is nonsigular, then the Schur complement is nonsigular.
</pre>

<p>
Suppose<br />
</p>
\begin{equation}
A' - vw^{\mathrm{T}}/a_{11} = L'U'
\end{equation}
<p>
where \(L'\) is unit lower-triangular and \(U'\) is upper-triangular. Then<br />
</p>


<div id="org823c3f8" class="figure">
<p><img src="images/c28-lu3.png" alt="c28-lu3.png" /><br />
</p>
</div>

<hr />
<p>
If \(a_{11} = 0\), this method doesn't work, because it divides by 0.<br />
It doesn't work if the upper leftmost entry of the Schur complement \(A' - vw^{\mathrm{T}}/a_{11}\) is 0, since we divide by it in the next step of the recursion.<br />
The elements by which we divide during LU decomposition are called <b>pivots</b>, and they occupy the diagonal elements of the matrix U.<br />
The reason we include a permutation matrix P during LUP decomposition is that it allows us to avoid dividing by 0.<br />
When we use permutations to avoid division by 0, we are pivoting.<br />
</p>
<hr />


<div id="org2f8ab42" class="figure">
<p><img src="images/c28-lu-decomposition.png" alt="c28-lu-decomposition.png" /><br />
</p>
</div>



<div id="org2d26af3" class="figure">
<p><img src="images/c28-lu-fig.png" alt="c28-lu-fig.png" /><br />
</p>
</div>
</div>
</li>


<li><a id="org0c8a34d"></a>Computing an LUP decomposition<br />
<div class="outline-text-5" id="text-8-2-1-4">
<p>
To avoid dividing by 0 and a small value, use matrix P to multiply the matrix A.<br />
</p>

<p>
Before we partition the matrix A, we move a nonzero element, say \(a_{k1}\), from somewhere in the first column to the \((1,1)\) position of the matrix.<br />
For numerical stability, we choose \(a_{k1}\) as the element in the first column with the greatest absolute value.<br />
In order to preserve the set of equations, we exchange row 1 with row k, which is equivalent to multiplying A by a permutation matrix Q on the left.<br />
</p>
\begin{equation}
QA = 
\begin{pmatrix}
a_{k1} & w^{\mathrm{T}} \\
v & A'
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 \\
v/a_{k1} & I_{n-1}
\end{pmatrix}
\begin{pmatrix}
a_{k1} & w^{\mathrm{T}} \\
0 & A' - vw^{\mathrm{T}} / a_{k1}
\end{pmatrix}
\end{equation}

<p>
If A is nonsigular, then the Schur complement \(A' - vw^{\mathrm{T}}/a_{k1}\) is nonsigular, too.<br />
Therefore we can recursively find an LUP decomposition for it.<br />
</p>
\begin{equation}
P'(A' - vw^{\mathrm{T}}/a_{k1}) = L'U'
\end{equation}

<p>
Define<br />
</p>
\begin{equation}
P = 
\begin{pmatrix}
1 & 0 \\
0 & P' 
\end{pmatrix}
Q
\end{equation}

<p>
Then<br />
</p>


<div id="orgddd6208" class="figure">
<p><img src="images/c28-lup.png" alt="c28-lup.png" /><br />
</p>
</div>


<div id="org76f6577" class="figure">
<p><img src="images/c28-lup-decomposition.png" alt="c28-lup-decomposition.png" /><br />
</p>
</div>


<div id="org03e8cf4" class="figure">
<p><img src="images/c28-lup-fig.png" alt="c28-lup-fig.png" /><br />
</p>
</div>
</div>
</li>
</ol>
</div>


<div id="outline-container-org02bf121" class="outline-4">
<h4 id="org02bf121"><span class="section-number-4">8.2.2.</span> Inverting matrics</h4>
<div class="outline-text-4" id="text-8-2-2">
</div>
<ol class="org-ol">
<li><a id="org6b498c5"></a>Computing a matrix inverse from a LUP decomposition<br />
<div class="outline-text-5" id="text-8-2-2-1">
<p>
Suppose that we have a LUP decompostion of a matrix A, using LUP-SOLVE, we can solve an equation of the form \(Ax=b\) in time \(\Theta(n^2)\).<br />
Since the LUP decomposition depends on A but not b, we can run LUP-SOLVE on a second set of equations of the form \(Ax=b'\) in additional time \(\Theta(n^2)\).<br />
We can think the equation<br />
</p>
\begin{equation}
AX=I_n
\end{equation}
<p>
as a set of n distinct equations of the form \(Ax=b\).<br />
</p>

<p>
Once we have the LUP decomposition, we can compute each of the n columns \(X_i\) in time \(\Theta(n^2)\), and so we can compute X from the LUP decomposition of A in time \(\Theta(n^2)\).<br />
</p>
</div>
</li>

<li><a id="org2e8f880"></a>Matrix multiplication and matrix inversion<br />
<div class="outline-text-5" id="text-8-2-2-2">

<div id="orga5d29a3" class="figure">
<p><img src="images/c28-theorem1.png" alt="c28-theorem1.png" /><br />
</p>
</div>


<div id="org33a7c45" class="figure">
<p><img src="images/c28-theorem1-proof.png" alt="c28-theorem1-proof.png" /><br />
</p>
</div>



<div id="orge3a7cd0" class="figure">
<p><img src="images/c28-theorem2.png" alt="c28-theorem2.png" /><br />
</p>
</div>
</div>
</li>
</ol>
</div>
</div>



<div id="outline-container-orgdd178f4" class="outline-3">
<h3 id="orgdd178f4"><span class="section-number-3">8.3.</span> Polynomials and FFT</h3>
<div class="outline-text-3" id="text-8-3">
</div>
<ol class="org-ol">
<li><a id="org5e23543"></a>Polynomials<br />
<div class="outline-text-5" id="text-8-3-0-1">
<p>
A polynomial in the variable x over an algebraic field F represents a function \(A(x)\) as a formal sum:<br />
</p>
\begin{equation}
A(x) = \sum_{j=0}^{n-1} a_jx^j
\end{equation}

<p>
We call the values \(a_0,a_1,...,a_{n-1}\) the coefficients of the polynomial.<br />
The coefficients are drawn from a field F, typically the set \(\mathbb{C}\) of complex numbers.<br />
A polynomial \(A(x)\) has degree k if its hightest nonzero coefficient is \(a_k\); we write that \(degree(A) = k\).<br />
Any integer strictly greater than the degree of a polynomial is a <b>degree-bound</b> of that polynomial.<br />
Therefore, the degree of a polynomial of degree-bound n may be any integer between 0 and n-1, inclusive.<br />
</p>

<p>
For <b>polynomial addition</b>, if \(A(x)\) and \(B(x)\) are polynomials of degree-bound n, their sum is a polynomial \(C(x)\), also of degree-bound n, such that \(C(x) = A(x) + B(x)\) for all x in the underlying field.<br />
That is, if<br />
</p>
\begin{equation}
A(x) = \sum_{j=0}^{n-1} a_jx^j
\end{equation}
<p>
and<br />
</p>
\begin{equation}
B(x) = \sum_{j=0}^{n-1} b_jx^j
\end{equation}
<p>
then<br />
</p>
\begin{equation}
C(x) = \sum_{j=0}^{n-1} c_jx^j
\end{equation}
<p>
where<br />
</p>
\begin{equation}
c_j = a_j + b_j
\end{equation}

<p>
For <b>polynomial multiplication</b>, if \(A(x)\) and \(B(x)\) are polynomials of degree-bound n, their product \(C(x)\) is a polynomial of degree-bound \(2n-1\) such that \(C(x)=A(x)B(x)\) for all x in the underlying field.<br />
</p>
\begin{equation}
C(x) = \sum_{j=0}^{2n-2} c_jx^j
\end{equation}
<p>
where<br />
</p>
\begin{equation}
c_j = \sum_{k=0}^j a_k b_{j-k}
\end{equation}
</div>
</li>
</ol>

<div id="outline-container-org05876db" class="outline-4">
<h4 id="org05876db"><span class="section-number-4">8.3.1.</span> Representing polynomials</h4>
<div class="outline-text-4" id="text-8-3-1">
<p>
The coefficient and point-value representations of polynomials are in a sense equivalent; that is, a polynomial in point-value form has a unique counterpart in coefficient form.<br />
</p>
</div>

<ol class="org-ol">
<li><a id="orge95d894"></a>Coefficient representation<br />
<div class="outline-text-5" id="text-8-3-1-1">
<p>
A coefficient representation of a polynomial \(A(x) = \sum_{j=0}^{n-1} a_j x^j\) of degree-bound n is a vector of coefficients \(a = (a_0, a_1,...,a_{n-1})\).<br />
</p>

<p>
We can evaluate a polynomial in \(\Theta(n)\) time using Horner's rule:<br />
</p>
\begin{equation}
A(x_0) = a_0 + x_0(a_1 + x_0(a_2 + \cdots + x_0(a_{n-2} + x_0(a_{n-1})) \cdots))
\end{equation}

<p>
The additions in \(\Theta(n)\) and the multiplication in \(\Theta(n^2)\).<br />
</p>


<p>
The resulting coefficients vector c in the multiplication<br />
</p>
\begin{equation}
c_j = \sum_{k=0}^j a_k b_{j-k}
\end{equation}
<p>
is also called the convolution of the input vector a and b, denote \(c = a \otimes b\).<br />
</p>
</div>
</li>


<li><a id="orgbc51aad"></a>Point-value representation<br />
<div class="outline-text-5" id="text-8-3-1-2">
<p>
A point-value representation of a polynomial \(A(x)\) of degree-bound n is a set of n point-value pairs<br />
</p>
\begin{equation}
\{ (x_0,y_0), (x_1,y_1), ..., (x_{n-1},y_{n-1})\}
\end{equation}
<p>
such that all of the \(x_k\) are distinct and<br />
</p>
\begin{equation}
\label{eq:point-representation}
y_k = A(x_k)
\end{equation}
<p>
for \(k = 0,1,...,n-1\).<br />
A polynomial has many different point-value representations, since we can use any set of n distinct points \(x_0,x_1,...,x_{n-1}\) as a basis for the representation.<br />
</p>

<pre class="example" id="org12c7a72">
Computing a point-value representation for a polynomial given in coefficient form is straightforward.
Determing the coefficient form of a polynomial from a point-value representation is interpolation.
</pre>


<p>
<b>Therem 30.1 (Uniqueness of an interpolating polynomial)</b><br />
For any set \(\{ (x_0,y_0), (x_1,y_1), ... , (x_{n-1},y_{n-1}) \}\) of n point-value pairs such that all the \(x_k\) values are distinct, there is a unique polynomial \(A(x)\) of degree-bound n such that \(y_k = A(x)\) for \(k = 0, 1, ... , n-1\).<br />
</p>

<p>
Equation \(\eqref{eq:point-representation}\) is equivalent to the matrix equation<br />
</p>
\begin{equation}
\begin{pmatrix}
1 & x_0 & x_0^2 & \cdots & x_0^{n-1} \\
1 & x_1 & x_1^2 & \cdots & x_1^{n-1} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n-1} & x_{n-1}^2 & \cdots & x_{n-1}^{n-1} \\
\end{pmatrix}
\begin{pmatrix}
a_0 \\ a_1 \\ \vdots \\ a_{n-1}
\end{pmatrix}
=
\begin{pmatrix}
y_0 \\ y_1 \\ \vdots \\ y_{n-1}
\end{pmatrix}
\end{equation}

<p>
The matrix on the left is denoted \(V(x_0, x_1, ..., x_{n-1})\) and is known as a Vandermonder matrix.<br />
This matrix has determinant<br />
</p>
\begin{equation}
\prod_{0 \le j < k \le n-1} (x_k - x_j)
\end{equation}
<p>
It is invertible if the \(x_k\) are distinct.<br />
That we can sovle the coefficient \(a_j\) uniquely given the point-value representation:<br />
</p>
\begin{equation}
a = V(x_0, x_1, ..., x_{n-1})^{-1}y \ .
\end{equation}

<p>
With Lagrange's formula:<br />
</p>
\begin{equation}
A(x) = \sum_{k=0}^{n-1} y_k \frac{\prod_{j\ne k} (x-x_j)}{\prod_{j\ne k} (x_k - x_j)} \ .
\end{equation}
<p>
We can compute the coefficients of A in time \(\Theta(n^2)\).<br />
</p>

<p>
The point-value representation is quite convenient for many operations on polynomials.<br />
The time is \(\Theta(n)\) for addition and multiplication.<br />
</p>

<p>
How to evaluate a polynomial given in point-value form at a new point?<br />
The approach is converting the polynomial to coefficient form first, and then evaluating it at the new point.<br />
</p>
</div>
</li>


<li><a id="org76a3b1e"></a>Fast multiplication of polynomials in coefficient form<br />
<div class="outline-text-5" id="text-8-3-1-3">
<p>
Can we use the linear-time multiplication method for polynomials in point-value form to expedite polynomial multiplication in coefficient form?<br />
The answer hinges on whether we can convert a polynomial quickly from a coefficient form to point-value form and vice versa.<br />
</p>


<div id="org6e55e8f" class="figure">
<p><img src="images/c30-fft-fig.png" alt="c30-fft-fig.png" /><br />
</p>
</div>


<p>
procedures:<br />
</p>
<ol class="org-ol">
<li>Double degree-bound: Create coefficient representation of \(A(x)\) and \(B(x)\) as degree-bound \(2n\) polynomials by adding n high-order zero coefficients to each.<br /></li>
<li>Evaluate: Compute point-value representations of \(A(x)\) and \(B(x)\) of length \(2n\) by applying the FFT of order \(2n\) on each polynomial.<br /></li>
<li>Pointwise multiply: Compute a point-value representation for the polynomial \(C(x) = A(x)B(x)\) by multiplying these values together pointwise.<br /></li>
<li>Interpolate: Create the coefficient representation of the polynomial \(C(x)\) by applying the FFT on \(2n\) point-value pairs to compute the inverse DFT.<br /></li>
</ol>

<p>
<b>Theorem 30.2</b><br />
We can multiply two polynomials of degree-bound n in time \(\Theta(n\lg n)\), with both the input and output representations in coefficient form.<br />
</p>
</div>
</li>
</ol>
</div>



<div id="outline-container-orgadb8bbc" class="outline-4">
<h4 id="orgadb8bbc"><span class="section-number-4">8.3.2.</span> The DFT and FFT</h4>
<div class="outline-text-4" id="text-8-3-2">
</div>
<ol class="org-ol">
<li><a id="orgf66c27c"></a>Complex roots of unity<br />
<div class="outline-text-5" id="text-8-3-2-1">
<p>
A complex nth root of unity is a complex number \(\omega\) such that<br />
</p>
\begin{equation}
\omega^n = 1
\end{equation}

<p>
There are exactly n complex nth roots of unity: \(e^{2\pi i k/n}\) for \(k = 0,1,...,n-1\).<br />
</p>


<p>
To interpret this formula, we use the definition of the exponential of a complex number<br />
</p>
\begin{equation}
e^{iu} = \cos(u) + i\sin(u)
\end{equation}


<div id="org26a65b9" class="figure">
<p><img src="images/c30-complex-root.png" alt="c30-complex-root.png" /><br />
</p>
</div>

<p>
The value<br />
</p>
\begin{equation}
\omega_n = e^{2\pi i/n}
\end{equation}
<p>
is the <b>principal nth root of unity</b>, all othe complex nth roots of unity are powers of \(\omega_n\)<br />
</p>

<p>
<b>Lemma 10.3 (Cancellation lemma)</b><br />
For any integers \(n \ge 0, k \ge 0\), and \(d >0\),<br />
</p>
\begin{equation}
\omega_{dn}^{dk} = \omega_n^k
\end{equation}

<hr />
\begin{equation}
\omega_{dn}^{dk} = (e^{2\pi i/dn})^{dk} = (e^{2\pi i/n})^k = \omega_n^k
\end{equation}
<hr />

<p>
<b>Corollary 30.4</b><br />
For any even integer \(n > 0\)<br />
</p>
\begin{equation}
\omega_n^{n/2} = \omega_2 = -1
\end{equation}

<hr />
\begin{equation}
\omega_n^{n/2} = (e^{2\pi i/n})^{n/2} = e^{2\pi i/2} = \omega_2 = e^{\pi i} = \cos(\pi) + i\sin(\pi) = -1
\end{equation}
<hr />

<p>
<b>Lemma 30.5 (Halving lemma)</b><br />
If \(n > 0\) is even, then the squares of the n complex nth roots of unity are the n/2 complex (n/2)th roots of unity<br />
</p>

<hr />
\begin{equation}
(\omega_{n}^{k+n/2})^2 = \omega_n^{2k+n} = \omega_n^{2k}\omega_n^n = \omega_n^{2k} = (\omega_n^k)^2
\end{equation}
<hr />

<p>
<b>Lemma 30.6 (Summation lemma)</b><br />
For any integer \(n > 1\) and nonzero integer k not divisible by n,<br />
</p>
\begin{equation}
\sum_{j=0}^{n-1} (\omega_n^k)^j = 0
\end{equation}

<hr />
\begin{equation}
\sum_{j=0}^{n-1} (\omega_n^k)^j = \frac{(\omega_n^k)^n -1}{\omega_n^k-1} = \frac{(\omega_n^n)^k -1}{\omega_n^k-1} = \frac{(1)^k - 1}{\omega_n^k-1} = 0
\end{equation}
<hr />
</div>
</li>


<li><a id="orge93b4b5"></a>The DFT<br />
<div class="outline-text-5" id="text-8-3-2-2">
<p>
We wish to evaluate a polynomial<br />
</p>
\begin{equation}
A(x) = \sum_{j=0}^{n-1} a_jx^j
\end{equation}
<p>
of degree-bound n at \(\omega_n^0,\omega_n^1,...,\omega_n^{n-1}\).<br />
We assume that A is given in coefficient form: \(a = (a_0, a_1, ... , a_{n-1})\).<br />
</p>

<p>
Define the results \(y_k\), for \(k = 0,1,...,n-1\), by<br />
</p>
\begin{equation}
y_k = A(\omega_n^k) = \sum_{j=0}^{n-1} a_j \omega_n^{kj}
\end{equation}

<p>
The vector \(y = (y_0, y_1, ... , y_{n-1})\) is the <b>discrete Fourier transform (DFT)</b> of the coefficient vector \(a = (a_0, a_1, ..., a_{n-1}\).<br />
</p>
</div>
</li>

<li><a id="org108c866"></a>The FFT<br />
<div class="outline-text-5" id="text-8-3-2-3">
<p>
By using FFT, which takes advantage of the special properties of the complex roots of unity, we can compute \(DFT_n(a)\) in time \(\Theta(n\lg n)\).<br />
</p>

<hr />
<p>
divide-and-conquer strategy:<br />
</p>
\begin{equation}
A^{[0]}(x) = a_0 + a_2x + a_4x + \cdots + a_{n-2}x^{n/2-1}
\end{equation}
\begin{equation}
A^{[1]}(x) = a_1 + a_3x + a_5x + \cdots + a_{n-1}x^{n/2-1}
\end{equation}

\begin{equation}
A(x) = A^{[0]}(x^2) + xA^{[1]}(x^2)
\end{equation}
<hr />


<div id="org0f06ec6" class="figure">
<p><img src="images/c30-recursive-fft.png" alt="c30-recursive-fft.png" /><br />
</p>
</div>
</div>
</li>

<li><a id="org81f7d61"></a>Interpolation at the complex roots of unity<br />
<div class="outline-text-5" id="text-8-3-2-4">

<div id="org3a18667" class="figure">
<p><img src="images/c30-interpolation.png" alt="c30-interpolation.png" /><br />
</p>
</div>

<p>
For the inverse operation, which we write as \(a = DFT_n^{-1}(y)\), we proceed by multiplying y by the matrix \(V_n^{-1}\), the inverse of \(V_n\).<br />
</p>

<p>
<b>Theorem 30.7</b><br />
For \(j,k = 0,1, ... , n-1\), the (j,k) entry of \(V_n^{-1}\) is \(\omega_n^{-kj}/n\).<br />
</p>


<p>
Given the inverse matrix \(V_n^{-1}\), we have that \(DFT_n^{-1}(y)\) is given by<br />
</p>
\begin{equation}
a_j = \frac{1}{n} \sum_{k=0}^{n-1} y_k \omega_n^{-kj}
\end{equation}
<p>
for \(j = 0,1,...,n-1\).<br />
</p>

<p>
Similary to FFT, we can compute \(DFT_n^{-1}\) in \(\Theta(n\lg n)\) time as well.<br />
</p>


<p>
<b>Theorem 30.8 (Convolution theorem)</b><br />
For any two vector a and b of length n, where n is a power of 2,<br />
</p>
\begin{equation}
a \otimes b = DFT_{2n}^{-1} (DFT_{2n}(a) \cdot DFT_{2n}(b))
\end{equation}
<p>
where the vector a and b are padded with 0s to length 2n and \(\cdot\) denote the componentwise product of two 2n-element vectors.<br />
</p>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-orgd05fdf4" class="outline-3">
<h3 id="orgd05fdf4"><span class="section-number-3">8.4.</span> Number-Theoretic Algorithms</h3>
<div class="outline-text-3" id="text-8-4">
<p>
For analyzing number-theoretic algorithms, the size of an input is measured in terms of the <b>number of bits</b>.<br />
It is also convenient to measure how many bit operations a number-theoretic algorithm requires.<br />
</p>
</div>

<div id="outline-container-orgf0685a6" class="outline-4">
<h4 id="orgf0685a6"><span class="section-number-4">8.4.1.</span> Elementary number-theoretic notions</h4>
<div class="outline-text-4" id="text-8-4-1">
\begin{equation}
\mathbb{Z} = \{ ..., -2, -1, 0, 1, 2, ...\}
\end{equation}
\begin{equation}
\mathbb{N} = \{ 0, 1, 2, ... \}
\end{equation}
</div>

<ol class="org-ol">
<li><a id="org1a6bbdc"></a>Divisibility and divisors<br />
<div class="outline-text-5" id="text-8-4-1-1">
<p>
The notion \(d | a\) (read "d divides a") means that \(a = kd\) for some integer \(k\).<br />
Every integer divides 0.<br />
</p>

<p>
If \(d | a\), then we say that \(a\) is a <b>multiple</b> of \(d\).<br />
</p>

<p>
If \(d | a\) and \(d \ge 0\), we say that \(d\) is a <b>divisor</b> of \(a\).<br />
For example, the divisors of 24 are 1,2,3,4,6,8,12 and 24.<br />
</p>

<hr />
<p>
Note that \(d|a\) if and only if \((-d) | a\), so that no generality is lost by defining the devisors to be nonnegative, with the understanding that the negative of any divisor of \(a\) also divides \(a\).<br />
</p>
<hr />

<p>
Every positive integer \(a\) is divisiable by the <b>trivial divisors</b> 1 and \(a\).<br />
The nontrival divisors of \(a\) are <b>factors</b> of \(a\).<br />
</p>
</div>
</li>




<li><a id="org4440644"></a>Prime and composite numbers<br />
<div class="outline-text-5" id="text-8-4-1-2">
<p>
An integer \(a > 1\) whose only divisors are the trival divisors \(1\) and \(a\) is a <b>prime number</b> or, more simply, a <b>prime</b>.<br />
</p>

<p>
The first 20 primes, in order are<br />
</p>
\begin{equation}
2,3,5,7,11,13,17,19,23,29,31,37,41,43,47,53,59,61,67,71
\end{equation}

<p>
An integer \(a > 1\) that is not prime is a <b>composite number</b> or, more simply, a <b>composite</b>.<br />
We call the integer 1 a <b>unit</b>, and it is neither prime nor composite.<br />
Similarly, the integer 0 and all negative integers are neither prime nor composite.<br />
</p>
</div>
</li>

<li><a id="org4b8650a"></a>The division theorem, remainders, and modular equivalence<br />
<div class="outline-text-5" id="text-8-4-1-3">
<p>
<b>Theorem 31.1 (Division theorem)</b><br />
For any integer \(a\) and any positive integer \(n\), there exist unique integers \(q\) and \(r\) such that \(0 \le r < n\) and \(a = qn + r\).<br />
</p>


<p>
The value \(q = \lfloor a/n \rfloor\) is the <b>quotitent</b> of the division.<br />
The value \(r = a \mod n\) is the <b>ramainder</b> (or <b>residue</b>) of the division.<br />
</p>

<p>
We can partition the integers into \(n\) equivalence classes according to their remainders modulo \(n\).<br />
The <b>equivalence class modulo n</b> containing an integer \(a\) is<br />
</p>
\begin{equation}
[a]_n = \{ a + kn: k \in \mathbb{Z} \}
\end{equation}

<p>
For example:<br />
</p>
\begin{equation}
[3]_7 = \{ ..., -11, -4, 3, 10, 17, ...\}
\end{equation}

<p>
The set of all such equivalence classes is<br />
</p>
\begin{equation}
\mathbb{Z}_n = \{ [a]_n: 0 \le a \le n-1 \} 
\end{equation}
</div>
</li>

<li><a id="orga25a1b3"></a>Common divisors and greatest common divisors<br />
<div class="outline-text-5" id="text-8-4-1-4">
<p>
If \(d\) is a divisor of \(a\) and \(d\) is also a divisor of \(b\), then \(d\) is <b>common divisor</b> of \(a\) and \(b\).<br />
</p>

<p>
An important property of common divisors is that<br />
</p>
\begin{equation}
d | a \quad \mathrm{and} \quad d | b \quad \mathrm{implies} \quad d | (ax+by)
\end{equation}
<p>
for any integers \(x\) and \(y\).<br />
</p>

<p>
If \(a|b\), then either \(|a| \le |b|\) or \(b = 0\), which implies that<br />
</p>
\begin{equation}
a|b \quad \mathrm{and} \quad b|a \quad \mathrm{implies} \quad a = \pm b 
\end{equation}

<p>
The <b>greatest common divisor</b> of two integers \(a\) and \(b\), not both zero, is the largest of the common divisors of \(a\) and \(b\); we denote it by \(gcd(a,b)\).<br />
</p>


<div id="org52244c0" class="figure">
<p><img src="images/c31-gcd-properties.png" alt="c31-gcd-properties.png" /><br />
</p>
</div>



<div id="org2793626" class="figure">
<p><img src="images/c31-theorem-2.png" alt="c31-theorem-2.png" /><br />
</p>
</div>


<div id="org00c6dca" class="figure">
<p><img src="images/c31-corollary-3.png" alt="c31-corollary-3.png" /><br />
</p>
</div>


<div id="orgda68b1f" class="figure">
<p><img src="images/c31-corollary-4.png" alt="c31-corollary-4.png" /><br />
</p>
</div>


<div id="org4973f6d" class="figure">
<p><img src="images/c31-corollary-5.png" alt="c31-corollary-5.png" /><br />
</p>
</div>
</div>
</li>


<li><a id="org8aa2b89"></a>Relatively prime integers<br />
<div class="outline-text-5" id="text-8-4-1-5">
<p>
Two integers \(a\) and \(b\) are <b>relatively prime</b> if their only common divisor is 1, that is, if \(gcd(a,b) = 1\).<br />
</p>


<div id="org4b5e3b4" class="figure">
<p><img src="images/c31-theorem-6.png" alt="c31-theorem-6.png" /><br />
</p>
</div>
</div>
</li>

<li><a id="org6dc0cd6"></a>Unique factorization<br />
<div class="outline-text-5" id="text-8-4-1-6">

<div id="org05f0093" class="figure">
<p><img src="images/c31-theorem-7.png" alt="c31-theorem-7.png" /><br />
</p>
</div>


<div id="org3257cd7" class="figure">
<p><img src="images/c31-theorem-8.png" alt="c31-theorem-8.png" /><br />
</p>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgb0bbeed" class="outline-4">
<h4 id="orgb0bbeed"><span class="section-number-4">8.4.2.</span> Greatest common divisor</h4>
<div class="outline-text-4" id="text-8-4-2">
<p>
In principle, we can compute \(gcd(a,b)\) for positive integers \(a\) and \(b\) from the prime factorizations of \(a\) and \(b\).<br />
</p>
\begin{equation}
a = p_1^{e_1} p_2^{e_2} \cdots p_r^{e_r}
\end{equation}
\begin{equation}
b = p_1^{f_1} p_2^{f_2} \cdots p_r^{f_r}
\end{equation}

\begin{equation}
gcd(a,b) = p_1^{\min(e_1,f_1)} p_2^{\min(e_2,f_2)} \cdots p_r^{\min(e_r,f_r)}
\end{equation}

<p>
<b>Theorem 31.9 (GCD recursive theorem)</b><br />
For any nonnegative integer \(a\) and any positive integer \(b\),<br />
</p>
\begin{equation}
gcd(a,b) = gcd(b, a\mod b)
\end{equation}
</div>


<ol class="org-ol">
<li><a id="org3f08f98"></a>Euclid's algorithm<br />
<div class="outline-text-5" id="text-8-4-2-1">
<pre class="example" id="orgcf445ea">
EUCLID(a,b)
    if b == 0
        return a
    else
        return EUCLID(b, a mod b)
</pre>

<p>
The correctness of EUCLID follows from Theorem 31.9 and the property of gcd.<br />
</p>
</div>
</li>

<li><a id="org32782c0"></a>The running time of Euclid's algorithm<br />
<div class="outline-text-5" id="text-8-4-2-2">
<p>
We assume with no loss of generality that \(a > b \ge 0\).<br />
</p>

<p>
<b>Lemma 31.10</b><br />
If \(a > b \ge 1\) and the call \(EUCLID(a,b)\) performs \(k \ge 1\) recursive calls, then \(a \ge F_{k+2}\) and \(b \ge F_{k+1}\).<br />
(\(F_k\) is the Fibonacci number)<br />
</p>


<p>
<b>Theorem 31.11 (Lamme's theorem)</b><br />
For any integer \(k \ge 1\), if \(a > b \ge 1\) and \(b < F_{k+1}\), then the call EUCLID(a,b) makes fewer than k recursive calls.<br />
</p>
</div>
</li>

<li><a id="org803305d"></a>The extended form of Euclid's algorithm<br />
<div class="outline-text-5" id="text-8-4-2-3">
\begin{equation}
d = gcd(a,b) = ax + by
\end{equation}


<div id="org9972d5d" class="figure">
<p><img src="images/c31-extended-euclid.png" alt="c31-extended-euclid.png" /><br />
</p>
</div>


<div id="orgc658452" class="figure">
<p><img src="images/c31-extended-euclid-fig.png" alt="c31-extended-euclid-fig.png" /><br />
</p>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgedeb458" class="outline-4">
<h4 id="orgedeb458"><span class="section-number-4">8.4.3.</span> Modular arithmetic</h4>
<div class="outline-text-4" id="text-8-4-3">
</div>
<ol class="org-ol">
<li><a id="orga7de8e5"></a>Finite groups<br />
<div class="outline-text-5" id="text-8-4-3-1">
<p>
A <b>group</b> (S, \(\oplus\)) is a set S together with a binary operation \(\oplus\) defined on S for which the following properties hold:<br />
</p>
<ol class="org-ol">
<li><b>closure</b> : For all \(a,b \in S\), we have \(a \oplus b \in S\).<br /></li>
<li><b>identity</b> : There exists an element \(e \in S\), called the identity of the group, such that \(e \oplus a = a \oplus e = a\) for all \(a \in S\).<br /></li>
<li><b>associativity</b> : For all \(a,b,c \in S\), we have \((a\oplus b)\oplus c = a \oplus (b \oplus c)\).<br /></li>
<li><b>inverses</b> : For all \(a \in S\), there exists a unique element \(b \in S\), called the inverse of \(a\), such that \(a \oplus b = b \oplus a = e\).<br /></li>
</ol>

<p>
For eaxmple, group \((\mathbb{Z}, +)\).<br />
</p>


<p>
If a group \((S, \oplus)\) satifies the <b>commutative law</b> \(a \oplus b = b \oplus a\) for all \(a,b in S\), then it is an <b>abelian group</b>.<br />
If a group \((S, \oplus)\) satifies \(|S| < \infty\), then it is called a <b>finite group</b>.<br />
</p>
</div>
</li>

<li><a id="orgdf00818"></a>The groups defined by modular addition and multiplication<br />
<div class="outline-text-5" id="text-8-4-3-2">
<p>
To define a group on \(\mathbb{Z}_n\), we need to have suitable binary operations.<br />
We can easily define addition and multiplication operations for \(\mathbb{Z}_n\), because the equivalence class of two integers uniquely determines the equivalence class of their sum or product.<br />
That is, if \(a \equiv a' (\mod n)\) and \(b \equiv b (\mod n)'\), then<br />
</p>
\begin{equation}
a + b \equiv a' + b' (\mod n)
\end{equation}
\begin{equation}
ab \equiv a'b' (\mod n)
\end{equation}

<p>
Thus, we define addition and multiplication modulo \(n\), denoted \(+_n\) and \(\cdot_n\), by<br />
</p>
\begin{equation}
[a]_n +_n [b]_n = [a+b]_n
\end{equation}
\begin{equation}
[a]_n \cdot_n [b]_n = [ab]_n
\end{equation}

<p>
Using this definition of addition modulo \(n\), we can define the additive group modulo n as \((\mathbb{Z}_n, +_n)\)<br />
The size of the additive group modulo \(n\) is \(|\mathbb{Z}_n| = n\).<br />
</p>

<p>
<b>Theorem 31.12</b><br />
The system \((\mathbb{Z}_n, +_n)\) is a finite abelian group.<br />
</p>

<p>
Using the definition of multiplication modulo \(n\), we define the multiplicative group modulo n as \((\mathbb{Z}_n^{*}, \cdot_n)\).<br />
The elements of this group are the set \(\mathbb{Z}_n^{*}\) of elements in \(\mathbb{Z}_n\) that are relatively prime to \(n\), so that each one has a unique inverse, modulo \(n\):<br />
</p>

\begin{equation}
\mathbb{Z}_n^{*} = \{ [a]_n \in \mathbb{Z}_n : gcd(a,n) = 1 \}
\end{equation}

<p>
<b>Theorem 31.13</b><br />
The system \((\mathbb{Z}_n^{*}, \cdot_n)\) is a finite abelian group.<br />
</p>


<div id="orgae560da" class="figure">
<p><img src="images/c31-additive-group.png" alt="c31-additive-group.png" /><br />
</p>
</div>


<p>
The size of \(\mathbb{Z}_n^{*}\) is denoted \(\phi(n)\).<br />
This function, known as <b>Euler's phi function</b>, satisifies the equation<br />
</p>
\begin{equation}
\phi(n) = n \prod_{p : p \ \mathrm{is\ prime\ and}\  p | n} \left ( 1 - \frac{1}{p} \right )
\end{equation}
</div>
</li>


<li><a id="orgbc3cbd3"></a>Subgroups<br />
<div class="outline-text-5" id="text-8-4-3-3">
<p>
If \((S,\oplus)\) is a group, \(S' \subseteq S\), and \((S', \oplus)\) is also a group, then \((S',\oplus)\) is a subgroup of \((S,\oplus)\).<br />
</p>

<p>
<b>Theorem 31.14 (A nonempty closed subset of a finite group is a subgroup)</b><br />
If \((S,\oplus)\) is a finite group and \(S'\) is any nonempty subset of \(S\) such that \(a \oplus b \in S'\) for all \(a,b \ in S'\), then \((S', \oplus)\) is a subgroup of \((S,\oplus)\).<br />
</p>

<p>
<b>Theorem 31.15 (Lagrange's theorem)</b><br />
If \((S,\oplus)\) is a finite group and \((S',\oplus)\) is a subgroup of \((S,\oplus)\), then \(|S'|\) is a divisor of \(|S|\).<br />
</p>

<p>
A subgroup \(S'\) of a group \(S\) is a <b>proper</b> subgroup if \(S' \ne S\).<br />
</p>


<p>
<b>Corollary 31.16</b><br />
If \(S'\) is a proper subgroup of a finite group \(S\), then \(|S'| \le |S|/2\).<br />
</p>
</div>
</li>

<li><a id="org706c1c6"></a>Subgroup generated by an element<br />
<div class="outline-text-5" id="text-8-4-3-4">
<p>
Therorem 31.14 gives us an easy way to produce a subgroup of a finite group \((S,\oplus)\):<br />
choose an element \(a\) and take all elements that can be generated from \(a\) using the group operation.<br />
</p>

<p>
Define \(a^{(k)}\) for \(k \ge 1\) by<br />
</p>
\begin{equation}
a^{(k)} = \overset{k}{\underset{i=1}{\large\oplus}}a = \underbrace{a \oplus a \oplus \cdots \oplus a}_{k}
\end{equation}

<p>
We define the <b>subgroup generated by a</b>, denoted \(\langle a \rangle\) or \((\langle a \rangle, \oplus)\) by<br />
</p>
\begin{equation}
\langle a \rangle = \{ a^{(k)}: k \ge 1 \}
\end{equation}

<p>
We say that \(a\) <b>generates</b> the subgroups \(\langle a \rangle\) or that \(a\) is a <b>generator</b> of \(\langle a \rangle\).<br />
</p>


<p>
For example:<br />
</p>


<div id="orgfdcdeb3" class="figure">
<p><img src="images/c31-example.png" alt="c31-example.png" /><br />
</p>
</div>

<p>
The <b>order</b> of \(a\) (in the group \(S\)), denoted ord(a), is defined as the smallest positive integer \(t\) such that \(a^{(t)} = e\).<br />
</p>

<p>
<b>Theorem 31.17</b><br />
For any finite group \((S, \oplus)\) and any \(a \in S\), the order of \(a\) is equal to the size of the subgroup it generates, or \(ord(a) = |\langle a \rangle|\).<br />
</p>

<p>
<b>Corollary 31.18</b><br />
The sequence \(a^{(1)},a^{(2)},\dots\) is periodic with period \(t = ord(a)\); that is \(a^{(i)} = a^{(j)}\) if and only if \(i \equiv j (\mod t)\).<br />
</p>

<p>
<b>Corollary 31.19</b><br />
If \((S,\oplus)\) is a finite group with identity \(e\), then for all \(a \in S\), \(a^{(|S|)} = e\).<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org76c90d1" class="outline-4">
<h4 id="org76c90d1"><span class="section-number-4">8.4.4.</span> Sovling modular linear equations</h4>
<div class="outline-text-4" id="text-8-4-4">
<p>
Find solutions to the equation<br />
</p>
\begin{equation}
ax \equiv b (\mod n)
\end{equation}

<p>
<b>Theorem 31.20</b><br />
For any positive integers \(a\) and \(n\), if \(d = gcd(a,n)\), then<br />
</p>
\begin{equation}
\langle a \rangle = \langle b \rangle  = \{ 0, d, 2d, ... , ((n/d) - 1)d \}
\end{equation}
<p>
in \(\mathbb{Z}_n\), and thus<br />
</p>
\begin{equation}
|\langle a \rangle | = n/d
\end{equation}


<p>
<b>Corollary 31.21 (solvable condition)</b><br />
The equation \(a \equiv b (\mod n)\) is solvable for the unknown \(x\) <b>if and only if</b> \(d|b\), where \(d = gcd(a,n)\).<br />
</p>

<p>
<b>Corollary 31.22 (has solution or not)</b><br />
The equation \(ax \equiv b (\mod n)\) either has \(d\) distinct solutions modulo \(n\), where \(d = gcd(a,n)\), or it has no solutions.<br />
</p>

<p>
<b>Theorem 31.23</b><br />
Let \(d = gcd(a,n)\), and suppose that \(d = ax' + by'\) for some integers \(x'\) and \(y'\). If \(d|b\) then the equation \(ax \equiv b (\mod n)\) has as one of its solutions the value \(x_0\), where<br />
</p>
\begin{equation}
x_0 = x'(b/d) \mod n
\end{equation}

<p>
<b>Theorem 31.24</b><br />
Suppose that the equation \(ax \equiv b (\mod n)\) is solvable and that \(x_0\) is any solution to this equation.<br />
Then this equation has exactly \(d\) distinct solutions, modulo \(n\), given by \(x_i = x_0 + i(n/d)\) for \(i = 0,1,...,d-1\).<br />
</p>



<div id="orgb61e2cb" class="figure">
<p><img src="images/c31-modular-linear-equation-solver.png" alt="c31-modular-linear-equation-solver.png" /><br />
</p>
</div>


<p>
<b>Corollary 31.25 (unique solution)</b><br />
For any \(n >1\), if \(gcd(a,n)=1\), then the equation \(ax \equiv b (\mod n)\) has unique solution, modulo \(n\).<br />
</p>

<p>
<b>Corollary 31.26 (unique solution)</b><br />
For any \(n >1\), if \(gcd(a,n)=1\), then the equation \(ax \equiv 1 (\mod n)\) has unique solution, modulo \(n\). Otherwise, it has no solution.<br />
</p>
</div>
</div>

<div id="outline-container-org7afbf8b" class="outline-4">
<h4 id="org7afbf8b"><span class="section-number-4">8.4.5.</span> <span class="todo TODO">TODO</span> The Chinese remainder theorem</h4>
<div class="outline-text-4" id="text-8-4-5">

<div id="org8166d28" class="figure">
<p><img src="images/c31-theorem27.png" alt="c31-theorem27.png" /><br />
</p>
</div>


<div id="org305354f" class="figure">
<p><img src="images/c31-corollary-28-29.png" alt="c31-corollary-28-29.png" /><br />
</p>
</div>
</div>
</div>


<div id="outline-container-org625bdf8" class="outline-4">
<h4 id="org625bdf8"><span class="section-number-4">8.4.6.</span> Power of an element</h4>
<div class="outline-text-4" id="text-8-4-6">
<p>
The question:<br />
the sequence of powers of \(a\), modulo \(n\), where \(a \in \mathbb{Z}_n^{*}\):<br />
\(a^0, a^1, a^2, a^3, \dots\)<br />
modulo \(n\).<br />
</p>

<p>
<b>Theorem 31.30 (Euler's theorem)</b><br />
For any integer \(n > 1\),<br />
\(a^{\phi(n)} \equiv 1 (\mod n)\) for all \(a\in \mathbb{Z}_n^{*}\).<br />
</p>

<p>
Note:<br />
\(\mathbb{Z}_n^{*} = \{ [a]_n \in \mathbb{Z}_n : gcd(a,n) = 1\}\).<br />
gcd: greatest common divisor<br />
\(\mathbb{Z}_n = \{ [a]_n: a \le a \le n-1\}\)<br />
\([a]_n = \{ a + kn: k\in \mathbb{Z}\}\)<br />
\(\mathbb{Z} = \{ \dots, -2, -1, 0, 1, 2, \dots \}\)<br />
\(\phi(n)\) denote the size of \(\mathbb{Z}_n^{*}\).<br />
</p>

<p>
<b>Theorem 31.31 (Fermat's theorem)</b><br />
If \(p\) is prime, then<br />
\(a^{p-1} \equiv 1 (\mod p)\) for all \(a \in \mathbb{Z}_p^{*}\).<br />
</p>

<p>
<b>Theorem 31.32</b><br />
The values of \(n > 1\) for which \(\mathbb{Z}_n^{*}\) is cyclic are 2,4,\(p^e\), and \(2p^3\), for all primes \(p>2\) and all positive integers \(e\).<br />
</p>

<p>
<b>Theorem 31.33 (Discrete logarithm theorem)</b><br />
If \(g\) is a primiitive root of \(\mathbb{Z}_n^{*}\), then the equation \(g^x \equiv g^y (\mod n)\) holds if and only if the equation \(x \equiv y (\mod \phi(n))\) holds.<br />
</p>

<p>
<b>Theorem 31.34</b><br />
If \(p\) is an odd prime and \(e\ge 1\), then the equation<br />
\(x^2 \equiv 1 (\mod p^e)\)<br />
has only two solutions, namely \(x=1\) and \(x=-1\).<br />
</p>
</div>


<ol class="org-ol">
<li><a id="org56c453a"></a><span class="todo TODO">TODO</span> Rasing to powers with repeated squaring (why?)<br />
<div class="outline-text-5" id="text-8-4-6-1">
<p>
operation: raising one number to a power modulo another number, also known as modular exponentiation, i.e. computing \(a^b \mod n\).<br />
</p>

<p>
Let \(\langle b_k, b_{k-1},\dots, b_1, b_0\) be the binary representation of \(b\).<br />
The following procedure computes \(a^c \mod n\) as \(c\) is increased by doubling and incrementations from 0 to \(b\).<br />
</p>




<div id="org549ac58" class="figure">
<p><img src="images/c31-modular-exponentiation.png" alt="c31-modular-exponentiation.png" /><br />
</p>
</div>


<div id="org44a546c" class="figure">
<p><img src="images/c31-modular-exponentiation-example.png" alt="c31-modular-exponentiation-example.png" /><br />
</p>
</div>
</div>
</li>
</ol>
</div>


<div id="outline-container-org8740ead" class="outline-4">
<h4 id="org8740ead"><span class="section-number-4">8.4.7.</span> The RSA public-key cryptosystem</h4>
<div class="outline-text-4" id="text-8-4-7">
<p>
The RSA public-key cryptosystem relies on the dramatic difference between the ease of finding large prime numbers and the difficulty of factoring the product of two large prime numbers.<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org7703743"></a>Public-key cryptosystems<br />
<div class="outline-text-5" id="text-8-4-7-1">
<p>
In a public-key cryptosystem, each participant has both a public and a secret key.<br />
Each key consists of a pair of integers.<br />
Suppose the paricipants are "Alice" and "Bob".<br />
\(P_A\),\(S_A\) is denoted the public and secret keys for Alice and \(P_B\),\(S_B\) for Bob.<br />
Secret keys are kept secret, but public keys can be revealed to anyone or even published.<br />
Let \(\mathscr{D}\) denote the set of permissible messages.<br />
Let \(P_A()\) denotes the function corresponding to Alice's public key \(P_A\).<br />
</p>

<p>
The public and secret keys for any participant are a "matched pair" in that they specify functions that are inverse of each other.<br />
That is,<br />
</p>
\begin{equation}
M = S_A(P_A(M))
\end{equation}
\begin{equation}
M = P_A(S_A(M))
\end{equation}


<div id="orgea016d9" class="figure">
<p><img src="images/c31-encryption.png" alt="c31-encryption.png" /><br />
</p>
</div>


<div id="org56f9542" class="figure">
<p><img src="images/c31-signature.png" alt="c31-signature.png" /><br />
</p>
</div>

<p>
By comparing the above protocols for encrytion and for signatures, we can create messages that are both protocols for encrypted.<br />
</p>
<ol class="org-ol">
<li>The signer appends the digital signauture to the message<br /></li>
<li>and then entrypts the resulting message/signature pair with the public key of the intended recipient<br /></li>
<li>The recipient decryts the received message with the secret key to obtain the original message and its digital signature<br /></li>
<li>verify the signature using the public key of the signer.<br /></li>
</ol>
</div>
</li>


<li><a id="orge7e6631"></a>The RSA cryptosystem<br />
<div class="outline-text-5" id="text-8-4-7-2">
<p>
In the RSA cryptosystem, a participant creates his or her public and secret keys with the following procedure:<br />
</p>
<ol class="org-ol">
<li>Select at random two large prime numbers \(p\) and \(q\) such that \(p\ne q\).<br /></li>
<li>Compute \(n = pq\).<br /></li>
<li>Select a small odd integer \(e\) that is relatively prime to \(\phi(n)\), which by Euler phi function, equals \((p-1)(q-1)\).<br /></li>
<li>Compute \(d\) as the multiplicative inverse of \(e\), modulo \(\phi(n)\). (Corollary 31.26 guarantees that \(d\) exists and is uniquely defined.) (if \(gcd(a,n) = 1\), then the unique solution to the equation \(ax\equiv 1 (\mod n)\) is the integer x returned by EXTENDED-EUCLID)<br /></li>
<li>Publish the pair \(P=(e,n)\) as the participant's RSA public key.<br /></li>
<li>Keep secret the pair \(S =(d,n)\) as the participant's RAS secret key.<br /></li>
</ol>

<p>
For this scheme, the domain \(\mathscr{D}\) is the set \(\mathbb{Z}_n\).<br />
To transform a message M associated with a public key \(P = (e,n)\), compute<br />
</p>
\begin{equation}
P(M) = M^e \mod n.
\end{equation}
<p>
To transform a ciphertext C associated with a secret key \(S=(d,n)\), compute<br />
</p>
\begin{equation}
S(C) = C^d \mod n.
\end{equation}
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Mingming Li</p>
<p class="date">Created: 2024-03-05 Tue 23:04</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>